<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ye Zheng&#39;s Blog</title>
    <link>http://www.yezheng.pro/</link>
    <description>Recent content on Ye Zheng&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 07 Mar 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://www.yezheng.pro/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>http://www.yezheng.pro/about/</link>
      <pubDate>Mon, 25 Sep 2017 21:38:52 +0800</pubDate>
      
      <guid>http://www.yezheng.pro/about/</guid>
      
        <description>&lt;p&gt;I&amp;rsquo;m graduated from the Hebei GEO University with a degree in marketing, because I thought computers were cool from an early age, so I chose programmers as my future employment direction in my third year of college. In the third year of college, I learned some front-end knowledge, and in the fourth year of college I learned some C ++ related knowledge. After graduating, I naturally found a job related to programming.&lt;/p&gt;
&lt;p&gt;In the first company, I was mainly engaged in web crawling. I was responsible for scraping various financial data including stock exchanges, performing data cleaning, and completing an announcement classification system during the period. I worked as a data development engineer in the second company, and solved development problems such as real-time data forwarding, reception, and storage. I worked as a back-end development engineer at a third company and built the entire edx-based online learning system.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Transport Layer</title>
      <link>http://www.yezheng.pro/post/specialization/computer-networking/computer-networks-3/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/computer-networking/computer-networks-3/</guid>
      
        <description>&lt;h2 id=&#34;introduction-and-transport-layer-services&#34;&gt;Introduction and Transport-Layer Services&lt;/h2&gt;
&lt;p&gt;运输层位于应用层和网络层之间，该层为运行在不同主机上的应用进程提供直接的通信服务，它将网络层的在两个端系统之间的交付服务扩展到运行在两个不同端系统上的应用进程之间的交付服务。这句话的理解是，运输层协议为运行在不同主机上的应用进程之间提供了&lt;strong&gt;逻辑通信&lt;/strong&gt;(logic communication)功能，应用程序使用运输层提供的逻辑通信功能彼此发送报文，而无需考虑承载这些报文的物理基础设施的细节。&lt;/p&gt;
&lt;p&gt;运输层协议是在端系统中而不是在路由器中实现的。在发送端，运输层将从发送应用进程接收到的报文转换成运输层分组，该分组称为&lt;strong&gt;报文段&lt;/strong&gt;(segment)。实现方式可能是&lt;strong&gt;将应用报文划分为较小的块，并为每块加上一个运输层首部以生成运输层报文段&lt;/strong&gt;，然后在发送端系统中，运输层将这些报文段传递给网络层。注意：网络路由器仅作用于该数据报的网路层字段；即它们不检查封装在该数据报的运输层报文段的字段。在接收端，网络层从数据报中提取出运输层报文段，并将该报文段上交给运输层，经运输层处理后上交给应用进程使用。&lt;/p&gt;
&lt;h4 id=&#34;relationship-between-transport-and-network-layers&#34;&gt;Relationship Between Transport and Network Layers&lt;/h4&gt;
&lt;p&gt;运输层位于网络层之上。&lt;strong&gt;网络层&lt;/strong&gt;提供了&lt;strong&gt;主机&lt;/strong&gt;之间的逻辑通信，而&lt;strong&gt;运输层&lt;/strong&gt;为运行在不同&lt;strong&gt;主机的进程&lt;/strong&gt;之间提供了逻辑通信。&lt;strong&gt;运输层协议只工作在端系统中&lt;/strong&gt;，在端系统中，运输层协议将来自应用进程的报文移动到网络边缘(即网络层)，但对这些报文在网络核心如何移动并不做任何规定。中间路由器既不处理也不识别运输层加在应用层报文的任何信息。&lt;/p&gt;
&lt;p&gt;运输层协议常常受制于底层网络协议的服务模型。但即使底层网络协议是不可靠的，会使分组丢失、篡改和冗余，运输层协议也能为应用程序提供可靠的数据传输服务。&lt;/p&gt;
&lt;h4 id=&#34;overview-of-the-transport-layer-in-the-internet&#34;&gt;Overview of the Transport Layer in the Internet&lt;/h4&gt;
&lt;p&gt;因特网为应用层提供了两种截然不同的可用运输层协议，一个是UDP(用户数据报协议)，它为调用它的应用程序提供一种不可靠的、无连接的服务；另一个是TCP(传输控制协议)，它为调用它的应用程序提供一种可靠的、面向连接的服务。&lt;/p&gt;
&lt;p&gt;在讲UDP和TCP之前，需要简单了解网络层。网络层协议非常重要的是IP，即网际协议。IP为主机之间提供逻辑通信，IP的服务模型是&lt;strong&gt;尽力而为交付服务&lt;/strong&gt;(best-effort delivery service)，即它不确保报文段的交付，不保证报文段的按序交付，不保证报文段中数据的完整性，因此IP被称为&lt;strong&gt;不可靠服务&lt;/strong&gt;(unreliable service)。&lt;/p&gt;
&lt;p&gt;再来看UDP和TCP，两种协议的基本责任是&lt;strong&gt;将两个端系统间IP的交付服务扩展为运行在端系统上的两个进程间的交付服务&lt;/strong&gt;。将主机间交付扩展到进程间交付被称为运输层的&lt;strong&gt;多路复用&lt;/strong&gt;(transport-layer multiplexing)与&lt;strong&gt;多路分解&lt;/strong&gt;(demultiplexing)。UDP和TCP还可以通过在其报文段首部中包括&lt;strong&gt;差错检查字段&lt;/strong&gt;而提供完整性检查。进程到进程的数据交付和差错检查是两种最低限度的运输层服务，也是UDP能提供的仅有的两种服务。&lt;/p&gt;
&lt;p&gt;另一方面，TCP为应用程序提供几种附加服务。一是&lt;strong&gt;可靠数据传输&lt;/strong&gt;(reliable data transfer)，通过使用流量控制、序号、确认和定时器，TCP确保正确地、按序地将数据交付；另一个是&lt;strong&gt;拥塞控制&lt;/strong&gt;(congestion control)，它防止任何一条TCP连接用过多流量来淹没通信主机之间的链路和交换设备。&lt;/p&gt;
&lt;h2 id=&#34;multiplexing-and-demultiplexing&#34;&gt;Multiplexing and Demultiplexing&lt;/h2&gt;
&lt;p&gt;一个主机有一个或多个进程，而一个进程有一个或多个套接字，将运输层报文段中的数据交付到正确的套接字的工作称为&lt;strong&gt;多路分解&lt;/strong&gt;(demultiplexing)；在源主机从不同套接字中收集数据块，并为每个数据块封装上首部信息从而生成报文段，然后将报文段传递到网络层，所有这些工作称为&lt;strong&gt;多路复用&lt;/strong&gt;(multiplexing)。&lt;/p&gt;
&lt;p&gt;要实现多路复用，运输层需要：套接字有唯一的标识符；每个报文段有特殊字段来指示该报文段所要交付到的套接字。如图2.1，这些特殊字段是&lt;strong&gt;源端口号字段&lt;/strong&gt;(source port number filed)和&lt;strong&gt;目的端口号字段&lt;/strong&gt;(destination port number filed)。端口号是一个16比特的数，大小在0~65535之间。0~1023范围的端口号被称为周知端口号(well-known port number)，是受限制的，保留给如HTTP等周知的应用层协议使用。当我们开发一个新的应用程序时，必须为其分配一个端口号。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153426043-1801664430.png&#34;&gt;&lt;img src=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153428108-345024607.png&#34; alt=&#34;clip_image002&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​          图2.1 运输层报文段中的源与目的端口号字段&lt;/p&gt;
&lt;p&gt;因此多路分解就是，在报文段到达主机时，运输层检查报文段中的目的端口号，将其定位到相应的套接字。UDP基本上就是这样做的，但TCP的却更为复杂。&lt;/p&gt;
&lt;h6 id=&#34;无连接的多路复用和多路分解&#34;&gt;无连接的多路复用和多路分解&lt;/h6&gt;
&lt;p&gt;无连接的多路复用和多路分解，即UDP多路复用和多路分解。复用与分解的关键在于为套接字关联一个特定的端口号。在使用UDP通信时，创建一个UDP套接字，运输层可以自动为该套接字分配一个端口号，也可以手动为该套接字绑定一个特定的端口号。在形成运输层的报文段时，其中包括&lt;strong&gt;应用程序数据、源端口号(作为“返回地址”)、目的端口号和两个其他值&lt;/strong&gt;。这样就能够精确描述UDP的复用与分解。&lt;/p&gt;
&lt;p&gt;需要注意的是，&lt;strong&gt;一个UDP套接字是由一个二元组全面标识的&lt;/strong&gt;，该二元组包含了一个目的IP地址和一个目的端口号。因此，如果两个UDP报文段有不同的源IP地址和源端口号，但是具有相同的目的IP地址和目的端口号，那么这两个报文段将&lt;strong&gt;通过&lt;/strong&gt;相同的&lt;strong&gt;目的套接字****被定向到&lt;/strong&gt;相同的目的&lt;strong&gt;进程&lt;/strong&gt;，**即目的主机只使用目的端口号和IP地址定位相应的套接字。这一点与TCP是不同的。&lt;/p&gt;
&lt;h6 id=&#34;面向连接的多路复用与多路分解&#34;&gt;面向连接的多路复用与多路分解&lt;/h6&gt;
&lt;p&gt;TCP套接字与UDP套接字不同的是，因为TCP是面向连接的，**TCP套接字是由一个四元组(源IP地址，源端口号，目的IP地址，目的端口号)来标识的。**因此，当一个TCP报文段从网络到达主机时，该主机使用全部4个值来将报文段定向到相应的套接字。比如，两个具有不同源IP地址或端口号的TCP报文段将被定向到两个不同的套接字，除非TCP报文段携带的是初始创建连接的请求(因为TCP服务器有一个“欢迎套接字”，TCP连接建立时都需要先向该套接字发送请求)。&lt;/p&gt;
&lt;p&gt;服务器主机可以支持很多并行的TCP套接字，每个套接字与一个进程相联系，并由其四元组来标识每个套接字。当一个TCP报文段到达主机时，所有4个字段被用来将报文段定向分解到相应的套接字。&lt;/p&gt;
&lt;p&gt;端口扫描器nmap能够扫描能够接收TCP连接或能响应的UDP端口。&lt;/p&gt;
&lt;p&gt;如图2.2，主机A和主机C以及服务器B都有自己的唯一IP地址A、B、C，其中主机C向服务器B发送两个会话，主机A向服务器B发送一个会话。主机C为两个会话分配了两个不同的源端口号(26145和7532)，因为主机A选择源端口号时与主机C互不相干，因此它也可以将源端口号26145分配给其会话连接。尽管如此，服务器B仍能正确地分解这两个具有相同源端口号的连接，因为两个连接的源IP地址不同。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153438895-2037525561.png&#34;&gt;&lt;img src=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153440762-1703603010.png&#34; alt=&#34;clip_image003&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​                     图2.3 两个客户使用相同目的端口号和源端口号与同一服务器应用通信&lt;/p&gt;
&lt;h6 id=&#34;web服务器与tcp&#34;&gt;Web服务器与TCP&lt;/h6&gt;
&lt;p&gt;需要注意的是，**连接套接字并非与进程之间并不是一一对应关系。**早期每个Web服务器可以为每条连接生成一个新进程，这样每个进程都有自己的连接套接字，但现在的高性能Web服务器通常只使用一个进程，当客户请求连接时，这个进程会为每个新的客户创建一个具有新连接套接字的新线程，因此很多具有不同标识的的连接套接字连接到的是相同的进程。&lt;/p&gt;
&lt;h4 id=&#34;connectionless-transport-udp&#34;&gt;Connectionless Transport: UDP&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;运输层的最低限度是必须提供一种复用/分解服务，以便在网络层和正确的应用级进程之间传递数据。&lt;strong&gt;UDP就是一种非常简单的协议，在[RFC 768]的定义中，UDP除了&lt;/strong&gt;复用/分解以及少量差错检测&lt;/strong&gt;之外，几乎没有对IP增加别的东西。UDP从应用进程得到数据，附加上用于复用/分解的源和目的端口号字段，以及两个其他的小字段，然后形成报文段交给网络层。UDP做了非常少的工作，差不多应用进程是直接与IP交互。&lt;/p&gt;
&lt;p&gt;使用UDP时，在发送报文前，发送方和接收方的运输层实体之间&lt;strong&gt;没有握手&lt;/strong&gt;，正因如此，UDP被称为是&lt;strong&gt;无连接&lt;/strong&gt;的。&lt;/p&gt;
&lt;p&gt;DNS是一个通常使用UDP的应用层协议的例子。&lt;/p&gt;
&lt;p&gt;许多应用更适合使用UDP，主要原因有：&lt;/p&gt;
&lt;p&gt;（1）&lt;strong&gt;关于发送什么数据以及何时发送的应用层控制更为精细。&lt;strong&gt;采用UDP时，只要应用进程将数据传递给UDP，UDP就会将该数据打包进UDP报文段并传递给网络层。而TCP提供可靠传输服务，还有拥塞控制机制和超时重传机制，TCP不管可靠交付需要多长时间，一定要保证目的主机接收并确认。而&lt;/strong&gt;实时应用&lt;/strong&gt;通常要求最小的发送速率，不希望过分地延迟报文段的传送，且能容忍一些数据的丢失，TCP服务模型并不是特别适合这些应用的需要。这些应用可以使用UDP，并作为应用的一部分来&lt;strong&gt;实现所需、超出UDP的不提供不必要的报文段交付服务之外的额外功能。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;（2）**无须建立连接。**UDP不需要任何准备即可进行数据传输，因此UDP不会引入建立连接的时延(这可能是DNS运行在UDP上的主要原因)。&lt;/p&gt;
&lt;p&gt;（3）**无连接状态。**TCP需要在端系统中维护连接状态，该状态包括缓存、拥塞控制参数以及序号确认序号。**而UDP不需要维护连接状态，**也不跟踪这些参数。因此，某些专门用于某种特定应用的服务器当应用程序运行在UDP之上而不是运行在TCP上时，&lt;strong&gt;一般都能支持更多活跃客户。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;（4）**分组首部开销小。**每个TCP报文段都有20个字节的首部开销，而UDP仅有8字节的开销。&lt;/p&gt;
&lt;p&gt;UDP不提供可靠数据传输服务，但是使用UDP的应用是可能实现可靠数据传输的。这&lt;strong&gt;可以通过在应用进程自身中建立可靠性机制来完成&lt;/strong&gt;，（例如，可通过增加确认和重传机制来实现）如谷歌浏览器中的QUIC协议就是在UDP之上的应用层协议实现了可靠性。这样做可以使应用进程进行可靠通信，而无需受制于TCP的拥塞控制机制带来的传输速率限制。&lt;/p&gt;
&lt;h4 id=&#34;udp-segment-structure&#34;&gt;UDP Segment Structure&lt;/h4&gt;
&lt;p&gt;UDP首部只有4个字段，每个字段由两个字节组成。源端口号和目的端口号在复用/分解时被使用；长度字段指示了在UDP报文段中的字节数(首部+数据)，因为数据字段的长度在一个UDP段中不同于在其他层封装后的段中，所以需要一个明确长度；接收方使用校验和来检查在该报文段是否出现差错。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153826665-627460470.png&#34;&gt;&lt;img src=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153849397-1476776484.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;​           图2.1 UDP报文段结构&lt;/p&gt;
&lt;h4 id=&#34;udp-checksum&#34;&gt;UDP Checksum&lt;/h4&gt;
&lt;p&gt;UDP提供了差错检测功能，具体为发送方的UDP对报文段中的所有16比特字段的和进行反码运算，求和时遇到的任何溢出都被回卷。得到的结果被放在UDP报文段中的检验和字段。&lt;/p&gt;
&lt;p&gt;用一个例子说明计算过程，有三个16比特：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153900674-1463253988.png&#34;&gt;&lt;img src=&#34;https://img2020.cnblogs.com/blog/1550970/202011/1550970-20201126153904492-1464881034.png&#34; alt=&#34;clip_image002&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最后一次加法有溢出，它要被回卷，其结果进行反码运算后得到(1011010100111101)校验和。在接收方，包括校验和的全部的4个16比特加在一起，若没有出现差错，最后结果将是(1111111111111111)。&lt;/p&gt;
&lt;p&gt;UDP提供端到端的差错检测，但不能对差错进行恢复，某些实现是将受损报文段丢弃，其他实现是将受损报文段交给应用程序并给出警告。&lt;/p&gt;
&lt;h2 id=&#34;principles-of-reliable-data-transfer&#34;&gt;Principles of Reliable Data Transfer&lt;/h2&gt;
&lt;h4 id=&#34;building-a-reliable-data-transfer-protocol&#34;&gt;Building a Reliable Data Transfer Protocol&lt;/h4&gt;
&lt;h4 id=&#34;pipelined-reliable-data-transfer-protocols&#34;&gt;Pipelined Reliable Data Transfer Protocols&lt;/h4&gt;
&lt;h4 id=&#34;go-back-n-gbn&#34;&gt;Go-Back-N (GBN)&lt;/h4&gt;
&lt;h4 id=&#34;selective-repeat-sr&#34;&gt;Selective Repeat (SR)&lt;/h4&gt;
&lt;h2 id=&#34;connection-oriented-transport-tcp&#34;&gt;Connection-Oriented Transport: TCP&lt;/h2&gt;
&lt;h4 id=&#34;the-tcp-connection&#34;&gt;The TCP Connection&lt;/h4&gt;
&lt;h4 id=&#34;tcp-segment-structure&#34;&gt;TCP Segment Structure&lt;/h4&gt;
&lt;h4 id=&#34;round-trip-time-estimation-and-timeout&#34;&gt;Round-Trip Time Estimation and Timeout&lt;/h4&gt;
&lt;h4 id=&#34;reliable-data-transfer&#34;&gt;Reliable Data Transfer&lt;/h4&gt;
&lt;h4 id=&#34;flow-control&#34;&gt;Flow Control&lt;/h4&gt;
&lt;h4 id=&#34;tcp-connection-management&#34;&gt;TCP Connection Management&lt;/h4&gt;
&lt;h2 id=&#34;principles-of-congestion-control&#34;&gt;Principles of Congestion Control&lt;/h2&gt;
&lt;h4 id=&#34;the-causes-and-the-costs-of-congestion&#34;&gt;The Causes and the Costs of Congestion&lt;/h4&gt;
&lt;h4 id=&#34;approaches-to-congestion-control&#34;&gt;Approaches to Congestion Control&lt;/h4&gt;
&lt;h4 id=&#34;network-assisted-congestion-control-example-atm-abr-congestion-control&#34;&gt;Network-Assisted Congestion-Control Example: ATM ABR Congestion Control&lt;/h4&gt;
&lt;h2 id=&#34;tcp-congestion-control&#34;&gt;TCP Congestion Control&lt;/h2&gt;
&lt;h4 id=&#34;fairness&#34;&gt;Fairness&lt;/h4&gt;
&lt;h2 id=&#34;programming-assignments&#34;&gt;Programming Assignments&lt;/h2&gt;
&lt;h2 id=&#34;wireshark-lab-exploring-tcp&#34;&gt;Wireshark Lab: Exploring TCP&lt;/h2&gt;
&lt;h2 id=&#34;wireshark-lab-exploring-udp&#34;&gt;Wireshark Lab: Exploring UDP&lt;/h2&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pearson.com/us/higher-education/product/Kurose-Computer-Networking-A-Top-Down-Approach-6th-Edition/9780132856201.html?tab=contents&#34;&gt;Computer Networking: A Top-Down Approach, 6th Edition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Application Layer</title>
      <link>http://www.yezheng.pro/post/specialization/computer-networking/computer-networks-2/</link>
      <pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/computer-networking/computer-networks-2/</guid>
      
        <description>&lt;h2 id=&#34;principles-of-network-applications&#34;&gt;Principles of Network Applications&lt;/h2&gt;
&lt;p&gt;研究网络应用程序的核心是写出能够运行在不同的端系统和通过网络彼此通信的程序。不需要写在网络核心设备如路由器或链路层交换机上运行的软件。&lt;/p&gt;
&lt;h4 id=&#34;network-application-architectures&#34;&gt;Network Application Architectures&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;在**客户-服务器体系结构 (client-server architecture)**中，有一个总是打开的主机称为服务器，它服务于来自许多其他称为客户的主机的请求。著名的应用程序包括Web、FTP、Telnet和电子邮件。&lt;/li&gt;
&lt;li&gt;在一个&lt;strong&gt;P2P体系结构(P2P architecture)&lt;strong&gt;中,对位于数据中心的专用服务器有最小的(或者没有)依赖。相反，应用程序在间断连接的主机对之间使用直接通信，这些主机对被称为&lt;/strong&gt;对等方&lt;/strong&gt;。这些应用包括文件共享（例如BitTorrent）、对等放协助下载加速器（如迅雷）、因特网电话（例如Skype）和IPTV（如迅雷看看）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;processes-communicating&#34;&gt;Processes Communicating&lt;/h4&gt;
&lt;p&gt;在操作系统术语中，进行通信的实际上是&lt;strong&gt;进程（process）&lt;/strong&gt;。一个进程可以被认为是运行在端系统中的一个程序。当程序运行在相同的端系统上时，它们使用进程间通信机制相互通信。在不同端系统上的进程，通过跨越计算机网络交换报文（message）而相互通信。&lt;/p&gt;
&lt;h6 id=&#34;客户与服务器进程&#34;&gt;客户与服务器进程&lt;/h6&gt;
&lt;p&gt;在一对进程之间的通信会话场景中，发起通信(即在该会话开始时发起与其他进程的联系)的进程被标识为客户（client），在会话开始时等待联系的进程是服务器（server）。&lt;/p&gt;
&lt;h6 id=&#34;进程与计算机网络之间的接口&#34;&gt;进程与计算机网络之间的接口&lt;/h6&gt;
&lt;p&gt;进程通过一个称为**套接字(socket)**的软件接口向网络发送报文和从网络接收报文。应用程序可以控制套接字在应用层端的一切，但是对该套接字的运输层几片没有控制权。除选择运输层协议与设定几个运输层参数，如最大缓存和最大报文段长度等。&lt;/p&gt;
&lt;h6 id=&#34;进程寻址&#34;&gt;进程寻址&lt;/h6&gt;
&lt;p&gt;为了标识接受进程的地址，需要定义两种信息，主机的地址和定义在目的主机中的接收进程的标识符。主机友IP地址（IP address）标识。目的地端口号标识进程。&lt;/p&gt;
&lt;p&gt;Web服务器用端口号80来标识，邮件服务器进程(使用SMTP协议)用端口号25来标识。&lt;/p&gt;
&lt;h4 id=&#34;transport-services-available-to-applications&#34;&gt;Transport Services Available to Applications&lt;/h4&gt;
&lt;p&gt;一个运输层协议能够为调用它的应用程序提供什么样的服务呢？从四个方面对应用程序服务要求进行分类：可靠数据传输、吞吐量、定时、安全性.&lt;/p&gt;
&lt;h4 id=&#34;transport-services-provided-by-the-internet&#34;&gt;Transport Services Provided by the Internet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;TCP服务&lt;/p&gt;
&lt;p&gt;TCP服务模型包括&lt;strong&gt;面向连接服务&lt;/strong&gt;和&lt;strong&gt;可靠数据传输&lt;/strong&gt;服务，TCP还提供&lt;strong&gt;拥塞控制&lt;/strong&gt;机制&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;面向连接：客户机/服务器进程间需建立连接。握手过程提示客户和服务器，使它们为大量分组的到来做好准备。在握手阶段后，一个TCP连接（TCP connection）就在两个进程的套接字之间建立了。这条连接是全双工的，即连接双方的进程可以在此连接上同时进行报文收发。&lt;/li&gt;
&lt;li&gt;可靠的传输：通信进程能够依靠TCP，无差错、按适当顺序交付所有发送的数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;UDP服务&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　UDP是一种不提供不需要服务的轻量级运输协议，无连接：它不需在两主机间建立连接，提供&lt;strong&gt;不可靠的数据传输&lt;/strong&gt;，UDP协议并不保证该报文将到达接收进程。不仅如此，到达接收进程的报文也可能是乱序到达的。没有包括拥塞控制机制。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因特网运输协议所不提供的服务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;无论TCP还是UDP都没有提供任何加密机制，TCP的加强版本称为&lt;strong&gt;安全套接字层（Secure Sockets Layer, SSL）&lt;/strong&gt;，包括加密、数据完整性和端点鉴别。SSL不是与TCP和UDP在相同层次上的第三种运输协议，**是在应用层上实现的强化。**如果一个应用要使用SSL的服务，需要在该应用程序的客户端和服务端包括SSL代码（利用现有的、高度优化的库和类）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pic2.zhimg.com/80/v2-a62b18a6ceee21254425f68cba7496f5_1440w.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;application-layer-protocols&#34;&gt;Application-Layer Protocols&lt;/h4&gt;
&lt;p&gt;应用层协议(application-layer protocol)定义了运行在不同 端系统上的应用程序进程如何相互传递报文&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;公开协议：由RFC定义，Web的应用层协议是HTTP,它定义了在浏览器和Web服务器之间传输的报文格式和序列。用于电子邮件的主要应用层协议就是SMTP&lt;/li&gt;
&lt;li&gt;私有协议：多数P2P文件共享应用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-web-and-http&#34;&gt;The Web and HTTP&lt;/h2&gt;
&lt;h4 id=&#34;overview-of-http&#34;&gt;Overview of HTTP&lt;/h4&gt;
&lt;p&gt;Web的应用层协议是&lt;strong&gt;超文本传输协议(HyperText Transfer Protocol, HTTP)&lt;/strong&gt;,它是Web 的核心，在[RFC 1945]和[RFC 2616]中进行了定义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;web页面=多个对象组成（对象只是一个文件，如一个html文件，一个图形，一个java小程序或一个视频）。每个对象通过一个对应的URL寻址。&lt;/li&gt;
&lt;li&gt;HTTP使用TCP作为它的支撑运输协议。&lt;/li&gt;
&lt;li&gt;HTTP客户首先发起一个与服务器的TCP连接。一旦连接建立，该浏览器和服务器进程就可以通过套接字接口访问TCP 。&lt;/li&gt;
&lt;li&gt;一旦客户向它的套接字接口发送了一个请求报文，该报文就脱离了客户控制并进入TCP的控制。 因为TCP协议，一个客户进程发出的每个HTTP请求报文最终能完整地到达服务器&lt;/li&gt;
&lt;li&gt;HTTP服务器并不保存关于客户的任何信息，所以我们说HTTP是一个无状态协议(stateless protocol)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;non-persistent-and-persistent-connections&#34;&gt;Non-Persistent and Persistent Connections&lt;/h4&gt;
&lt;p&gt;每个请求/响应对是经一个单独的TCP连接发送，还是所有的请求及其响应经相同的TCP连接发送呢?采用前一种方法，该应用程序被称为使用非持续连接(non-persistent connection); 采用后一种方法，该应用程序被称为使用持续连接(persistent connection) 。&lt;/p&gt;
&lt;p&gt;使用持续连接或非持续连接是由应用层决定的（HTTP默认使用的是持续连接）&lt;/p&gt;
&lt;h6 id=&#34;采用非持续连接的http&#34;&gt;采用非持续连接的HTTP&lt;/h6&gt;
&lt;p&gt;每个TCP连接只传输一个请求报文和一个响应报文。每次服务器发送响应报文后，会通知该TCP断开该TCP连接。HTTP客户接收响应报文，TCP连接关闭。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pic3.zhimg.com/80/v2-7172e4c449549b0887fe70294748815e_1440w.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;客户向服务器发送一个TCP报文段，服务器用TCP报文段做出响应，客户向服务器返回确认。三次握手中前两个部分所耗费时间占一个RTT。完成了三次握手的前两部分后，客户结合三次握手的第三部分(确认)向该TCP连接发送一个请求报文。一旦请求报文到达服务器，服务器就在服务器就在该TCP连接上发送 HTML文件。TCP三次握手以及时间分析，总计两个往返时间RTT（Round-Trip Time, RTT）+服务器传输Html文件的时间。因此非持续连接是非常低效率的。&lt;/p&gt;
&lt;p&gt;非持续连接的两个缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;要为每个请求的对象建立一个连接，对于每个连接，客户和服务器中都要分配TCP缓冲区和保持TCP变量，给服务器带来严重负担；&lt;/li&gt;
&lt;li&gt;每个对象都要遭受两倍RTT的交付时延，一个RTT用来创建TCP，另一个RTT用来请求和接受一个对象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;采用持续连接的http&#34;&gt;采用持续连接的HTTP&lt;/h6&gt;
&lt;p&gt;服务器在发送响应之后保持该TCP连接打开。因此，位于同一台服务器的多个Web页面都可以在单个TCP上进行传输。请求可以一个接一个地发而不必等待未决请求的回答（流水线）。如果一条连接经过一定时间间隔（一个可配置的超时间隔）仍未被使用，HTTP服务器就关闭该连接。&lt;/p&gt;
&lt;p&gt;HTTTP的默认模式是使用带流水线的持续连接。&lt;/p&gt;
&lt;h4 id=&#34;http-message-format&#34;&gt;HTTP Message Format&lt;/h4&gt;
&lt;h6 id=&#34;http请求报文&#34;&gt;HTTP请求报文&lt;/h6&gt;
&lt;p&gt;一个典型的http请求报文：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET /somedir/page.html HTTP/1.1
Host: www.hans941.tk
Connection: close
User-agent: Chrome/57.0
Accept-language: ch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Host首部行是Web代理高速缓存所要求的。Connection:close高速浏览器不希望使用持续连接，服务器发送完请求对象后就关闭连接。&lt;/p&gt;
&lt;p&gt;User-agent服务器可以为不同类型的用户发送相同对象的不同的版本。&lt;/p&gt;
&lt;p&gt;下图是请求报文的通用格式&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img src=&#34;https://img2018.cnblogs.com/blog/226988/201905/226988-20190525102530882-1860346972.jpg&#34; alt=&#34;&#34;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首部行后面的 实体体(Entity body),在使用 POST 方法时存储用户提交的表单。&lt;/p&gt;
&lt;h6 id=&#34;http响应报文&#34;&gt;HTTP响应报文&lt;/h6&gt;
&lt;p&gt;http响应报文由三部分组成：一个初始状态行（status line），之后有6个首部行（header line），然后是实体体（entity body）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HTTP/1.1 200 OK
Connection: close
Date: Tue, 09 Aug 2011 15:44:04 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 09 Aug 2011 15:11:03 GMT
Content-Length: 6821
Content-Type: text/html

(data data data data data ......)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;服务器永Connection: close首部行高速客户，发送完报文后就关闭该TCP连接。Date首部行指的是服务器从文件系统中检索到对象，插入响应报文的时间，而不是对象创建或最后修改的时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Last-Modified首部行指示了对象创建或者最后修改的日期和时间，对既可能在本地客户也可能在网络缓存服务器上的对象缓存来说非常重要。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个HTTP响应报文的通用格式&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://img2018.cnblogs.com/blog/226988/201905/226988-20190525103019639-986868984.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;常见状态码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;200 OK&lt;/li&gt;
&lt;li&gt;301 Moved Permanently&lt;/li&gt;
&lt;li&gt;400 Bad Request&lt;/li&gt;
&lt;li&gt;404 Not Found&lt;/li&gt;
&lt;li&gt;505 HTTP Version Not Supported&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;user-server-interaction-cookies&#34;&gt;User-Server Interaction: Cookies&lt;/h4&gt;
&lt;p&gt;HTTP服务器是无状态化的，这简化了服务器的设计，这让工程师可以去开发能同时处理大量数据的高性能服务器。但是有时Web站点希望能去识别用户，为此HTTP使用了cookie来进行用户跟踪。&lt;/p&gt;
&lt;p&gt;　　cookie技术有4个组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 HTTP 响应报文中的一个 cookie 首部行；&lt;/li&gt;
&lt;li&gt;在 HTTP 请求报文中的一个 cookie 首部行；&lt;/li&gt;
&lt;li&gt;在用户端系统中保留有一个 cookie 文件，并由用户的浏览器进行管理；&lt;/li&gt;
&lt;li&gt;位于 Web 站点的一个后端数据库。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　&lt;img src=&#34;https://img2018.cnblogs.com/blog/226988/201905/226988-20190525103608022-1309036437.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;cookie可以用于标识一个用户，cookie可以在无状态的HTTP之上建立一个用户会话层。&lt;/p&gt;
&lt;p&gt;站点不必知道用户身份，可以记录按什么顺序，在什么时间，访问了哪些页面。能够根据过去访问的网页对用户推荐产品。&lt;/p&gt;
&lt;h4 id=&#34;web-caching&#34;&gt;Web Caching&lt;/h4&gt;
&lt;p&gt;Web缓存器(Web cache)也叫代理服务器(proxy server)，它是能够代表初始Web服务器来满足HTTP请求的网络实体.Web缓存器有自己的磁盘存储空间，并在存储空间中保存最近请求过的对象的副本。&lt;/p&gt;
&lt;p&gt;客户对过Web缓存器请求对象:&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&#34;https://img2018.cnblogs.com/blog/226988/201905/226988-20190525104346988-822885012.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;浏览器建立一个到Web缓存器的TCP连接，并向Web缓存器中的对象发送一个HTTP请求。&lt;/li&gt;
&lt;li&gt;Web缓存器进行检查，看看本地是否缓存了该对象副本。如果有，Web缓存器就向客户浏览器用HTTP响应报文返回该对象。&lt;/li&gt;
&lt;li&gt;如果web缓存器没有该对象，就打开一个与该对象的初始服务器的TCP连接，Web缓存器向初始服务器的TCP连接上发送一个HTTP请求。&lt;/li&gt;
&lt;li&gt;Web缓存器收到该对象，本地存储空间存储副本，并向客户浏览器利用现有TCP连接发送HTTP响应包含该副本&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Web缓存器既是服务器又是客户。当它接收浏览器的请求并发回响应 时，它是一个服务器。当它向初始服务器发出请求并接收响应时，它是一个客户。&lt;/p&gt;
&lt;p&gt;Web缓存器通常由ISP购买并安装。Web缓存器可以减少成本，降低响应时间。&lt;/p&gt;
&lt;p&gt;通过使用内容分发网络（Content Distribution Network, CDN），Web缓存服务器正在因特网中发挥着越来越重要的作用。&lt;/p&gt;
&lt;h4 id=&#34;the-conditional-get&#34;&gt;The Conditional GET&lt;/h4&gt;
&lt;p&gt;那么Web缓存器什么时候才会去刷新缓存下来的页面，以保存提供给客户是最新的对象呢？&lt;/p&gt;
&lt;p&gt;HTTP协议有一处机制&amp;mdash;&amp;ndash;条件GET(conditional GET)方法。&lt;/p&gt;
&lt;p&gt;若同时满足以下两点的则称为 条件 GET 方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;请求报文使用 GET 方法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;请求报文中包含一个 &lt;strong&gt;If-Modified-Since&lt;/strong&gt;: 首部行。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原始服务器向Web缓存器中返回HTTP响应时包含&lt;strong&gt;Last-Modified&lt;/strong&gt;首部行，缓存器在存储该对象的同事也存储了最后修改日期。&lt;/p&gt;
&lt;p&gt;Web缓存器向原始Web服务器发送请求：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET / HTTP/1.1
Host: www.sina.com.cn:80
If-Modified-Since:Thu, 4 Feb 2010 20:39:13 GMT
Connection: Close
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Web服务器向缓存器发送一个响应报文&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HTTP/1.0 304 Not Modified
Date: Thu, 04 Feb 2010 12:38:41 GMT

(empty entity body)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&amp;ldquo;Not Modified&amp;rdquo;，没有在响应报文中包含所请求的对象，高速缓存器可以使用该对象，向请求浏览器转发代理缓存器缓存的对象副本。&lt;/p&gt;
&lt;h2 id=&#34;dnsthe-internets-directory-service&#34;&gt;DNS–The Internet’s Directory Service&lt;/h2&gt;
&lt;h4 id=&#34;services-provided-by-dns&#34;&gt;Services Provided by DNS&lt;/h4&gt;
&lt;h4 id=&#34;overview-of-how-dns-works&#34;&gt;Overview of How DNS Works&lt;/h4&gt;
&lt;h4 id=&#34;dns-records-and-messages&#34;&gt;DNS Records and Messages&lt;/h4&gt;
&lt;h2 id=&#34;peer-to-peer-applications&#34;&gt;Peer-to-Peer Applications&lt;/h2&gt;
&lt;h4 id=&#34;p2p-file-distribution&#34;&gt;P2P File Distribution&lt;/h4&gt;
&lt;h4 id=&#34;distributed-hash-tables-dhts&#34;&gt;Distributed Hash Tables (DHTs)&lt;/h4&gt;
&lt;h4 id=&#34;case-study-p2p-internet-telephony-with-skype&#34;&gt;Case Study: P2P Internet Telephony with Skype&lt;/h4&gt;
&lt;h2 id=&#34;socket-programming-creating-network-applications&#34;&gt;Socket Programming: Creating Network Applications&lt;/h2&gt;
&lt;p&gt;网络应用程序有两类。&lt;strong&gt;一类是实现在协议标准&lt;/strong&gt;（如一个RFC或某种其他标准文档）&lt;strong&gt;中所定义的操作；这样的应用程序又称为“开放”的&lt;/strong&gt;，因为定义其操作的这些规则人所共知。对于这样的实现，客户程序和服务器程序必须遵守由该RFC所规定的规则。如果一个开发者编写客户程序的代码，另一个开发者编写服务器程序的代码，并且两者都完全遵从该RFC的各种规则，那么这两个程序将能够交互操作。&lt;/p&gt;
&lt;p&gt;另一类网络应用程序是&lt;strong&gt;专用的网络应用程序&lt;/strong&gt;。在这种情况下，由客户和服务器程序应用的应用层协议没有公开发布在某RFC中或其他地方。某单独的开发者（或开发团队）创建了客户和服务器程序，并且该开发者用他的代码完全控制程序的功能。但是因为这些代码并没有实现一个开放的协议，其他独立的开发者将不能开发出和该应用程序交互的代码。&lt;/p&gt;
&lt;p&gt;在研发阶段，开发者必须最先做的一个决定是，应用程序是运行在TCP上还是运行在UDP上。前面讲过TCP是面向连接的，并且为两个端系统之间的数据流动提供可靠的字节流通道。UDP是无连接的，从一个端系统向另一个端系统发送独立的数据分组，不对交付提供任何保证。前面也讲过当客户或服务器程序&lt;strong&gt;实现了一个由某RFC定义的协议，它应当使用与该协议关联的周知端口号&lt;/strong&gt;；与之相反，&lt;strong&gt;当研发一个专用应用程序，研发者必须注意避免使用这样的周知端口号&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;socket-programming-with-udp&#34;&gt;Socket Programming with UDP&lt;/h4&gt;
&lt;p&gt;应用程序开发者在套接字的应用层一侧可以控制所有东西；然而，它几乎无法控制运输层一侧。&lt;/p&gt;
&lt;p&gt;在发送进程能够将数据分组推出套接字之门之前，当使用UDP时，必须先将目的地址附在该分组之上。在该分组传过发送方的套接字之后，因特网将使用该目的地址通过因特网为该分组选路到接收进程的套接字。当分组到达接收套接字时，接收进程将通过该套接字取回分组，进而检查分组的内容并采取适当的动作。&lt;/p&gt;
&lt;p&gt;当生成一个套接字时，就为它分配一个称为端口号（port number）的标识符。因此，如你所期待的，分组的目的地址也包括该套接字的端口号。&lt;/p&gt;
&lt;p&gt;归纳起来，&lt;strong&gt;发送进程为分组附上的目的地址是由目的主机的IP地址和目的地套接字的端口号组成的&lt;/strong&gt;。此外，如我们很快将看到的那样，发送方的源地址也是由源主机的IP地址和源套接字的端口号组成，该源地址也要附在分组之上。然而，将源地址附在分组之上通常并不是由UDP应用程序代码所为，而是由底层操作系统自动完成的。&lt;/p&gt;
&lt;p&gt;下显示了客户和服务器的主要与套接字相关的活动，两者通过UDP运输服务进行通信。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yqfile.alicdn.com/f3a0f4417d3d8aa377ee4f987f52623a4031c0fc.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;客户程序被称为UDPClient.py，服务器程序被称为UDPServer.py。为了强调关键问题，我们有意提供最少的代码。“好代码”无疑将具有一些更为辅助性的代码行，特别是用于处理出现差错的情况。&lt;/p&gt;
&lt;h6 id=&#34;udpclientpy&#34;&gt;UDPClient.py&lt;/h6&gt;
&lt;p&gt;下面是该应用程序客户端的代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from socket import *

# 服务器的P地址（如“128.138.32.126”）或者服务器的主机名（如“cis.poly.edu”）的字符串, 如果我们使用主机名，则将自动执行DNS lookup从而得到IP地址
serverName = ‘hostname’                                           
serverPort = 12000 

# 创建了客户的套接字，第一个参数指示了地址簇；特别是，AF_INET指示了底层网络使用了IPv4。
# 第二个参数指示了该套接字是SOCK_DGRAM类型的，这意味着它是一个UDP套接字
# 没有指定客户套接字的端口号,操作系统会设置
clientSocket = socket(AF_INET, SOCK_DGRAM) 

message = raw_input(’Input lowercase sentence:’) 

# 为报文附上目的地址，并且向进程的套接字clientSocket发送结果分组
# 源地址也附到分组上，尽管这是自动完成的，而不是显式地由代码完成的。
clientSocket.sendto(message.encode(),(serverName, serverPort)) 

# 方法recvfrom也取缓存长度2048作为输入
modifiedMessage, serverAddress = clientSocket.recvfrom(2048) 

print modifiedMessage.decode() 

# 关闭套接字
clientSocket.close()
&lt;/code&gt;&lt;/pre&gt;&lt;h6 id=&#34;udpserverpy&#34;&gt;UDPServer.py&lt;/h6&gt;
&lt;p&gt;现在来看看这个应用程序的服务器端：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from socket import * 

serverPort = 12000 

# 创建套接字类型SOCK_DGRAM（一种UDP套接字）
serverSocket = socket(AF_INET, SOCK_DGRAM) 

# 将端口号12000与个服务器的套接字绑定（即分配）在一起,代码显式地为该套接字分配一个端口号
serverSocket.bind((&#39;&#39;, serverPort)) 

print (“The server is ready to receive”) 

while True: 
    # 当某分组到达该服务器的套接字时，该分组的数据被放置到变量message中，其源地址被放置到变量clientAddress中。
	message, clientAddress = serverSocket.recvfrom(2048) 
	modifiedMessage = message.decode().upper() 
	# UDPServer将利用该源地址信息返回
	# 将该客户的地址（IP地址和端口号）附到大写报文上，并将所得的分组发送到服务器的套接字中。
	# 服务器地址也附在分组上，尽管这是自动而不是显式地由代码完成的。
	serverSocket.sendto(modifiedMessage.encode(), clientAddress)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了测试这对程序，在服务器主机上执行编译的服务器程序UDPServer.py。这在服务器上创建了一个进程，等待着某个客户与之联系。然后，保证在UDPClient.py中包括适当的服务器主机名或IP地址，在客户主机上执行编译的客户器程序UDPClient.py。这在客户上创建了一个进程。最后，在客户上使用应用程序，键入一个句子并以回车结束。&lt;/p&gt;
&lt;h4 id=&#34;socket-programming-with-tcp&#34;&gt;Socket Programming with TCP&lt;/h4&gt;
&lt;p&gt;与UDP不同，TCP是一个面向连接的协议。这意味着在客户和服务器能够开始互相发送数据之前，它们先要握手和创建一个TCP连接。TCP连接的一端与客户套接字相联系，另一端与服务器套接字相联系。当创建该TCP连接时，我们将其与客户套接字地址（IP地址和端口号）和服务器套接字地址（IP地址和端口号）关联起来。使用创建的TCP连接，当一侧要向另一侧发送数据时，它只需经过其套接字将数据丢给TCP连接。这与UDP不同，UDP服务器在将分组丢进套接字之前必须为其附上一个目的地地址。
现在我们仔细观察一下TCP中客户程序和服务器程序的交互。客户具有向服务器发起接触的任务。服务器为了能够对客户的初始接触做出反应，服务器必须已经准备好。这意味着两件事。第一，与在UDP中的情况一样，TCP服务器在客户试图发起接触前必须作为进程运行起来。第二，服务器程序必须具有一扇特殊的门，更精确地说是一个特殊的套接字，该门欢迎来自运行在任意主机上的客户进程的某些初始接触。使用房子/门来比喻进程/套接字，有时我们将客户的初始接触称为“敲欢迎之门”。
随着服务器进程的运行，客户进程能够向服务器发起一个TCP连接。这是由客户程序通过创建一个TCP套接字完成的。当该客户生成其TCP套接字时，它指定了服务器中的欢迎套接字的地址，即服务器主机的IP地址及其套接字的端口号。生成其套接字后，该客户发起了一个三次握手并创建与服务器的一个TCP连接。发生在运输层的三次握手，对于客户和服务器程序是完全透明的。
在三次握手期间，客户进程敲服务器进程的欢迎之门。当该服务器“听”到敲门时，它将生成一扇新门（更精确地讲是一个新套接字），它专门用于特定的客户。在我们下面的例子中，欢迎之门是一个我们称为serverSocket的TCP套接字对象；它专门对客户进行连接的新生成的套接字，称为连接套接字（connection Socket）。初次遇到TCP套接字的学生有时会混淆欢迎套接字（这是所有要与服务器通信的客户的起始接触点）和每个新生成的服务器侧的连接套接字（这是随后为与每个客户通信而生成的套接字）。
从应用程序的观点来看，客户套接字和服务器连接套接字直接通过一根管道连接。如图2-29所示，客户进程可以向它的套接字发送任意字节，并且TCP保证服务器进程能够按发送的顺序接收（通过连接套接字）每个字节。TCP因此在客户和服务器进程之间提供了可靠服务。此外，就像人们可以从同一扇门进和出一样，客户进程不仅能向它的套接字发送字节，也能从中接收字节；类似地，服务器进程不仅从它的连接套接字接收字节，也能向其发送字节。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yqfile.alicdn.com/449d86f1fb6c99e9dda16b5e075cf1e77ec2289a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;下图显示了客户和服务器的主要与套接字相关的活动，两者通过TCP运输服务进行通信。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yqfile.alicdn.com/4054507a62b63bc6c0ce7c4ac9496aa6979190b6.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h6 id=&#34;tcpclientpy&#34;&gt;TCPClient.py&lt;/h6&gt;
&lt;p&gt;这里给出了应用程序客户端的代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from socket import * 

serverName = ’servername’ 
serverPort = 12000 

# 第一个参数仍指示底层网络使用的是IPv4。第二个参数指示该套接字是SOCK_STREAM类型。这表明它是一个TCP套接字（而不是一个UDP套接字）
# 创建该客户套接字时仍未指定其端口号；操作系统会自动附上端口号
clientSocket = socket(AF_INET, SOCK_STREAM) 

# 前面讲过在客户能够使用一个TCP套接字向服务器发送数据之前（反之亦然），必须在客户与服务器之间创建一个TCP连接。
# 这行发起了客户和服务器之间的这条TCP连接。这行代码执行完后，执行三次握手，并在客户和服务器之间创建起一条TCP连接。
clientSocket.connect((serverName,serverPort)) 

sentence = raw_input(‘Input lowercase sentence:’) 

# 该程序并未显式地创建一个分组并为该分组附上目的地址，而使用UDP套接字却要那样做。客户程序只是将字符串sentence中的字节放入该TCP连接中去。
clientSocket.send(sentence.encode()) 

# 等待接收来自服务器的字节，字符继续积累在modifiedSentence中，直到收到回车符才会结束该行。
modifiedSentence = clientSocket.recv(1024) 

print(‘From Server:’, modifiedSentence.decode()) 

# 关闭客户的套接字，因此关闭了客户和服务器之间的TCP连接。它引起客户中的TCP向服务器中的TCP发送一条TCP报文
clientSocket.close()
&lt;/code&gt;&lt;/pre&gt;&lt;h6 id=&#34;tcpserverpy&#34;&gt;TCPServer.py&lt;/h6&gt;
&lt;p&gt;现在我们看一下服务器程序&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from socket import * 

serverPort = 12000 

# 服务器创建一个TCP套接字
serverSocket = socket(AF_INET,SOCK_STREAM) 

# 将服务器的端口号serverPort与该套接字关联起来
serverSocket.bind((‘’,serverPort)) 

# 但对TCP而言，serverSocket将是我们的欢迎套接字
# 该行让服务器聆听来自客户的TCP连接请求。其中参数定义了请求连接的最大数（至少为1）
serverSocket.listen(1) 

print ‘The server is ready to receive’ 

while True: 
    # 当客户敲该门时，程序为serverSocket调用accept()，这在服务器中创建了一个称为connectionSocket的新套接字，由这个特定的客户专用。
    # 客户和服务器则完成了握手，在客户的clientSocket和服务器的serverSocket之间创建了一个TCP连接。
    connectionSocket, addr = serverSocket.accept() 
    
    # 借助于创建的TCP连接，客户与服务器现在能够通过该连接相互发送字节。
    sentence = connectionSocket.recv(1024).decode() 
    
    capitalizedSentence = sentence.upper() 
    
    # 使用TCP，从一侧发送的所有字节不仅确保到达另一侧，而且确保按序到达。
    connectionSocket.send(capitalizedSentence)
    
    # 关闭了该连接套接字,但由于serverSocket保持打开，所以另一个客户此时能够敲门并向该服务器发送一个句子要求修改。
    connectionSocket.close()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在两台单独的主机上运行这两个程序，也可以修改它们以达到稍微不同的目的。你应当将前面两个UDP程序与这两个TCP程序进行比较，观察它们的不同之处。&lt;/p&gt;
&lt;h2 id=&#34;socket-programming-assignments&#34;&gt;Socket Programming Assignments&lt;/h2&gt;
&lt;p&gt;The companion Web site includes six socket programming assignments. The first four assignments are summarized below. The fifth assignment makes use of the ICMP protocol and is summarized at the end of Chapter 4. The sixth assignment employs multimedia protocols and is summarized at the end of Chapter 7. It is highly recommended that students complete several, if not all, of these assignments.  Students can find full details of these assignments, as well as important snippets of the Python code, at the Web site &lt;a href=&#34;http://www.awl.com/kurose-ross&#34;&gt;http://www.awl.com/kurose-ross&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;assignment-1-web-server&#34;&gt;Assignment 1: Web Server&lt;/h4&gt;
&lt;p&gt;In this assignment, you will develop a simple Web server in Python that is capable of processing only one request. Specifically, your Web server will (i) create a con- nection socket when contacted by a client (browser); (ii) receive the HTTP request from this connection; (iii) parse the request to determine the specific file being requested; (iv) get the requested file from the server’s file system; (v) create an HTTP response message consisting of the requested file preceded by header lines; and (vi) send the response over the TCP connection to the requesting browser. If a browser requests a file that is not present in your server, your server should return a “404 Not Found” error message.&lt;/p&gt;
&lt;p&gt;In the companion Web site, we provide the skeleton code for your server. Your job is to complete the code, run your server, and then test your server by sending requests from browsers running on different hosts. If you run your server on a host that already has a Web server running on it, then you should use a different port than port 80 for your Web server.&lt;/p&gt;
&lt;h4 id=&#34;assignment-2-udp-pinger&#34;&gt;Assignment 2: UDP Pinger&lt;/h4&gt;
&lt;p&gt;In this programming assignment, you will write a client ping program in Python.  Your client will send a simple ping message to a server, receive a corresponding pong message back from the server, and determine the delay between when the client sent the ping message and received the pong message. This delay is called the Round Trip Time (RTT). The functionality provided by the client and server is similar to the functionality provided by standard ping program available in modern operating systems. However, standard ping programs use the Internet Control Mes- sage Protocol (ICMP) (which we will study in Chapter 4). Here we will create a nonstandard (but simple!) UDP-based ping program.  Your ping program is to send 10 ping messages to the target server over UDP.  For each message, your client is to determine and print the RTT when the correspon- ding pong message is returned. Because UDP is an unreliable protocol, a packet sent by the client or server may be lost. For this reason, the client cannot wait indefinitely for a reply to a ping message. You should have the client wait up to one second for a reply from the server; if no reply is received, the client should assume that the packet was lost and print a message accordingly.&lt;/p&gt;
&lt;p&gt;In this assignment, you will be given the complete code for the server (avail- able in the companion Web site). Your job is to write the client code, which will be very similar to the server code. It is recommended that you first study carefully the server code. You can then write your client code, liberally cutting and pasting lines from the server code.&lt;/p&gt;
&lt;h4 id=&#34;assignment-3-mail-client&#34;&gt;Assignment 3: Mail Client&lt;/h4&gt;
&lt;p&gt;The goal of this programming assignment is to create a simple mail client that sends email to any recipient. Your client will need to establish a TCP connection with a mail server (e.g., a Google mail server), dialogue with the mail server using the SMTP protocol, send an email message to a recipient (e.g., your friend) via the mail server, and finally close the TCP connection with the mail server.&lt;/p&gt;
&lt;p&gt;For this assignment, the companion Web site provides the skeleton code for your client. Your job is to complete the code and test your client by sending email to different user accounts. You may also try sending through different servers (for example, through a Google mail server and through your university mail server).&lt;/p&gt;
&lt;h4 id=&#34;assignment-4-multi-threaded-web-proxy&#34;&gt;Assignment 4: Multi-Threaded Web Proxy&lt;/h4&gt;
&lt;p&gt;In this assignment, you will develop a Web proxy. When your proxy receives an HTTP request for an object from a browser, it generates a new HTTP request for the same object and sends it to the origin server. When the proxy receives the corresponding HTTP response with the object from the origin server, it creates a new HTTP response, including the object, and sends it to the client. This proxy will be multi-threaded, so that it will be able to handle multiple requests at the same time.&lt;/p&gt;
&lt;p&gt;For this assignment, the companion Web site provides the skeleton code for the proxy server. Your job is to complete the code, and then test it by having different browsers request Web objects via your proxy.&lt;/p&gt;
&lt;h2 id=&#34;wireshark-lab-http&#34;&gt;Wireshark Lab: HTTP&lt;/h2&gt;
&lt;p&gt;In this lab, we’ll explore several aspects of the HTTP protocol: &lt;strong&gt;the basic GET/response interaction&lt;/strong&gt;, &lt;strong&gt;HTTP message formats&lt;/strong&gt;, &lt;strong&gt;retrieving large HTML files&lt;/strong&gt;, &lt;strong&gt;retrieving HTML files with embedded objects&lt;/strong&gt;, and &lt;strong&gt;HTTP authentication and security&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;the-basic-http-getresponse-interaction&#34;&gt;The Basic HTTP GET/response interaction&lt;/h4&gt;
&lt;p&gt;Let’s begin our exploration of HTTP by downloading a very simple HTML file - one that is very short, and contains no embedded objects. Do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start up your web browser.&lt;/li&gt;
&lt;li&gt;Start up the Wireshark packet sniffer, as described in the Introductory lab (but don’t yet begin packet capture). Enter “http” (just the letters, not the quotation marks) in the display-filter-specification window, so that only captured HTTP messages will be displayed later in the packet-listing window. (We’re only interested in the HTTP protocol here, and don’t want to see the clutter of all captured packets).&lt;/li&gt;
&lt;li&gt;Wait a bit more than one minute (we’ll see why shortly), and then begin Wireshark packet capture.&lt;/li&gt;
&lt;li&gt;Enter the following to your browser &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file1.html&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file1.html&lt;/a&gt; Your browser should display the very simple, one-line HTML file.&lt;/li&gt;
&lt;li&gt;Stop Wireshark packet capture.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Your Wireshark window should look similar to the window shown in Figure 1. If you are unable to run Wireshark on a live network connection, you can download a packet trace that was created when the steps above were followed.&lt;/p&gt;
&lt;p&gt;Figure 1: Wireshark Display after &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/&lt;/a&gt; HTTPwireshark-file1.html has been retrieved by your browser&lt;/p&gt;
&lt;p&gt;The example in Figure 1 shows in the packet-listing window that two HTTP messages were captured: the GET message (from your browser to the gaia.cs.umass.edu web server) and the response message from the server to your browser. The packet-contents window shows details of the selected message (in this case the HTTP OK message, which is highlighted in the packet-listing window). Recall that since the HTTP message was carried inside a TCP segment, which was carried inside an IP datagram, which was carried within an Ethernet frame, Wireshark displays the Frame, Ethernet, IP, and TCP packet information as well. We want to minimize the amount of non-HTTP data displayed (we’re interested in HTTP here, and will be investigating these other protocols is later labs), so make sure the boxes at the far left of the Frame, Ethernet, IP and TCP information have a plus sign or a right-pointing triangle (which means there is hidden, undisplayed information), and the HTTP line has a minus sign or a down-pointing triangle (which means that all information about the HTTP message is displayed).&lt;/p&gt;
&lt;p&gt;Download the zip file &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/wireshark-traces.zip&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/wireshark-traces.zip&lt;/a&gt; and extract the file http-ethereal-trace-1. The traces in this zip file were collected by Wireshark running on one of the author’s computers, while performing the steps indicated in the Wireshark lab. Once you have downloaded the trace, you can load it into Wireshark and view the trace using the File pull down menu, choosing Open, and then selecting the http-ethereal-trace-1 trace file. The resulting display should look similar to Figure 1. (The Wireshark user interface displays just a bit differently on different operating systems, and in different versions of Wireshark).&lt;/p&gt;
&lt;p&gt;(Note: You should ignore any HTTP GET and response for favicon.ico. If you see a reference to this file, it is your browser automatically asking the server if it (the server) has a small icon file that should be displayed next to the displayed URL in your browser. We’ll ignore references to this pesky file in this lab.).&lt;/p&gt;
&lt;p&gt;By looking at the information in the HTTP GET and response messages, answer the following questions. When answering the following questions, you should print out the GET and response messages (see the introductory Wireshark lab for an explanation of how to do this) and indicate where in the message you’ve found the information that answers the following questions. When you hand in your assignment, annotate the output so that it’s clear where in the output you’re getting the information for your answer (e.g., for our classes, we ask that students markup paper copies with a pen, or annotate electronic copies with text in a colored font).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is your browser running HTTP version 1.0 or 1.1? What version of HTTP is the server running?&lt;/li&gt;
&lt;li&gt;What languages (if any) does your browser indicate that it can accept to the server?&lt;/li&gt;
&lt;li&gt;What is the IP address of your computer? Of the gaia.cs.umass.edu server?&lt;/li&gt;
&lt;li&gt;What is the status code returned from the server to your browser?&lt;/li&gt;
&lt;li&gt;When was the HTML file that you are retrieving last modified at the server?R&lt;/li&gt;
&lt;li&gt;How many bytes of content are being returned to your browser?&lt;/li&gt;
&lt;li&gt;By inspecting the raw data in the packet content window, do you see any headers within the data that are not displayed in the packet-listing window? If so, name one.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In your answer to question 5 above, you might have been surprised to find that the document you just retrieved was last modified within a minute before you downloaded the document. That’s because (for this particular file), the gaia.cs.umass.edu server is setting the file’s last-modified time to be the current time, and is doing so once per minute. Thus, if you wait a minute between accesses, the file will appear to have been recently modified, and hence your browser will download a “new” copy of the document.&lt;/p&gt;
&lt;h4 id=&#34;the-http-conditional-getresponse-interaction&#34;&gt;The HTTP CONDITIONAL GET/response interaction&lt;/h4&gt;
&lt;p&gt;Recall from Section 2.2.5 of the text, that most web browsers perform object caching and thus perform a conditional GET when retrieving an HTTP object. Before performing the steps below, make sure your browser’s cache is empty. (To do this under Firefox, select Tools-&amp;gt;Clear Recent History and check the Cache box, or for Internet Explorer, select Tools-&amp;gt;Internet Options-&amp;gt;Delete File; these actions will remove cached files from your browser’s cache.) Now do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start up your web browser, and make sure your browser’s cache is cleared, as discussed above.&lt;/li&gt;
&lt;li&gt;Start up the Wireshark packet sniffer&lt;/li&gt;
&lt;li&gt;Enter the following URL into your browser &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file2.html&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file2.html&lt;/a&gt; Your browser should display a very simple five-line HTML file.&lt;/li&gt;
&lt;li&gt;Quickly enter the same URL into your browser again (or simply select the refresh button on your browser)&lt;/li&gt;
&lt;li&gt;Stop Wireshark packet capture, and enter “http” in the display-filter-specification window, so that only captured HTTP messages will be displayed later in the packet-listing window.&lt;/li&gt;
&lt;li&gt;(Note: If you are unable to run Wireshark on a live network connection, you can use the http-ethereal-trace-2 packet trace to answer the questions below; see footnote 1. This trace file was gathered while performing the steps above on one of the author’s computers.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Answer the following questions:&lt;/p&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Inspect the contents of the first HTTP GET request from your browser to the server. Do you see an “IF-MODIFIED-SINCE” line in the HTTP GET?&lt;/li&gt;
&lt;li&gt;Inspect the contents of the server response. Did the server explicitly return the contents of the file? How can you tell?&lt;/li&gt;
&lt;li&gt;Now inspect the contents of the second HTTP GET request from your browser to the server. Do you see an “IF-MODIFIED-SINCE:” line in the HTTP GET? If so, what information follows the “IF-MODIFIED-SINCE:” header?&lt;/li&gt;
&lt;li&gt;What is the HTTP status code and phrase returned from the server in response to this second HTTP GET? Did the server explicitly return the contents of the file? Explain.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;retrieving-long-documents&#34;&gt;Retrieving Long Documents&lt;/h4&gt;
&lt;p&gt;In our examples thus far, the documents retrieved have been simple and short HTML files. Let’s next see what happens when we download a long HTML file. Do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start up your web browser, and make sure your browser’s cache is cleared, as discussed above.&lt;/li&gt;
&lt;li&gt;Start up the Wireshark packet sniffer&lt;/li&gt;
&lt;li&gt;Enter the following URL into your browser &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file3.html&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file3.html&lt;/a&gt; Your browser should display the rather lengthy US Bill of Rights.&lt;/li&gt;
&lt;li&gt;Stop Wireshark packet capture, and enter “http” in the display-filter-specification window, so that only captured HTTP messages will be displayed.&lt;/li&gt;
&lt;li&gt;(Note: If you are unable to run Wireshark on a live network connection, you can use the http-ethereal-trace-3 packet trace to answer the questions below; see footnote 1. This trace file was gathered while performing the steps above on one of the author’s computers.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the packet-listing window, you should see your HTTP GET message, followed by a multiple-packet TCP response to your HTTP GET request. This multiple-packet response deserves a bit of explanation. Recall from Section 2.2 (see Figure 2.9 in the text) that the HTTP response message consists of a status line, followed by header lines, followed by a blank line, followed by the entity body. In the case of our HTTP GET, the  entity body in the response is the entire requested HTML file. In our case here, the HTML file is rather long, and at 4500 bytes is too large to fit in one TCP packet. The single HTTP response message is thus broken into several pieces by TCP, with each piece being contained within a separate TCP segment (see Figure 1.24 in the text). In recent versions of Wireshark, Wireshark indicates each TCP segment as a separate packet, and the fact that the single HTTP response was fragmented across multiple TCP packets is indicated by the “TCP segment of a reassembled PDU” in the Info column of the Wireshark display. Earlier versions of Wireshark used the “Continuation” phrase to indicated that the entire content of an HTTP message was broken across multiple TCP segments.. We stress here that there is no “Continuation” message in HTTP!&lt;/p&gt;
&lt;p&gt;Answer the following questions:&lt;/p&gt;
&lt;ol start=&#34;12&#34;&gt;
&lt;li&gt;How many HTTP GET request messages did your browser send? Which packet number in the trace contains the GET message for the Bill or Rights?&lt;/li&gt;
&lt;li&gt;Which packet number in the trace contains the status code and phrase associated with the response to the HTTP GET request?&lt;/li&gt;
&lt;li&gt;What is the status code and phrase in the response?&lt;/li&gt;
&lt;li&gt;How many data-containing TCP segments were needed to carry the single HTTP response and the text of the Bill of Rights?&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;html-documents-with-embedded-objects&#34;&gt;HTML Documents with Embedded Objects&lt;/h4&gt;
&lt;p&gt;Now that we’ve seen how Wireshark displays the captured packet traffic for large HTML files, we can look at what happens when your browser downloads a file with embedded objects, i.e., a file that includes other objects (in the example below, image files) that are stored on another server(s).&lt;/p&gt;
&lt;p&gt;Do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start up your web browser, and make sure your browser’s cache is cleared, as discussed above.&lt;/li&gt;
&lt;li&gt;Start up the Wireshark packet sniffer&lt;/li&gt;
&lt;li&gt;Enter the following URL into your browser &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file4.html&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file4.html&lt;/a&gt; Your browser should display a short HTML file with two images. These two images are referenced in the base HTML file. That is, the images themselves are not contained in the HTML; instead the URLs for the images are contained in the downloaded HTML file. As discussed in the textbook, your browser will have to retrieve these logos from the indicated web sites. Our publisher’s logo is retrieved from the gaia.cs.umass.edu web site. The image of the cover for our 5th edition (one of our favorite covers) is stored at the caite.cs.umass.edu server. (These are two different web servers inside cs.umass.edu).&lt;/li&gt;
&lt;li&gt;Stop Wireshark packet capture, and enter “http” in the display-filter-specification window, so that only captured HTTP messages will be displayed.&lt;/li&gt;
&lt;li&gt;(Note: If you are unable to run Wireshark on a live network connection, you can use the http-ethereal-trace-4 packet trace to answer the questions below; see footnote 1. This trace file was gathered while performing the steps above on one of the author’s computers.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Answer the following questions:&lt;/p&gt;
&lt;ol start=&#34;16&#34;&gt;
&lt;li&gt;How many HTTP GET request messages did your browser send? To which Internet addresses were these GET requests sent?&lt;/li&gt;
&lt;li&gt;Can you tell whether your browser downloaded the two images serially, or whether they were downloaded from the two web sites in parallel? Explain.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;http-authentication&#34;&gt;HTTP Authentication&lt;/h4&gt;
&lt;p&gt;Finally, let’s try visiting a web site that is password-protected and examine the sequence of HTTP message exchanged for such a site. The URL &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/protected_pages/HTTP-wireshark-file5.html&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/protected_pages/HTTP-wireshark-file5.html&lt;/a&gt; is password protected. The username is “wireshark-students” (without the quotes), and the password is “network” (again, without the quotes). So let’s access this “secure” password-protected site. Do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make sure your browser’s cache is cleared, as discussed above, and close down your browser. Then, start up your browser&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start up the Wireshark packet sniffer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enter the following URL into your browser &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/protected_pages/HTTP-wiresharkfile5.html&#34;&gt;http://gaia.cs.umass.edu/wireshark-labs/protected_pages/HTTP-wiresharkfile5.html&lt;/a&gt; Type the requested user name and password into the pop up box.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop Wireshark packet capture, and enter “http” in the display-filter-specification window, so that only captured HTTP messages will be displayed later in the packet-listing window.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Note: If you are unable to run Wireshark on a live network connection, you can use the http-ethereal-trace-5 packet trace to answer the questions below; see footnote 2. This trace file was gathered while performing the steps above on one of the author’s computers.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s examine the Wireshark output. You might want to first read up on HTTP authentication by reviewing the easy-to-read material on “HTTP Access Authentication Framework” at &lt;a href=&#34;http://frontier.userland.com/stories/storyReader$2159&#34;&gt;http://frontier.userland.com/stories/storyReader$2159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Answer the following questions:&lt;/p&gt;
&lt;ol start=&#34;18&#34;&gt;
&lt;li&gt;What is the server’s response (status code and phrase) in response to the initial HTTP GET message from your browser?&lt;/li&gt;
&lt;li&gt;When your browser’s sends the HTTP GET message for the second time, what new field is included in the HTTP GET message?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The username (wireshark-students) and password (network) that you entered are encoded in the string of characters (d2lyZXNoYXJrLXN0dWRlbnRzOm5ldHdvcms=) following  the “Authorization: Basic” header in the client’s HTTP GET message. While it may appear that your username and password are encrypted, they are simply encoded in a format known as Base64 format. The username and password are not encrypted! To see this, go to &lt;a href=&#34;http://www.motobit.com/util/base64-decoder-encoder.asp&#34;&gt;http://www.motobit.com/util/base64-decoder-encoder.asp&lt;/a&gt; and enter the base64-encoded string d2lyZXNoYXJrLXN0dWRlbnRz and decode. Voila! You have translated from Base64 encoding to ASCII encoding, and thus should see your username! To view the password, enter the remainder of the string Om5ldHdvcms= and press decode. Since anyone can download a tool like Wireshark and sniff packets (not just their own) passing by their network adaptor, and anyone can translate from Base64 to ASCII (you just did it!), it should be clear to you that simple passwords on WWW sites are not secure unless additional measures are taken.&lt;/p&gt;
&lt;p&gt;Fear not! As we will see in Chapter 8, there are ways to make WWW access more secure. However, we’ll clearly need something that goes beyond the basic HTTP authentication framework!&lt;/p&gt;
&lt;h2 id=&#34;wireshark-lab-dns&#34;&gt;Wireshark Lab: DNS&lt;/h2&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pearson.com/us/higher-education/product/Kurose-Computer-Networking-A-Top-Down-Approach-6th-Edition/9780132856201.html?tab=contents&#34;&gt;Computer Networking: A Top-Down Approach, 6th Edition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gaia.cs.umass.edu/kurose_ross/wireshark.htm&#34;&gt;&lt;strong&gt;Wireshark Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Computer Networks and the Internet</title>
      <link>http://www.yezheng.pro/post/specialization/computer-networking/computer-networks-1/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/computer-networking/computer-networks-1/</guid>
      
        <description>&lt;h2 id=&#34;what-is-the-internet&#34;&gt;What Is the Internet?&lt;/h2&gt;
&lt;h4 id=&#34;a-nuts-and-bolts-description&#34;&gt;A Nuts-and-Bolts Description&lt;/h4&gt;
&lt;p&gt;设备：称为&lt;strong&gt;主机（host）&lt;strong&gt;或&lt;/strong&gt;端系统（end system）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;端系统通过**通信链路（communication link）&lt;strong&gt;和&lt;/strong&gt;分组交换机（packet switch）**连接到一起。&lt;/p&gt;
&lt;p&gt;通信链路的物理媒体包含：电缆、铜线、光纤和无线电频谱。&lt;/p&gt;
&lt;p&gt;端系统向另一台端系统发送数据时，发送端系统将数据分段，并为每段加上首部字节，形成的信息包称为&lt;strong&gt;分组（packet）&lt;/strong&gt;，分组到目的端系统被装配成初始数据。&lt;/p&gt;
&lt;p&gt;分组交换机：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**路由器（router）**通常用于网络核心中&lt;/li&gt;
&lt;li&gt;**链路层交换机（link-layer swtich）**通常用于接入网中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;端系统通过**因特网服务提供商（Internet Service Provider, ISP）**接入因特网。每个ISP自身就是一个由多台分组交换机和多段通信链路组成的网络。因特网就是将端系统互联，因此为端系统提供的ISP也必须互联起来。无论是较高层还是较低层ISP网络,它们每个都是独立管理的,运行着IP协议,遵从一定的命名和地址规则。&lt;/p&gt;
&lt;p&gt;端系统、分组交换机和其他因特网部件都要运行一系列&lt;strong&gt;协议(protocol)&lt;/strong&gt;,这些协议控制因特网中信息的接收和发送。**TCP(Transmission Control Protocol, 传输控制协议)&lt;strong&gt;和&lt;/strong&gt;IP(Intemnet Protocol,网际协议)**是因特网中两个最为重要的协议。IP协议定义了在路由器和端系统之间发送和接收的分组格式。&lt;strong&gt;因特网的主要协议统称为TCP/IP。&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;a-services-description&#34;&gt;A Services Description&lt;/h4&gt;
&lt;p&gt;涉及多个相互交换数据的端系统的应用程序, 被称为&lt;strong&gt;分布式应用程序(distributed application)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;与因特网相连的端系统提供了一个&lt;strong&gt;套接字接口（socket interface）&lt;/strong&gt;,该接口规定了运行在一个端系统上的程序请求因特网基础设施向运行在另一个端系统上的特定目的地程序交付数据的方式。&lt;/p&gt;
&lt;h4 id=&#34;what-is-a-protocol&#34;&gt;What Is a Protocol?&lt;/h4&gt;
&lt;p&gt;**协议（protocol）**定义了在两个或多个通信实体之间交换的报文的格式和顺序,以及报文发送和/或接收一条报文或其他事件所采取的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;掌握计算机网络领域知识的过程就是理解网络协议的构成、原理和工作方式的过程。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-network-edge&#34;&gt;The Network Edge&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;主机 = 端系统&lt;/strong&gt;。主机有时又被进一步划分为两类：&lt;strong&gt;客户（client）&lt;strong&gt;和&lt;/strong&gt;服务器(server)&lt;/strong&gt;。今天, 大部分提供搜索结果、电子邮件、Web页面和视频的服务器都属于&lt;strong&gt;大型数据中心(data center)&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;access-networks&#34;&gt;Access Networks&lt;/h4&gt;
&lt;p&gt;接入网是指将端系统物理连接到其**边缘路由器（edge rounter）**的物理链路。边缘路由器是端系统到任何其他远程端系统的路径上的第一台路由器。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;家庭接人:数字用户线（Digital Subscriber Line, DSL）、电缆（Cable）、光纤入户（FTTH）、拨号（很慢）和卫星&lt;/li&gt;
&lt;li&gt;企业(和家庭)接人:以太网和WiFi，以太网是用局域网（LAN）技术，WIFI是无线LAN。现在家庭通常电缆或DSL可以与无线局域网结合使用&lt;/li&gt;
&lt;li&gt;广域无线接人:3G和LTE，通过蜂窝网提供商运营的基站发送接收分组。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;physical-media&#34;&gt;Physical Media&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;双绕铜线&lt;/li&gt;
&lt;li&gt;同轴电缆&lt;/li&gt;
&lt;li&gt;光纤&lt;/li&gt;
&lt;li&gt;陆地无线电信道&lt;/li&gt;
&lt;li&gt;卫星无线电信道&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-network-core&#34;&gt;The Network Core&lt;/h2&gt;
&lt;h4 id=&#34;packet-switching&#34;&gt;Packet Switching&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;在各种网络应用中,端系统彼此交换&lt;strong&gt;报文(message)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;报文能够包含协议设计者需要的任何东西。&lt;strong&gt;报文可以执行一种控制功能,也可以包含数据&lt;/strong&gt;, 例如电子邮件数据、JPEC图像或MP3音频文件。&lt;/li&gt;
&lt;li&gt;为了从源端系统向目的端系统发送一个报文, 源将长报文划分为较小的数据块,称之为&lt;strong&gt;分组(packet)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;在源和目的地之间,每个分组都通过通信链路和**分组交换机(packet switch)**传送。(交换机主要有两类:&lt;strong&gt;路由器(router)&lt;strong&gt;和&lt;/strong&gt;链路层交换机(link-layer switch)&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;存储转发传输&#34;&gt;存储转发传输&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;多数分组交换机在链路的输人端使用**存储转发传输(store-and-forward transmission)**机制。存储转发传输是指在交换机能够开始向输出链路传输该分组的第一个比特之前,必须接收到整个分组。&lt;/li&gt;
&lt;li&gt;通过由N条速率均为R的链路组成的路径，有N-1台路由器，从源到目的地发送一个分组，端到端时延是$$N\frac{L}{R}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;排队时延和分组丢失&#34;&gt;排队时延和分组丢失&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;每台分组交换机有多条链路与之相连。对于每条相连的链路,该分组交换机具有一个&lt;strong&gt;输出缓存(output buffer,也称为输出队列(output queue))&lt;/strong&gt;,它用于存储路由器准备发往那条链路的分组。该输出缓存在分组交换中起着重要的作用。&lt;/li&gt;
&lt;li&gt;除了存储转发时延以外,分组还要承受输出缓存的&lt;strong&gt;排队时延(queuing delay)&lt;/strong&gt;。这些时延是变化的,变化的程度取决于网络的拥塞程度。因为缓存空间的大小是有限的,一个到达的分组可能发现该缓存已被其他等待传输的分组完全充满了。在此情况下,将出现&lt;strong&gt;分组丢失(丢包)(packet loss)&lt;/strong&gt;,到达的分组或已经排队的分组之一将被丢弃。&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;转发表和路由选择协议&#34;&gt;转发表和路由选择协议&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;在因特网中，每个端系统具有一个称为&lt;strong&gt;IP地址&lt;/strong&gt;的地址。&lt;/li&gt;
&lt;li&gt;每台路由器具有一个&lt;strong&gt;转发表(forwarding table)&lt;/strong&gt;,用于将目的地址（或目的地址的一部分）映射成为输出链路。&lt;/li&gt;
&lt;li&gt;因特网具有一些特殊的&lt;strong&gt;路由选择协议（routing protocol）&lt;/strong&gt; ，用于自动地设置这些转发表。&lt;/li&gt;
&lt;li&gt;使用Traceroute程序可以看到分组在因特网中所走的端到端路由。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;circuit-switching&#34;&gt;Circuit Switching&lt;/h4&gt;
&lt;p&gt;通过网络链路和交换机移动数据有两种基本方法:&lt;strong&gt;电路交换(circuit switching)&lt;strong&gt;和&lt;/strong&gt;分组交换(packet switching)&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;a-network-of-networks&#34;&gt;A Network of Networks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;因为接入ISP向全球传输ISP付费,故接人ISP被认为是&lt;strong&gt;客户(customer)&lt;/strong&gt;,而全球传输ISP被认为是&lt;strong&gt;提供商(provider)&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;在任何给定的区域,可能有一个&lt;strong&gt;区域ISP(regional ISP)&lt;/strong&gt;，区域中的接人ISP与之连接。每个区域ISP则与&lt;strong&gt;第一层ISP(tier-1 ISP)连接&lt;/strong&gt;。第一层ISP类似于我们假想的全球传输ISP,尽管它不是在世界上每个城市中都存在,但它确实存在。&lt;/li&gt;
&lt;li&gt;任何ISP〔(除了第一层ISP)可以选择&lt;strong&gt;多宿(multi-home)&lt;/strong&gt;,即可以与两个或更多提供商ISP连接。例如,一个接人ISP可能与两个区域ISP多宿,既可以与两个区域ISP多宿,也可以与多个第一层ISP多宿。当一个ISP提供商出现故障，能够继续发送和接收分组。&lt;/li&gt;
&lt;li&gt;位于相同等级结构层次的邻近一对ISP能够&lt;strong&gt;对等(peer)&lt;/strong&gt;, 也就是说,能够直接将它们的网络连到一起,使它们之间的所有流量经直接连接而不是通过上游的中间ISP传输。当两个ISP对等时,通常不进行结算,即任一个ISP不向其对等付费。因特网交换点（Internet Exchange Point, IXP）多个ISP能够在这里共同对等。&lt;/li&gt;
&lt;li&gt;顶部有&lt;strong&gt;内容提供商网络（content provider network）&lt;/strong&gt;，数据中心绕过较高层ISP，直接与较低层ISP连接。&lt;/li&gt;
&lt;li&gt;今天的因特网是一个&lt;strong&gt;网络的网络&lt;/strong&gt;，其结构复杂，由十多个第一层ISP和数十万个较低层ISP组成。较低层的ISP与较高层的ISP相连,较高层ISP彼此互联。用户和内容提供商是较低层ISP的客户,较低层ISP是较高层ISP的客户。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-aeb1ce8889ea9b88cb57afbe5c49cc6c_1440w.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;delay-loss-and-throughput-in-packet-switched-networks&#34;&gt;Delay, Loss, and Throughput in Packet-Switched Networks&lt;/h2&gt;
&lt;h4 id=&#34;overview-of-delay-in-packet-switched-networks&#34;&gt;Overview of Delay in Packet-Switched Networks&lt;/h4&gt;
&lt;p&gt;当分组从一个节点(主机或路由器)沿着这条路径到后继节点(主机或路由器),该分组在沿途的冬个节点经受了几种不同类型的时延。这些时延最为重要的是&lt;strong&gt;节点处理时延(nodalprocessing delay)&lt;/strong&gt;、&lt;strong&gt;排队时延(queuing delay)&lt;/strong&gt;、&lt;strong&gt;传输时延(transmission delay)&lt;strong&gt;和&lt;/strong&gt;传播时延(propagation delay)&lt;/strong&gt;,这些时延总体累加起来是&lt;strong&gt;节点总时延(totalnodal delay)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-5d5e8ce120b48b8ea632f8e6564480f0_1440w.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;处理时延&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;检查分组首部和决定将该分组导向何处所需要的时间是&lt;strong&gt;处理时延&lt;/strong&gt;的一部分。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;排队时延&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;在队列中,当分组在链路上等待传输时,它经受&lt;strong&gt;排队时延&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传输时延&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;用L比特表示该分组的长度,用Rbps(即b/s)表示从路由器A到路由器B的链路传输速率。链路,速率史=10Mbps;对于100Mbps的以太网链路,速率R=100Mbps。&lt;strong&gt;传输时延是L/R&lt;/strong&gt;。这是将所有分组的比特推向链路(即传输,或者说发射)所需要的时间。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传播时延&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;一旦一个比特被推向链路,该比特需要向路由器B传播。从该链路的起点到路由器B传播所需要的时间是&lt;strong&gt;传播时延&lt;/strong&gt;。该比特以该链路的传播速率传播。该传播速率取决于该链路的物理媒体(即光纤、双绞铜线等),其速率范围是2x10~3x10m/s,这等于或略小于光速。该传播时延等于两台路由器之间的距离除以传播速率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;传输时延是路由器将分组推出所需要的时间，是分组长度和链路传输速率的函数，而与两台路由器之间的距离无关。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;传播时延是一个比特从一台路由器向另一台路由器传播所需要的时间，是两台路由器之间距离的函数，而与分组长度或链路传播速率无关。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;queuing-delay-and-packet-loss&#34;&gt;Queuing Delay and Packet Loss&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;节点时延的最为复杂和有趣的成分是排队时延。&lt;/li&gt;
&lt;li&gt;与其他三项时延（即、 和 ）不同的是，排队时延对不同的分组可能是不同的&lt;/li&gt;
&lt;li&gt;令 $a$ 表示分组到达队列的平均速率（单位：分组/秒，即 $pkt/s$）；$R$是传输速率，即从队列中推出比特的速率（单位 $bps$：，即$b/s$）；假定所以分组都是由$L$比特组成的**，**则比特到达队列的平均速率是$La/R &amp;gt; 1$。假定该队列非常大，因此它基本能容纳无限数量的比特。比率$La/R$被称为**流量强度（traffic intensity）**。&lt;/li&gt;
&lt;li&gt;如果，则比特到达队列的平均速率超过从该队列传输出去的速率，该队列趋向于无限增加，并且排队时延将趋向无穷大！因此，流量工程的一条金科玉律是：&lt;strong&gt;设计系统时流量强度不能大于1&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;在上述讨论中,我们已经假设队列能够容纳无穷多的分组。在现实中,一条链路前的队列只有有限的容量,尽管排队容量极大地依赖于路由器设计和成本。因为该排队容量是有限的,随着流量强度接近1,排队时延并不真正趋向无穷大。相反,到达的分组将发现一个满的队列。由于没有地方存储这个分组,路由器将&lt;strong&gt;丢弃(drop)&lt;strong&gt;该分组,即该分组将会&lt;/strong&gt;丢失(lost)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分组丢失的比例随着流量强度增加而增加&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;end-to-end-delay&#34;&gt;End-to-End Delay&lt;/h4&gt;
&lt;p&gt;假定在源主机和目的主机之间有台路由器。我们还要假设该网络此时是无拥塞的(因此排队时延是微不足道的),在每台路由器和源主机上的处理时延是$d_{proc}$。,每台路由器和源主机的输出速率是$R$ bps,每条链路的传播时延是$d_{prop}$。节点时延累加起来, 得到端到端时延:$d_{end-end}=N(d_{proc}+d_{trans}+d_{prop})$  式中，其中$L$是分组长度，$d_{trans} = L/R$&lt;/p&gt;
&lt;h4 id=&#34;throughput-in-computer-networks-计算机网络中的吞吐量&#34;&gt;Throughput in Computer Networks (计算机网络中的吞吐量)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;考虑从主机A到主机B跨越计算机网络传送一个大文件，在任何时间瞬间的&lt;strong&gt;瞬时吞吐量（instantaneous throughput）&lt;strong&gt;是主机B接收到该文件的&lt;/strong&gt;速率&lt;/strong&gt;（以计）。&lt;/li&gt;
&lt;li&gt;如果该文件由比特组成，主机B接收到所有F比特用去秒，则文件传送的**平均吞吐量（average throughout）**是&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;protocol-layers-and-their-service-models&#34;&gt;Protocol Layers and Their Service Models&lt;/h2&gt;
&lt;h4 id=&#34;layered-architecture&#34;&gt;Layered Architecture&lt;/h4&gt;
&lt;h6 id=&#34;协议分层&#34;&gt;协议分层&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;协议分层的优缺点&lt;/strong&gt;：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：协议分层具有概念化和结构化的优点。分层提供了一种结构化方式来讨论系统组件。模块化使更新系统组件更为容易。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：分层的一个潜在缺点是一层可能冗余较低层的功能。第二个潜在的缺点 是某层的功能可能需要仅在其他某层才出现的信息（如时间戳值），这违反了层次分离的目标。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;各层的所有协议被称为协议栈(protocol stack)。&lt;/p&gt;
&lt;p&gt;五层因特网协议栈:&lt;strong&gt;物理层、链路层、网络层、运输层和应用层&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;七层ISO OSI参考模型：&lt;strong&gt;物理层、链路层、网络层、运输层、会话层、表示层和应用层&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因特网协议栈自顶向下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;应用层&lt;/p&gt;
&lt;p&gt;因特网的应用层包括许多协议，例如HTTP（Web文档请求和传送）、SMTP（电子邮件报文的传输）、FTP（两个端系统之间的文件传送）、DNS（域名系统）等。&lt;/p&gt;
&lt;p&gt;应用层协议分布在多个端的系统上，一个端系统中的应用程序使用协议与另一个端系统中的应用程序交换信息的分组。应用层的信息分组称为&lt;strong&gt;报文(message)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;运输层&lt;/p&gt;
&lt;p&gt;因特网的运输层在应用程序端点之间&lt;strong&gt;传送应用层报文&lt;/strong&gt;。因特网中有两个运输协议，即TCP和UDP，两个都可以运输应用层报文。&lt;/p&gt;
&lt;p&gt;TCP是面向连接的，这种服务包括了应用层报文向目的地的&lt;strong&gt;确保传递&lt;/strong&gt;和&lt;strong&gt;流量控制&lt;/strong&gt;，TCP也将长报文&lt;strong&gt;划分为短报文&lt;/strong&gt;，并提供&lt;strong&gt;拥塞控制&lt;/strong&gt;机制，因此当网络拥塞时，源地址会抑制其传输速率。&lt;/p&gt;
&lt;p&gt;UDP协议提供无连接服务，这是一种不提供不必要服务的服务，&lt;strong&gt;没有可靠性，没有流量控制，也没有拥塞控制&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;运输层分组称为&lt;strong&gt;报文段(segment)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;网络层
网络层的分组称为 &lt;strong&gt;数据报(datagram)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因特网的网络层把数据包从一台主机移动到另一台主机。在一台源主机中的因特网运输层协议（TCP或UDP）向网络层&lt;strong&gt;递交运输层报文段和目的地址&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;网络层中包括&lt;strong&gt;IP协议&lt;/strong&gt;，该协议定义了数据包中的各个字段以及端系统和路由器如何作用于这些字段。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;链路层
因特网的网络层通过源和目的地之间的一系列路由器路由数据报。网络层依靠链路层的服务来将分组从结点(主机或路由器)移动到路径上的下一个结点。在每一个结点，网络层将数据报下，链路层沿着路径将数据报传递给下一个结点，在下一个结点，链路层将数据报上传给网络层。&lt;/p&gt;
&lt;p&gt;某些协议基于链路层提供可靠传递，从传输结点跨越一条链路到接收结点，这种可靠传输不同于TCP的可靠传输。(从一个端系统到另一个端系统)&lt;/p&gt;
&lt;p&gt;链路层分组称为 &lt;strong&gt;帧(frame)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;物理层
链路层的任务是把整个帧从一个网络元素移动到邻近的网络元素，物理层的任务则是把&lt;strong&gt;该帧中的一个个比特从一个结点移动到下一个结点&lt;/strong&gt;，这些仍是和链路相关的，但是是与链路的实际传输媒体相关的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OSI模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;表示层&lt;/p&gt;
&lt;p&gt;表示层的作用是使通信的应用程序能够解释交换数据的含义。这些服务包括数据压缩、数据加密以及数据描述。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;会话层&lt;/p&gt;
&lt;p&gt;会话层提供了数据交换定界和同步功能，包括了建立检查点和恢复方案的方法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因特网缺少了OSI模型中的两层，这两层留给应用程序开发者处理。&lt;/p&gt;
&lt;h4 id=&#34;encapsulation&#34;&gt;Encapsulation&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://pic3.zhimg.com/80/v2-9a7b7f552162bcb27dd2c4986c03568a_1440w.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;数据从发送端系统的协议栈向下，向上和向下经过中间的链路层交换机和路由器协议栈，进而向上到达接收端系统的协议栈。&lt;/p&gt;
&lt;p&gt;路由器和链路层交换机并不实现协议栈中的所有层次。链路层交换机实现了链路层和物理层，路由器实现了链路层、物理层和网络层。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;封装（encapsulation）&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;运输层收到&lt;strong&gt;应用层报文(application-layer message)&lt;strong&gt;并附上运输层首部信息，构成了&lt;/strong&gt;运输层报文端（transport-layer segment）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;网络层增加了如源和目的端系统地址和网络层首部信息，产生&lt;strong&gt;网络层数据报（network-layer datagram）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;链路层增加链路层首部信息并创建&lt;strong&gt;链路层帧（link-layer frame）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在每一层，一个分组都具有两种类型的字段：&lt;strong&gt;首部字段和有效载荷字段（payload field）&lt;/strong&gt;，有效符合通常来自上一层的分组。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pearson.com/us/higher-education/product/Kurose-Computer-Networking-A-Top-Down-Approach-6th-Edition/9780132856201.html?tab=contents&#34;&gt;Computer Networking: A Top-Down Approach, 6th Edition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>How the Internet Works</title>
      <link>http://www.yezheng.pro/post/specialization/computer-networking/how-the-internet-works/</link>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/computer-networking/how-the-internet-works/</guid>
      
        <description>&lt;ul&gt;
&lt;li&gt;Multiple packets in pipeline at same time&lt;/li&gt;
&lt;li&gt;Packets may be &lt;strong&gt;damaged&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Packets may arrive &lt;strong&gt;out of order&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Packets may be &lt;strong&gt;duplicated&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;They may not arrive at all!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary so far: Applications send and receive data in &lt;strong&gt;packets&lt;/strong&gt; over an Internet that is &lt;strong&gt;unreliable&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;how-packets-find-their-way-across-the-internet&#34;&gt;How packets find their way across the Internet?&lt;/h3&gt;
&lt;h6 id=&#34;internet-addresses&#34;&gt;Internet addresses&lt;/h6&gt;
&lt;p&gt;| Data | Internet &amp;ldquo;IP&amp;rdquo; Address |&lt;/p&gt;
&lt;p&gt;All Internet packets carry a destination IP address. We usually write the IP address like this: 171.64.74.58&lt;/p&gt;
&lt;h6 id=&#34;routers-forward-packets-one-at-a-time&#34;&gt;Routers forward packets one at a time&lt;/h6&gt;
&lt;p&gt;Routers look at IP addresses, then send packets to a router closer to the destination.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The IP address tells a router where to send the packet next.&lt;/strong&gt;
IP addresses have structure:
A network in The Stanford the computer CS department University yuba.stanford.edu at Stanford University
171.64.74.58
An address managed by RIPE. An address  The computer at (European Koç University &lt;a href=&#34;http://www.ku.edu.tr&#34;&gt;www.ku.edu.tr&lt;/a&gt; IP Networks)
88.255.96.208&lt;/p&gt;
&lt;h4 id=&#34;can-we-see-the-path-our-packets-take&#34;&gt;Can we see the path our packets take?&lt;/h4&gt;
&lt;p&gt;On your computer, try: “&lt;strong&gt;ping&lt;/strong&gt; yuba.stanford.edu” and “&lt;strong&gt;traceroute&lt;/strong&gt; yuba.stanford.edu”
(Windows: “tracert yuba.stanford.edu”)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Round Trip Time (RTT)&lt;/strong&gt; is the length time it takes for a data packet to be sent to a destination plus the time it takes for an acknowledgment of that packet to be received back at the origin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Packets are forwarded hop-by-hop based on the final destination address.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Internet cannot be trusted!!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Internet doesn’t promise to deliver packets in order.&lt;/li&gt;
&lt;li&gt;It doesn’t promise to deliver packets quickly, or on time.&lt;/li&gt;
&lt;li&gt;It doesn’t even promise to deliver them at all!&lt;/li&gt;
&lt;li&gt;It just makes a “best-effort” attempt.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sending data reliably over an Internet that is unreliable.&lt;/p&gt;
&lt;h4 id=&#34;how-network-applications-communicate&#34;&gt;How Network Applications Communicate&lt;/h4&gt;
&lt;p&gt;The most common method:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Communication is in both directions – “bidirectional”.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Communication is reliable&lt;/p&gt;
&lt;p&gt;(if there is a working path between the two computers).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s like an unformatted pipe:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You push data in at one end, and it pops out correctly at the other end.&lt;/li&gt;
&lt;li&gt;The applications decide how the data is formatted inside the pipe.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Byte Stream Model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setup connection&lt;/li&gt;
&lt;li&gt;Stream of bytes&lt;/li&gt;
&lt;li&gt;Close connection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;World Wide Web (HTTP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Setup connection&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stream of bytes&lt;/p&gt;
&lt;p&gt;GET/HTTP/1.1 index.html&lt;/p&gt;
&lt;p&gt;HTTP/1.1 200 OK &amp;lt;&amp;hellip;contents of index.html&amp;hellip;&amp;gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Close connection&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TCP makes sure all the data is delivered&lt;/p&gt;
&lt;h4 id=&#34;tcps-job&#34;&gt;TCP’s job&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Makes sure all data is delivered correctly. Delivers data to the application in the right order.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add sequence numbers to every packet (so the receiver can check if any are missing, and put them in right order)&lt;/li&gt;
&lt;li&gt;When a packet arrives, send an acknowledgment of receipt or “&lt;strong&gt;ACK&lt;/strong&gt;” back to the sender&lt;/li&gt;
&lt;li&gt;If no acknowledgment is received, resend the data&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;packets-are-encapsulated-by-different-layers-of-processing&#34;&gt;Packets are “encapsulated” by different “layers” of processing&lt;/h4&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Applications send and receive data in packets over an Internet that is unreliable.&lt;/p&gt;
&lt;p&gt;Packets are forwarded hop-by-hop using the IP destination address.&lt;/p&gt;
&lt;p&gt;Our applications use TCP to make sure they are delivered and put back in the correct order.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>evaluate the quality of the training phrases in intents</title>
      <link>http://www.yezheng.pro/post/specialization/artificial-intelligence/chatbot/evaluate-the-quality-of-the-training-phrases-in-intents/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/artificial-intelligence/chatbot/evaluate-the-quality-of-the-training-phrases-in-intents/</guid>
      
        <description>&lt;p&gt;This tutorial shows you how to analyze and evaluate the quality of the training phrases supplied to your &lt;a href=&#34;https://dialogflow.com/&#34;&gt;Dialogflow&lt;/a&gt; agent&amp;rsquo;s intents. The purpose of this analysis is to avoid confusing the agent with phrases irrelevant to the intents supplied to, or more relevant to, other intents.&lt;/p&gt;
&lt;p&gt;The approach you take is to generate semantic embeddings of the training phrases by using the &lt;a href=&#34;https://www.tensorflow.org/hub/&#34;&gt;TensorFlow Hub&lt;/a&gt; (&lt;code&gt;tf.Hub&lt;/code&gt;) Universal Sentence Encoder module. You then compute cohesion and separation measurements based on the similarity between embeddings within the same intents and different intents. The tutorial also identifies &amp;ldquo;confusing&amp;rdquo; training phrases, where they are nearer—in the embedding space—to intents that are different from the ones supplied for.&lt;/p&gt;
&lt;p&gt;You can find the code for this tutorial in this &lt;a href=&#34;https://colab.research.google.com/drive/1QQblWJX5aZ6TajVXNAogFpVJlzYDbMtX&#34;&gt;Colab notebook&lt;/a&gt;. The article assumes that you have a basic background knowledge of Dialogflow. To learn more about Dialogflow, see this &lt;a href=&#34;https://cloud.google.com/solutions/building-and-deploying-chatbot-dialogflow&#34;&gt;multi-part tutorial&lt;/a&gt; on how to build, secure, and scale a chatbot by using Dialogflow Enterprise Edition on Google Cloud.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://dialogflow.com/&#34;&gt;Dialogflow&lt;/a&gt; lets you build conversational interfaces on top of your products and services by providing a powerful natural-language understanding (NLU) engine to process and understand natural language input. Use cases for Dialogflow include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Building booking and reservation bots for airlines, cinemas, and so on.&lt;/li&gt;
&lt;li&gt;Simplifying a system for ordering fast food for delivery.&lt;/li&gt;
&lt;li&gt;Enabling efficient customer service through semi-automated call centers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although you can implement complex conversational flows to handle a user utterance, Dialogflow fundamentally performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The user asks questions like, &amp;ldquo;What is the total of my bill for the last month?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;agent&lt;/em&gt; parses the input and matches it to an &lt;em&gt;intent&lt;/em&gt; such as &lt;code&gt;bill_value_inquiry&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The agent also extracts &lt;em&gt;entities&lt;/em&gt; information, like &amp;ldquo;last month&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Given the intent of the extracted entities, the agent then invokes a &lt;em&gt;fulfillment&lt;/em&gt; to respond to the user&amp;rsquo;s request.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following table describes the key concepts in the Dialogflow platform.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Term&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://dialogflow.com/docs/agents&#34;&gt;agent&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Agents are best described as NLU modules that can be integrated into your system. An agent converts text or spoken user requests into actionable data, when a user&amp;rsquo;s input matches an intent in your agent.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://dialogflow.com/docs/intents&#34;&gt;intent&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;In a conversation, intents map user input to responses. In each intent, you define examples (training phrases) of user utterances that can trigger the intent, what to extract from each utterance, and how to respond.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://dialogflow.com/docs/entities&#34;&gt;entities&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Where intents allow your agent to understand the motivation behind a particular user input, entities are used to pick out specific pieces of information that your users mention. For example, street addresses, product names, or amounts with units can be used to fulfill the user&amp;rsquo;s request.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://dialogflow.com/docs/fulfillment&#34;&gt;fulfillment&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Fulfillment allows you to use the entity information extracted by the agent to generate dynamic responses or trigger actions on your backend on an intent-by-intent basis.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For more details on Dialogflow concepts, see the &lt;a href=&#34;https://dialogflow.com/docs/&#34;&gt;Dialogflow documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Intents are essential to a Dialogflow system, because they link the user request to the right business logic to fulfill it. For example, a Dialogflow system for a telecom services provider might have intents like &lt;code&gt;bill_value_inquiry&lt;/code&gt;, &lt;code&gt;pay_bill&lt;/code&gt;, &lt;code&gt;upgrade_contract&lt;/code&gt;, &lt;code&gt;cancel_contract&lt;/code&gt;, and &lt;code&gt;add_service&lt;/code&gt;. However, in order to match the user utterance (text or speech) to the right intent, intents need to be trained with a set of relevant training phrases. For example, for a weather inquiry intent, training phrases might be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;What is the weather like right now?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What is the temperature in Cairo tomorrow?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Do I need to take an umbrella with me to Zurich next week?&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When you create several intents in your system, some phrases you supply to an intent might be confusing or misleading—for example, a phrase that&amp;rsquo;s more relevant to another intent might be used to train the wrong intent. For example, suppose you have a Dialogflow agent that serves as the source of truth for a sales organization. You might have two intents for fetching contacts: one for the internal account teams and one for the customer. You might call these &lt;code&gt;get_internal_contacts&lt;/code&gt; and &lt;code&gt;get_external_contacts&lt;/code&gt;. A typical training phrase for each intent would be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;get_internal_contacts&lt;/code&gt;: &amp;ldquo;Who is the point of contact for Customer X?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get_external_contacts&lt;/code&gt;: &amp;ldquo;How do I get in contact with Customer X?&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose that your users supplied the following request as they were looking for the external contacts: &amp;ldquo;Contacts for Customer X&amp;rdquo;. This request can confuse the Dialogflow agent because the phrase can match both intents. If the wrong intent matches, users will have a poor experience because they must change the formulation of the request, which is annoying and time consuming.&lt;/p&gt;
&lt;p&gt;Therefore, you want to make sure that phrases within the same intent are more similar, while phrases between different intents are less similar. The rest of the tutorial explains how to evaluate the quality of the training phrase supplied for each intent, and how to identify potentially confusing training phrases.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;The approach used in this tutorial is to compute the similarity between two phrases and, by extension, to compute the similarity matrix for all the training phrases. Once you have that matrix, you can compute the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cohesion&lt;/strong&gt;: The average similarity value between each pair of phrases in the same intent. That value is computed for each intent. The higher the intent cohesion value, the better the intent training phrases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Separation&lt;/strong&gt;: Given two intents, the average distance between each pair of training phrases in the two intents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Confusing phrases&lt;/strong&gt;: Training phrases that are highly similar to training phrases in other intents.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To compute a similarity value between two phrases, you must convert each phrase to a real-value feature vector, which represents the semantics of the phrase (&lt;a href=&#34;https://developers.google.com/machine-learning/glossary/#embeddings&#34;&gt;embeddings&lt;/a&gt;). To help with this task, the tutorial uses &lt;a href=&#34;https://www.tensorflow.org/hub/&#34;&gt;TensorFlow Hub&lt;/a&gt; (&lt;code&gt;tf.Hub&lt;/code&gt;), a library used for the publication, discovery, and consumption of reusable &lt;a href=&#34;https://tfhub.dev/&#34;&gt;modules&lt;/a&gt; of machine learning models. These modules can be pre-trained models or embeddings that are extracted from text, images, and so on. You can browse the &lt;a href=&#34;https://tfhub.dev/s?module-type=text-embedding&#34;&gt;available text embeddings&lt;/a&gt;. The tutorial uses the &lt;a href=&#34;https://tfhub.dev/google/universal-sentence-encoder/2&#34;&gt;Universal Sentence Encoder&lt;/a&gt; (v2) module, which is used to encode text into 512 dimensional vectors that can be used for text classification, semantic similarity, clustering, and other natural-language tasks.&lt;/p&gt;
&lt;p&gt;In this tutorial, you use &lt;a href=&#34;https://wikipedia.org/wiki/Cosine_similarity&#34;&gt;cosine similarity&lt;/a&gt; as a proximity metric between two embedding vectors. Given two real-value vectors (in our example, two embedding vectors extracted from two training phrases), cosine similarity calculates the cosine of the angle between them, using the following formula:&lt;/p&gt;
&lt;p&gt;$$ \cos(A,B) = \frac{\sum_{i=1}^{n}A_iB_i}{\sqrt{\sum_{i=1}^{n}{A_i^2}}\sqrt{\sum_{i=1}^{n}{B_i^2}}} $$&lt;/p&gt;
&lt;p&gt;cos⁡(A,B)=∑i=1nAiBi∑i=1nAi2∑i=1nBi2&lt;/p&gt;
&lt;p&gt;In this formula, &lt;em&gt;n&lt;/em&gt; is the number of elements in the vector. The smaller the angle between the vectors, the bigger the cosine value of this angle, indicating higher similarity. The cosine similarity value between any two vectors is always between 0 and 1.&lt;/p&gt;
&lt;p&gt;Figure 1 shows an overview of the approach:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-eval-intents.svg&#34; alt=&#34;Overview of evaluating intents cohesion and separation&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Overview of evaluating intents cohesion and separation&lt;/p&gt;
&lt;p&gt;The figure illustrates the following sequence:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Import the intents and their training phrases.&lt;/li&gt;
&lt;li&gt;Generate embeddings for the training phrases using the &lt;code&gt;tf.Hub&lt;/code&gt; Universal Sentence Encoder pre-trained module.&lt;/li&gt;
&lt;li&gt;Create a visualization of the generated embeddings in a two-dimensional space.&lt;/li&gt;
&lt;li&gt;Compute the embeddings cosine similarity matrix containing the pairwise similarity values between all the training phrases in different intents.&lt;/li&gt;
&lt;li&gt;Calculate the cohesion and separation metrics.&lt;/li&gt;
&lt;li&gt;Identify the confusing phrases.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(Optional) Create a Dialogflow agent.&lt;/li&gt;
&lt;li&gt;Import intents with training phrases.&lt;/li&gt;
&lt;li&gt;Run the Colab notebook for intent quality assessment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;costs&#34;&gt;Costs&lt;/h2&gt;
&lt;p&gt;This tutorial uses the following billable components of Google Cloud:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dialogflow&lt;/strong&gt;: Standard Edition is free, while Enterprise Edition offers paid enterprise support. You can choose which edition to use when you create your Dialogflow agent. Your account can include agents from both editions. For more details, refer to the &lt;a href=&#34;https://dialogflow.com/pricing&#34;&gt;Dialogflow Pricing page&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;before-you-begin&#34;&gt;Before you begin&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In the Google Cloud Console, on the project selector page, select or create a Google Cloud project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you don&amp;rsquo;t plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/projectselector2/home/dashboard&#34;&gt;Go to the project selector page&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure that billing is enabled for your Cloud project. &lt;a href=&#34;https://cloud.google.com/billing/docs/how-to/modify-project&#34;&gt;Learn how to confirm that billing is enabled for your project&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enable the Dialogflow API.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/flows/enableapi?apiid=dialogflow.googleapis.com&#34;&gt;Enable the API&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a service account to call the Dialogflow API.&lt;/p&gt;
&lt;p&gt;Create service account&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;strong&gt;Service account details&lt;/strong&gt; dialog, enter the account name and description as shown in the following screenshot, and then click &lt;strong&gt;Create&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set the role to &lt;strong&gt;Dialogflow API Client&lt;/strong&gt; and click &lt;strong&gt;Continue&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;completing-the-tutorial-in-the-colab-notebook&#34;&gt;Completing the tutorial in the Colab notebook&lt;/h2&gt;
&lt;p&gt;The following sections walk through the steps discussed in the &lt;a href=&#34;https://cloud.google.com/solutions/assessing-the-quality-of-training-phrases-in-dialogflow-intents#approach&#34;&gt;approach&lt;/a&gt; section to calculate the cohesion and separation metrics and to identify confusing phrases.&lt;/p&gt;
&lt;h3 id=&#34;getting-started-with-the-colab-notebook&#34;&gt;Getting started with the Colab notebook&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Colab notebook: &lt;a href=&#34;https://colab.research.google.com/drive/1QQblWJX5aZ6TajVXNAogFpVJlzYDbMtX&#34;&gt;https://colab.research.google.com/drive/&amp;hellip;&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a local copy to your Google Drive.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-copy.png&#34; alt=&#34;Copying the notebook to your Google Drive&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In Cloud Shell, install the Python libraries needed for the rest of the tutorial, before importing the required libraries and modules.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install --quiet --upgrade tensorflow dialogflow scipy tensorflow-hub seaborn
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set your Google Cloud &lt;code&gt;PROJECT_ID&lt;/code&gt; and the &lt;code&gt;SERVICE_ACCOUNT_EMAIL&lt;/code&gt; that you created in the &lt;a href=&#34;https://cloud.google.com/solutions/assessing-the-quality-of-training-phrases-in-dialogflow-intents#before-you-begin&#34;&gt;Before you begin&lt;/a&gt; section.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-id-email.png&#34; alt=&#34;Set your Google Cloud PROJECT_ID and SERVICE_ACCOUNT_EMAIL&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Authenticate your session to create a key for your service account:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;auth.authenticate_user()
!gcloud config set project {PROJECT_ID}
!gcloud iam service-accounts keys create sa-key.json \
    --iam-account={SERVICE_ACCOUNT_EMAIL} --project={PROJECT_ID}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After you run these commands, a link is displayed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow the link to authenticate your user account.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy the authentication code from the web page, and paste it in the &lt;strong&gt;Enter verification code&lt;/strong&gt; field in the notebook:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-verification.png&#34; alt=&#34;Enter verification code field in the notebook&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;setting-up-a-dialogflow-agent&#34;&gt;Setting up a Dialogflow agent&lt;/h3&gt;
&lt;p&gt;If you already have a Dialogflow agent that you want to use in this tutorial, you can skip this step. However, if you don&amp;rsquo;t have an agent, or you want to set up a new one, you can download a zip file with the content of an exported Dialogflow agent, called &lt;code&gt;intents-healthcheck&lt;/code&gt;. You import this agent into your Dialogflow account as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download the zip file of the imported agent:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gsutil cp gs://dialogflow-intent-health-check/intent-quality-demo.zip .
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;a href=&#34;https://dialogflow.com/&#34;&gt;https://dialogflow.com/&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &lt;a href=&#34;https://console.dialogflow.com/api-client/#/login&#34;&gt;Go to Console&lt;/a&gt; button on the top right.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the left menu, click &lt;strong&gt;Create new agent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-console.png&#34; alt=&#34;Create new agent&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enter the &lt;strong&gt;agent name&lt;/strong&gt;: &lt;code&gt;intents-healthcheck&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select your Google Cloud project from the &lt;strong&gt;Google Project&lt;/strong&gt; list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Google Cloud project can have only one Dialogflow agent. So if you don&amp;rsquo;t find your Google Cloud project in the list, an agent is already associated with your project.&lt;/li&gt;
&lt;li&gt;If you select &lt;strong&gt;Create a new project&lt;/strong&gt;, Dialogflow creates a Google Cloud project with the same name as your agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-agent-info.png&#34; alt=&#34;Entering information about the agent&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the left-hand menu, select the new agent and then click the &lt;strong&gt;Settings&lt;/strong&gt; &lt;em&gt;settings&lt;/em&gt; icon. Then in the menu in the middle of the page, select &lt;strong&gt;Export and Import&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-import-export.png&#34; alt=&#34;Export and Import dialog&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &lt;strong&gt;Restore from zip&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the &lt;code&gt;agent-backup.zip&lt;/code&gt; file you downloaded in step 1.&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;RESTORE&lt;/code&gt; in the text box at the bottom of the form to confirm.&lt;/li&gt;
&lt;li&gt;Click &lt;strong&gt;Restore&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-zip-restore.png&#34; alt=&#34;Restore the agent from a zip file&#34;&gt;&lt;/p&gt;
&lt;p&gt;After restoring the agent, Dialogflow creates five intents.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify the imported intents by selecting &lt;strong&gt;Intents&lt;/strong&gt; from the menu on the left. You find the following intents:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-imported-intents.png&#34; alt=&#34;Verifying the imported intents&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You use this restored agent for the rest of the tutorial.&lt;/p&gt;
&lt;h2 id=&#34;walking-through-the-code-in-the-colab-notebook&#34;&gt;Walking through the code in the Colab notebook&lt;/h2&gt;
&lt;p&gt;The sections that follow describe what the code in the notebook does when you run it.&lt;/p&gt;
&lt;h3 id=&#34;fetching-your-intents&#34;&gt;Fetching your intents&lt;/h3&gt;
&lt;p&gt;The following code fetches intents and their training phrases from the Dialogflow agent using the &lt;code&gt;fetch_intents_training_phrases&lt;/code&gt; method. This method returns a dictionary, where the keys are the intents named in your Dialogflow agent, and each value is a list of the training phrases in each entity. In the code, &lt;code&gt;project&lt;/code&gt; references the project to which your agent belongs, and &lt;code&gt;service_account_file&lt;/code&gt; references the file that you created earlier.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def get_intents(service_account_file, project):

    dialogflow_entity_client =  dialogflow.EntityTypesClient.from_service_account_file(service_account_file)
    parent = dialogflow_entity_client.project_agent_path(project)
    entities = list(dialogflow_entity_client.list_entity_types(parent))

    dialogflow_intents_client = dialogflow.IntentsClient.from_service_account_file(service_account_file)
    parent = dialogflow_intents_client.project_agent_path(project)
    intents = list(dialogflow_intents_client.list_intents(
        parent=parent,
        intent_view=dialogflow.enums.IntentView.INTENT_VIEW_FULL))

    entities_name_to_value = {}
    for intent in intents:
        entities_used = {entity.display_name
            for entity in intent.parameters}

        for entity in entities:
            if entity.display_name in entities_used \
                    and entity.display_name not in entities_name_to_value:
                entities_name_to_value[entity.display_name] = np.random.choice(
                    np.random.choice(entity.entities).synonyms, replace=False)

    intent_to_training_phrases = defaultdict(list)
    for intent in intents:
        for training_phrase in intent.training_phrases:
            parts = [entities_name_to_value[part.alias] if part.entity_type else part.text
                for part in training_phrase.parts]
            intent_to_training_phrases[intent.display_name].append(&amp;quot;&amp;quot;.join(parts))
        # Remove intents with no training phrases
        if not intent_to_training_phrases[intent.display_name]:
            del intent_to_training_phrases[intent.display_name]
    return intent_to_training_phrases
 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following code verifies the retrieved intents:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;intent_training_phrases = fetch_intents_training_phrases(&amp;quot;sa-key.json&amp;quot;, project_id)
for intent in intent_training_phrases:
    print(&amp;quot;{}:{}&amp;quot;.format(intent, len(intent_training_phrases[intent])))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;code&gt;fetch_intents_training_phrases&lt;/code&gt; method returns the following listing. This code snippet shows the intents in the demo &lt;code&gt;intents-healthcheck&lt;/code&gt; agent, followed by the count of the training phrases available in each intent.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;start_conversation:4
close_conversation:5
get_internal_contacts:17
request_help:7
get_external_contacts:6
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;generating-embeddings-for-the-training-phrases&#34;&gt;Generating embeddings for the training phrases&lt;/h3&gt;
&lt;p&gt;The following code downloads the &lt;code&gt;tf.Hub&lt;/code&gt; Universal Sentence Encoder pre-trained module:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;embed_module = hub.Module(&amp;quot;https://tfhub.dev/google/universal-sentence-encoder/2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After the first use, the module is cached locally.&lt;/p&gt;
&lt;p&gt;The following code implements a method that accepts a list of sentences and returns a list of embeddings based on the &lt;code&gt;tf.Hub&lt;/code&gt; module:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def make_embeddings_fn():
    placeholder = tf.placeholder(dtype=tf.string)
    embed = embed_module(placeholder)
    session = tf.Session()
    session.run([tf.global_variables_initializer(), tf.tables_initializer()])
    def _embeddings_fn(sentences):
        computed_embeddings = session.run(
            embed, feed_dict={placeholder: sentences})
        return computed_embeddings
    return _embeddings_fn

generate_embeddings = make_embeddings_fn()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This method ensures that the &lt;code&gt;tf.Session&lt;/code&gt; is created and that the embedding module is loaded only once, not every time the method is called.&lt;/p&gt;
&lt;p&gt;The following code generates embeddings for the training phrases in the intents:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    intent: {
        training_phrase&#39;: [embedding_array]
    }
}

training_phrases_with_embeddings = defaultdict(list)
for intent_name, training_phrases_list in intent_training_phrases.items():
    computed_embeddings = generate_embeddings(training_phrases_list)
    training_phrases_with_embeddings[intent_name] = dict(zip(training_phrases_list, computed_embeddings))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This code snippet creates the &lt;code&gt;training_phrases_with_embeddings&lt;/code&gt; nested dictionary.&lt;/p&gt;
&lt;p&gt;The following code verifies the generated embeddings:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;training_phrases_with_embeddings = defaultdict(list)
for intent_name, training_phrases_list in intent_training_phrases.items():
    computed_embeddings = generate_embeddings(training_phrases_list)
    training_phrases_with_embeddings[intent_name] = dict(zip(training_phrases_list, computed_embeddings))
 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following code snippet shows each training phrase in the &lt;code&gt;start_conversation&lt;/code&gt; intent, along with the first five elements of the embedding vector of each phrase. The Universal Sentence Encoder generates a 512-dimension embedding vector for each training phrase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Ciao!:[-0.03649221  0.02498418 -0.03456857  0.02827227  0.00471277]
Howdy!:[-0.02732556 -0.00821852 -0.00794602  0.06356855 -0.03726532]
Hello!:[-0.0255452   0.00690543 -0.00611844  0.05633081 -0.0142823 ]
Hi!:[-0.03227544 -0.00985429 -0.01329378  0.06012927 -0.03646606]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;visualizing-embeddings-in-two-dimensional-space&#34;&gt;Visualizing embeddings in two-dimensional space&lt;/h3&gt;
&lt;p&gt;The following code reduces the dimensionality of the embeddings from 512 to 2 by using &lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis&lt;/a&gt; to compute the principal components:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.decomposition import PCA
embedding_vectors = None

for intent in training_phrases_with_embeddings:
    embeddings = list(training_phrases_with_embeddings[intent].values())
    if embedding_vectors is None:
        embedding_vectors = embeddings
    else:
        embedding_vectors = np.concatenate((only_embeddings, embeddings))

pca = PCA(n_components=3)
pca.fit(embedding_vectors)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This code snippet uses the &lt;code&gt;PCA&lt;/code&gt; class in &lt;code&gt;sklearn&lt;/code&gt; to generate a 2D representation of the training phrases embeddings.&lt;/p&gt;
&lt;p&gt;The following code generates a visualization of the phrase embeddings with the reduced dimensionality:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import matplotlib.pyplot as plt

fig = plt.figure(figsize=(15,10))
ax = fig.add_subplot(111)

legend = []

for color, intent in enumerate(training_phrases_with_embeddings):
    phrases = list(training_phrases_with_embeddings[intent].keys())
    embeddings = list(training_phrases_with_embeddings[intent].values())
    points = pca.transform(embeddings)
    xs = points[:,0]
    ys = points[:,1]
    ax.scatter(xs, ys, marker=&#39;o&#39;, s=100, c=&amp;quot;C&amp;quot;+str(color))
    for i, phrase in enumerate(phrases):
        ax.annotate(phrase, (xs[i], ys[i]))
    legend.append(intent)

ax.legend(legend)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following figure shows the resulting visualization: &lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-visualize-embeddings.png&#34; alt=&#34;Visualizing the phrase embeddings with the reduced dimensionality&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;computing-pairwise-similarity-between-phrases&#34;&gt;Computing pairwise similarity between phrases&lt;/h3&gt;
&lt;p&gt;The following code computes the pairwise cosine similarity for the training phrases embeddings, using &lt;code&gt;sklearn.metrics.pairwise.cosine_similarity&lt;/code&gt;. The code creates a Dataframe, &lt;code&gt;similarity_df&lt;/code&gt;, with the pairwise similarity values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.metrics.pairwise import cosine_similarity

flatten = []
for intent in training_phrases_with_embeddings:
        for phrase in training_phrases_with_embeddings[intent]:
            flatten.append((intent, phrase, training_phrases_with_embeddings[intent][phrase]))

data = []
for i in range(len(flatten)):
    for j in range(i+1, len(flatten)):
        intent_1 = flatten[i][0]
        phrase_1 = flatten[i][1]
        embedd_1 = flatten[i][2]
        intent_2 = flatten[j][0]
        phrase_2 = flatten[j][1]
        embedd_2 = flatten[j][2]
        similarity = cosine_similarity([embedd_1], [embedd_2])[0][0]
        record = [intent_1, phrase_1, intent_2, phrase_2, similarity]
        data.append(record)

similarity_df = pd.DataFrame(data,
    columns=[&amp;quot;Intent A&amp;quot;, &amp;quot;Phrase A&amp;quot;, &amp;quot;Intent B&amp;quot;, &amp;quot;Phrase B&amp;quot;, &amp;quot;Similarity&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following code displays sample similarity records:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;different_intent = similarity_df[&#39;Intent A&#39;] != similarity_df[&#39;Intent B&#39;]
display(similarity_df[different_intent].sort_values(&#39;Similarity&#39;,
ascending=False).head(5))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following code snippet shows the most similar training phrases that don&amp;rsquo;t belong to the same intent:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-similar-phrases.png&#34; alt=&#34;The most similar training phrases that don&amp;rsquo;t belong to the same intent&#34;&gt;&lt;/p&gt;
&lt;p&gt;Phrases in different intents that have high similarity value can be confusing to the Dialogflow agent, and could lead to directing the user input to the wrong intent.&lt;/p&gt;
&lt;h3 id=&#34;measuring-cohesion-and-separation-of-intents&#34;&gt;Measuring cohesion and separation of intents&lt;/h3&gt;
&lt;p&gt;The following code computes a cohesion value for each intent, as described in the &lt;a href=&#34;https://cloud.google.com/solutions/assessing-the-quality-of-training-phrases-in-dialogflow-intents#approach&#34;&gt;Approach section&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;same_intent = similarity_df[&#39;Intent A&#39;] == similarity_df[&#39;Intent B&#39;]
cohesion_df = pd.DataFrame(similarity_df[different_intent].groupby(&#39;Intent A&#39;, as_index=False)[&#39;Similarity&#39;].mean())
cohesion_df.columns = [&#39;Intent&#39;, &#39;Cohesion&#39;]
display(cohesion_df)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The result is a cohesion value for each intent:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-cohesion-values.png&#34; alt=&#34;Computing a cohesion value for each intent&#34;&gt;&lt;/p&gt;
&lt;p&gt;The following code computes the pairwise separation between intents, as described in the &lt;a href=&#34;https://cloud.google.com/solutions/assessing-the-quality-of-training-phrases-in-dialogflow-intents#approach&#34;&gt;Approach section&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;different_intent = similarity_df[&#39;Intent A&#39;] != similarity_df[&#39;Intent B&#39;]
separation_df = pd.DataFrame(similarity_df[different_intent].groupby([&#39;Intent A&#39;, &#39;Intent B&#39;], as_index=False)[&#39;Similarity&#39;].mean())
separation_df[&#39;Separation&#39;] = 1 - separation_df[&#39;Similarity&#39;]
del separation_df[&#39;Similarity&#39;]
display(separation_df.sort_values(&#39;Separation&#39;))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The result is the pairwise separation between intents:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/assessing-quality-of-training-phrases-pairwise-separation.png&#34; alt=&#34;Computing the pairwise separation between intents&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;further-improvements&#34;&gt;Further improvements&lt;/h2&gt;
&lt;p&gt;To improve the quality of the training phrases for your intents, consider the following approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find the phrases in different intents with high similarity, and change or remove them.&lt;/li&gt;
&lt;li&gt;Find the phrases with the most similar phrases that belong to different intents.&lt;/li&gt;
&lt;li&gt;Add more training phrases in intents with low cohesion, and investigate training phrases in intents with low separation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cleaning-up&#34;&gt;Cleaning up&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Caution&lt;/strong&gt;: Deleting a project has the following effects:&lt;strong&gt;Everything in the project is deleted.&lt;/strong&gt; If you used an existing project for this tutorial, when you delete it, you also delete any other work you&amp;rsquo;ve done in the project.&lt;strong&gt;Custom project IDs are lost.&lt;/strong&gt; When you created this project, you might have created a custom project ID that you want to use in the future. To preserve the URLs that use the project ID, such as an &lt;code&gt;appspot.com&lt;/code&gt; URL, delete selected resources inside the project instead of deleting the whole project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the Cloud Console, go to the&lt;/p&gt;
&lt;p&gt;Manage resources&lt;/p&gt;
&lt;p&gt;page.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/iam-admin/projects&#34;&gt;Go to Manage resources&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the project list, select the project that you want to delete, and then click &lt;strong&gt;Delete&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the dialog, type the project ID, and then click &lt;strong&gt;Shut down&lt;/strong&gt; to delete the project.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read the &lt;a href=&#34;https://cloud.google.com/solutions/machine-learning/overview-extracting-and-serving-feature-embeddings-for-machine-learning&#34;&gt;Overview of extracting and serving feature embeddings for machine learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For more details about embeddings and &lt;code&gt;tf.Hub&lt;/code&gt;, see &lt;a href=&#34;https://cloud.google.com/solutions/machine-learning/overview-extracting-and-serving-feature-embeddings-for-machine-learning&#34;&gt;Overview of extracting and serving feature embeddings for machine learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Learn about the &lt;a href=&#34;https://cloud.google.com/solutions/architecture-of-a-serverless-ml-model&#34;&gt;Architecture of a serverless machine learning model&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Learn about &lt;a href=&#34;https://cloud.google.com/solutions/comparing-ml-model-predictions-using-cloud-dataflow-pipelines&#34;&gt;Comparing machine learning models for predictions in Cloud Dataflow pipelines&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Take the 5-course Coursera specialization on Machine Learning with &lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-tensorflow-gcp&#34;&gt;TensorFlow on Google Cloud&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Learn about best practices for ML engineering in &lt;a href=&#34;https://developers.google.com/machine-learning/guides/rules-of-ml/&#34;&gt;Rules of ML&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Try out other Google Cloud features for yourself. Have a look at our &lt;a href=&#34;https://cloud.google.com/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/solutions/assessing-the-quality-of-training-phrases-in-dialogflow-intents&#34;&gt;Assessing the quality of training phrases in Dialogflow intents&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Joint intent and slot classification</title>
      <link>http://www.yezheng.pro/post/specialization/artificial-intelligence/chatbot/joint-intent-and-slot-classification/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/artificial-intelligence/chatbot/joint-intent-and-slot-classification/</guid>
      
        <description>&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/nlp/Joint_Intent_and_Slot_Classification.ipynb&#34;&gt;Joint_Intent_and_Slot_Classification.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Named Entity Recognition using Spacy and Tensorflow</title>
      <link>http://www.yezheng.pro/post/specialization/artificial-intelligence/natural-language-processing/named-entity-recognition/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/artificial-intelligence/natural-language-processing/named-entity-recognition/</guid>
      
        <description>&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://aihub.cloud.google.com/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba&#34;&gt;Named Entity Recognition using Spacy and Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Building a Recommendation System in TensorFlow</title>
      <link>http://www.yezheng.pro/post/specialization/artificial-intelligence/recommendation-system/building-a-recommendation-system-in-tensorflow/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/artificial-intelligence/recommendation-system/building-a-recommendation-system-in-tensorflow/</guid>
      
        <description>&lt;h2 id=&#34;background-theory-for-recommendations&#34;&gt;Background theory for recommendations&lt;/h2&gt;
&lt;p&gt;the background theory for matrix factorization-based collaborative filtering as applied to recommendation systems.&lt;/p&gt;
&lt;h3 id=&#34;collaborative-filtering-for-recommendation-systems&#34;&gt;Collaborative filtering for recommendation systems&lt;/h3&gt;
&lt;p&gt;Collaborative filtering relies only on &lt;strong&gt;observed user behavior&lt;/strong&gt; to make recommendations—no profile data or content access is necessary.&lt;/p&gt;
&lt;p&gt;The basic &lt;strong&gt;assumption&lt;/strong&gt; is that similar user behavior reflects similar fundamental preferences, allowing a recommendation engine to make suggestions accordingly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For example&lt;/strong&gt;, suppose User 1 has viewed items A, B, C, D, E, and F. User 2 has viewed items A, B, D, E and F, but not C. Because both users viewed five of the same six items, it&amp;rsquo;s likely that they share some basic preferences. User 1 liked item C, and it&amp;rsquo;s probable that User 2 would also like item C if the user were aware of its existence.&lt;/p&gt;
&lt;h3 id=&#34;matrix-factorization-for-collaborative-filtering&#34;&gt;Matrix factorization for collaborative filtering&lt;/h3&gt;
&lt;p&gt;The collaborative filtering problem can be solved using matrix factorization.&lt;/p&gt;
&lt;p&gt;Suppose you have a &lt;strong&gt;matrix&lt;/strong&gt; consisting of &lt;strong&gt;user IDs&lt;/strong&gt; and their interactions with your products. &lt;strong&gt;Each row&lt;/strong&gt; corresponds to a unique user, and &lt;strong&gt;each column&lt;/strong&gt; corresponds to an &lt;strong&gt;item&lt;/strong&gt;. Each entry in the matrix captures a user&amp;rsquo;s rating or preference for a single item. The rating could be &lt;strong&gt;explicit&lt;/strong&gt;, directly generated by user feedback, or it could be &lt;strong&gt;implicit&lt;/strong&gt;, based on user purchases or time spent interacting with an article or video.&lt;/p&gt;
&lt;p&gt;Ratings in the &lt;code&gt;MovieLens&lt;/code&gt; dataset range from 1 to 5. Empty rating entries have value 0, meaning that a given user hasn&amp;rsquo;t rated the item.&lt;/p&gt;
&lt;h4 id=&#34;defining-the-matrix-factorization-method&#34;&gt;Defining the matrix factorization method&lt;/h4&gt;
&lt;p&gt;A ratings matrix consists of a matrix $R$, where entries $r_{ij}$ are ratings of user $i$ for item $j$.&lt;/p&gt;
&lt;p&gt;The matrix factorization method &lt;strong&gt;assumes&lt;/strong&gt; that there is a set of &lt;strong&gt;attributes&lt;/strong&gt; common to all items, with &lt;strong&gt;items differing&lt;/strong&gt; in the &lt;strong&gt;degree&lt;/strong&gt; to which they &lt;strong&gt;express these attributes&lt;/strong&gt;. Furthermore, the matrix factorization method &lt;strong&gt;assumes&lt;/strong&gt; that each user has their own expression for each of these attributes, independent of the items.  In this way, a &lt;strong&gt;user&amp;rsquo;s item rating can be approximated by summing&lt;/strong&gt; the user&amp;rsquo;s strength for each attribute weighted by the degree to which the item expresses this attribute. These attributes are sometimes called &lt;strong&gt;hidden or &lt;em&gt;latent&lt;/em&gt; factors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Intuitively, it&amp;rsquo;s easy to see that these hypothetical latent factors actually exist. In the case of movies, it&amp;rsquo;s clear that many users prefer certain genres, actors, or directors.  Much of the power of the matrix factorization approach to collaborative filtering derives from the fact that it&amp;rsquo;s not necessary to know the number of  hypothetical latent factors that might comprise the entirety of a given user&amp;rsquo;s preferences. It&amp;rsquo;s &lt;strong&gt;simply assumed there are an arbitrary number of hypothetical latent factors&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;transforming-the-matrix-to-represent-latent-factors&#34;&gt;Transforming the matrix to represent latent factors&lt;/h4&gt;
&lt;p&gt;To translate the existence of latent factors into the matrix of ratings, you do this: for a set of users $U$ of size $u$ and items $I$ of size $i$, you &lt;strong&gt;pick an arbitrary number $k$ of latent factors&lt;/strong&gt; and &lt;strong&gt;factorize&lt;/strong&gt; the large matrix $R$ into two much smaller matrices $X$ (the &amp;ldquo;row factor&amp;rdquo;) and $Y$ (the &amp;ldquo;column factor&amp;rdquo;). Matrix $X$ has dimension $u × k$, and $Y$ has dimension $k × i$.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;In linear algebra this is called a &lt;strong&gt;low-rank approximation&lt;/strong&gt;. Every user is represented by a vector in this $k$-dimensional space, and each row $x_u$ in $X$ corresponds to the strength of the user&amp;rsquo;s preferences for these $k$ factors. Similarly, every item represented by a vector in this $k$-dimensional space has a column $y_i$ in $Y$ corresponding to the item&amp;rsquo;s expression of the same $k$ factors.&lt;/p&gt;
&lt;p&gt;To calculate the predicted rating of user &lt;em&gt;u&lt;/em&gt; for item &lt;em&gt;i&lt;/em&gt;, you take the dot product of the two vectors:
$$
r_{ui} = x_u^T . y_i
$$
You can define a loss function measuring the accuracy of the approximation in the following way, sum the squared difference between the approximate rating $x_u^T⋅y_i$ and the actual rating from the training set $r_{ui}$.:
$$
L=\sum_{u,i}(r_{ui}−x_u^T⋅y_i)^2
$$
It&amp;rsquo;s common practice to add **regularization terms** to this loss function to help prevent **overfitting**. Adding **L2 regularization terms** for both row and column factors gives the following:
$$
L=\sum_{u,i}(r_{ui}−x_u^T⋅y_i)^2 + \lambda\sum_{u}||x_u||^2 + \lambda\sum_{i}||y_i||^2
$$
Here, $λ$ is a **regularization constant**, one of the model’s hyperparameters.&lt;/p&gt;
&lt;h3 id=&#34;the-wals-method-of-matrix-factorization&#34;&gt;The WALS method of matrix factorization&lt;/h3&gt;
&lt;h4 id=&#34;alternating-least-squares-method-交替最小二乘法&#34;&gt;Alternating least squares method (交替最小二乘法)&lt;/h4&gt;
&lt;p&gt;The alternating least squares method of matrix factorization is an &lt;strong&gt;iterative&lt;/strong&gt; method for determining the &lt;strong&gt;optimal factors&lt;/strong&gt; $X$ and $Y$ that best approximate $R$. In each iteration, &lt;strong&gt;one of&lt;/strong&gt; the row or column factors &lt;strong&gt;is held fixed&lt;/strong&gt; and &lt;strong&gt;the other is computed&lt;/strong&gt; by minimizing the loss function with respect to the other factor.&lt;/p&gt;
&lt;p&gt;First, you begin with the row factors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set the column factors to constant values.&lt;/li&gt;
&lt;li&gt;Take the derivative of the loss function with respect to the row factors.&lt;/li&gt;
&lt;li&gt;Set the equation equal to zero.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\frac {\partial{L}} {\partial{x_u}} = −2 \sum_i (r_{ui} − x_u^T⋅y_i)y_i^T + 2\lambda_{xu}^T
$$&lt;/p&gt;
&lt;p&gt;$$
0 = −(r_u − x_u^TY^T)Y + \lambda x_u^T
$$&lt;/p&gt;
&lt;p&gt;$$
x_u^T(Y^TY + \lambda I) = r_uY
$$&lt;/p&gt;
&lt;p&gt;$$
x_u^T = r_uY(Y^TY + \lambda I)^{−1}
$$&lt;/p&gt;
&lt;p&gt;Alternating row and column factors, the iterative process is repeated until convergence.&lt;/p&gt;
&lt;h4 id=&#34;weighted-alternating-least-squares-wals-method-加权交替最小二乘法&#34;&gt;Weighted alternating least squares (WALS) method (加权交替最小二乘法)&lt;/h4&gt;
&lt;p&gt;$$
L^w = {W} \sum_{u,i}(r_{ui}−x_u^T⋅y_i)^2
$$&lt;/p&gt;
&lt;p&gt;In this equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$$w_{ui} = \omega_{0}$$                                  for zero (unobserved) entries in the ratings matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$$w_{ui} = \omega_{0} + f(c_{i})$$                   for observed entries&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$$c_{i} = \sum_{u,i}1 \text{  if  } r_{ui} &amp;gt; 0$$            the sum of the number of nonzero entries for column $i$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The weight is scaled by the sum of the nonzero entries in a row to normalize the weight for users who have rated a different number of items. This type of weighting allows for a more flexible model of the user&amp;rsquo;s preferences and produces better empirical results than the unweighted version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The function $f$ varies&lt;/strong&gt; according to the dataset and whether ratings are explicit or implicit.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;MovieLens&lt;/code&gt; dataset contains &lt;strong&gt;explicit ratings&lt;/strong&gt;. In this case, a better choice for $f$ is one that weighs the observed entries linearly:&lt;/p&gt;
&lt;p&gt;$$f = \omega_{k}c_{i}$$   where $\omega_{k}$ is the observed weight.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &lt;strong&gt;implicit ratings&lt;/strong&gt; related to dynamic events, where each rating corresponds to the number of times a video has been watched, an article read, or a web page viewed, the rating itself may have an exponential distribution due to user behavior. There will be many low-value ratings as users click on a page or video and navigate away quickly. There will be fewer large implicit ratings as users read an entire article, watch an entire video, or watch a given scheduled show multiple times. In this case, an appropriate $f$ is one that weights the observed ratings to account &lt;strong&gt;for this distribution&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$f = \left(\frac{1}{c_{i}}\right)^{e}$   where $e$ is the feature weight exponent.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wals-compared-to-other-techniques&#34;&gt;WALS compared to other techniques&lt;/h3&gt;
&lt;p&gt;Many matrix factorization techniques are used for collaborative filtering, including &lt;strong&gt;SVD&lt;/strong&gt; and &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt;. In some cases these techniques give better reduced-rank approximations than WALS. It&amp;rsquo;s worth noting the following advantages of WALS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The weights used in WALS make it suitable for &lt;strong&gt;implicit ratings&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WALS includes &lt;strong&gt;algorithmic optimizations&lt;/strong&gt; that make it easy to incorporate weights and efficiently calculate row and column factor updates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s relatively straightforward to create a &lt;strong&gt;distributed version&lt;/strong&gt; of the algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;create-the-model&#34;&gt;Create the Model&lt;/h2&gt;
&lt;h3 id=&#34;create-docker-image-environment&#34;&gt;Create Docker image environment&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull tensorflow/tensorflow:1.15.4-py3-jupyter
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;git clone https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals
cd tensorflow-recommendation-wals
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;vim Dockerfile
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# Use an official tensorflow runtime as a parent image
FROM tensorflow/tensorflow:1.15.4-py3-jupyter

# Set the working directory to /app
WORKDIR /app

# Copy the requirement list into the container at /app
ADD requirements.txt /app

# Change mirrors and install vim
RUN sed -i &amp;quot;s@http://archive.ubuntu.com@http://mirrors.tuna.tsinghua.edu.cn@g&amp;quot; /etc/apt/sources.list &amp;amp;&amp;amp; sed -i &amp;quot;s@http://security.ubuntu.com@http://mirrors.tuna.tsinghua.edu.cn@g&amp;quot; /etc/apt/sources.list &amp;amp;&amp;amp; rm -Rf /var/lib/apt/lists/* &amp;amp;&amp;amp; apt-get update &amp;amp;&amp;amp; apt-get install -y vim

# Upgrading pip
RUN pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U

# set package index url
RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# Install any needed packages specified in requirements.txt
RUN pip install -r requirements.txt

# Make port 8888 available to the world outside this container
EXPOSE 8888
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;docker build -t tfrec:latest --no-cache --network=host .
docker image ls
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;download-the-movielens-dataset&#34;&gt;Download the &lt;code&gt;MovieLens&lt;/code&gt; dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;curl -O &#39;http://files.grouplens.org/datasets/movielens/ml-100k.zip&#39;
umzip ml-100k.zip
mkdir /data/datasets/ml-100k
cp ml-100k/u.data /data/datasets/ml-100k/
rm -r ml-100k ml-100k.zip
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;run-jupyter-notebook&#34;&gt;Run Jupyter notebook&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker run -it --rm -v $PWD:/app -v /data/datasets/ml-100k:/datasets -w /app -p 8888:8888 tfrec:latest bash
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;jupyter notebook --ip 0.0.0.0 --no-browser --allow-root ./notebooks/Part1.ipynb
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;train-and-tune-the-model&#34;&gt;Train and tune the model&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;cd wals_ml_engine
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;docker run -it --rm -v $PWD:/app -v /data/datasets/ml-100k:/datasets -w /app tfrec:latest python -m trainer.task --job-dir=/app --train-file=/datasets/u.data --data-type=ratings --delimiter=&#39;\t&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Install miniconda&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yay -S miniconda3
$ echo &amp;quot;export PATH=/opt/miniconda3/bin/:$PATH&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ echo &amp;quot;[ -f /opt/miniconda3/etc/profile.d/conda.sh ] &amp;amp;&amp;amp; source /opt/miniconda3/etc/profile.d/conda.sh&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Install the Python packages and TensorFlow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda create -y -n tfrec                     # -y:  Do not ask for confirmation, -n: Name of environment.
$ conda install -y -n tfrec --file conda.txt   # --file FILE: Read package versions from the given file
$ source activate tfrec
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;$ conda deactivate
$ conda remove -n tfrec --all
$ conda create -y -n tfrec python=3.7          # with a specific version of Python
$ pip install -r requirements.txt
$ pip install tensorflow==1.15
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;api-and-predict&#34;&gt;API and Predict&lt;/h2&gt;
&lt;p&gt;ensure that the application bind to &lt;code&gt;0.0.0.0&lt;/code&gt; and not &lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it --rm -v $PWD:/app -v /data/datasets/ml-100k:/datasets -w /app tfrec:latest python -m trainer.task --job-dir=/app --train-file=/datasets/u.data --data-type=ratings --delimiter=&#39;\t&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;deploy-the-recommendation-system&#34;&gt;Deploy the Recommendation System&lt;/h2&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/solutions/machine-learning/recommendation-system-tensorflow-overview&#34;&gt;Building a Recommendation System in TensorFlow&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/recommendation&#34;&gt;&lt;strong&gt;Recommendation Systems&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://aihub.cloud.google.com/p/products%2F88c7d41c-bc75-4e36-a966-50a0aa38fa2a&#34;&gt;Recommender System with Matrix Factorization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.gcp.expert/recommendation-system-tensorflow-overview/&#34;&gt;如何在 TensorFlow 內建立推薦系統：總覽&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/19711823&#34;&gt;有哪些好用的开源推荐系统？&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/models/tree/08bb9eb5ad79e6bceffc71aeea6af809cc78694b/official/recommendation&#34;&gt;Recommendation Model&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/57902387/matrix-factorization-in-tensorflow-2-0-using-wals-method&#34;&gt;Matrix Factorization in tensorflow 2.0 using WALS Method&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;reviewing &lt;a href=&#34;https://github.com/tensorflow/models/tree/08bb9eb5ad79e6bceffc71aeea6af809cc78694b/official/recommendation&#34;&gt;https://github.com/tensorflow/models/tree/08bb9eb5ad79e6bceffc71aeea6af809cc78694b/official/recommendation&lt;/a&gt; for how to get started&lt;/p&gt;
&lt;p&gt;I was able to dig around and find a very similar approach in the tutorials
&lt;a href=&#34;https://developers.google.com/machine-learning/recommendation/dnn/softmax&#34;&gt;https://developers.google.com/machine-learning/recommendation/dnn/softmax&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(props to whoever put those tutorials together was helpful getting another look at the approach and tradeoffs)&lt;/p&gt;
&lt;p&gt;note for anyone else looking for example apps I bumped into a couple by Nvidia
&lt;a href=&#34;https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF&#34;&gt;https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF&lt;/a&gt;
&lt;a href=&#34;https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/NCF&#34;&gt;https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/NCF&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;cool overview for intel&amp;rsquo;s analytics zoo (office depot) across models MF, ncf, wide &amp;amp; deep, and session recommender
&lt;a href=&#34;https://software.intel.com/en-us/articles/real-time-product-recommendations-for-office-depot-using-apache-spark-and-analytics-zoo-on&#34;&gt;https://software.intel.com/en-us/articles/real-time-product-recommendations-for-office-depot-using-apache-spark-and-analytics-zoo-on&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;it looks like Google&amp;rsquo;s official example leverages apache spark&amp;rsquo;s als model
&lt;a href=&#34;https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine&#34;&gt;https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>data lakes</title>
      <link>http://www.yezheng.pro/post/specialization/big-data/data-lakes/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/big-data/data-lakes/</guid>
      
        <description>&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://delta.io/&#34;&gt;Delta Lake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html&#34;&gt;What is a Lakehouse?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>edX Analytics Pipeline</title>
      <link>http://www.yezheng.pro/post/specialization/big-data/edx-analytics-pipeline/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/big-data/edx-analytics-pipeline/</guid>
      
        <description>&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://edx.readthedocs.io/projects/edx-installing-configuring-and-running/en/latest/insights/install_insights.html&#34;&gt;edX Insights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://edx-analytics-pipeline-reference.readthedocs.io/en/latest/index.html&#34;&gt;edX Analytics Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/edx/edx-analytics-pipeline&#34;&gt;edx-analytics-pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spotify/luigi&#34;&gt;luigi&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Machine Learning Models for Predictions in Cloud Dataflow Pipelines</title>
      <link>http://www.yezheng.pro/post/specialization/artificial-intelligence/deep-learning/ml-model-predictions-in-dataflow-pipelines/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/artificial-intelligence/deep-learning/ml-model-predictions-in-dataflow-pipelines/</guid>
      
        <description>&lt;p&gt;This solution describes and compares the different design approaches for calling a machine learning model during a Dataflow pipeline, and examines the tradeoffs involved in choosing one approach or another. We present the results of a series of experiments that we ran to explore different approaches and illustrate these tradeoffs, both in batch and stream processing pipelines. This solution is designed for people who integrate trained models into data processing pipelines, rather than for data scientists who want to build machine learning models.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As the person responsible for integrating this ML model into the Dataflow pipeline, you might wonder what the various approaches are, and which one suits best system requirements. Several considerations need your attention, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Throughput&lt;/li&gt;
&lt;li&gt;Latency&lt;/li&gt;
&lt;li&gt;Cost&lt;/li&gt;
&lt;li&gt;Implementation&lt;/li&gt;
&lt;li&gt;Maintenance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s not always easy to balance these considerations, but this solution can help you navigate the decision-making process based on your priorities. The solution compares three approaches for making predictions with a TensorFlow-trained machine learning (ML) model in batch and stream data pipelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using a deployed model as a REST/HTTP API for streaming pipelines.&lt;/li&gt;
&lt;li&gt;Using AI Platform (AI Platform) batch prediction jobs for batch pipelines.&lt;/li&gt;
&lt;li&gt;Using Dataflow direct-model prediction for both batch and streaming pipelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the experiments use an existing trained model, called the Natality dataset, which predicts baby weights based on various inputs. Because the goal of this solution is not to build a model, it doesn&amp;rsquo;t discuss how the model was built or trained. See the &lt;a href=&#34;https://cloud.google.com/solutions/comparing-ml-model-predictions-using-cloud-dataflow-pipelines#heading=h.eskc9ld7ie95&#34;&gt;Next Steps&lt;/a&gt; section for more details about the Natality dataset.&lt;/p&gt;
&lt;h2 id=&#34;platform&#34;&gt;Platform&lt;/h2&gt;
&lt;p&gt;There are various ways to run a data pipeline and call a trained ML model. But the functional requirements are always the same:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Ingesting&lt;/em&gt; data from a bounded (batch) or unbounded (streaming) source. Examples of sources from which to ingest data include sensor data, website interactions, and financial transactions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transforming and enriching&lt;/em&gt; the input data by calling ML models for predictions. An example is parsing a JSON file to extract relevant fields to predict a maintenance date, make a product recommendation, or detect fraud.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Storing&lt;/em&gt; the transformed data and predictions for analytics or for backup, or to pass to a queuing system to trigger a new event or additional pipelines. Examples include detecting potential fraud in real time or storing maintenance schedule information in a store that&amp;rsquo;s accessible from a dashboard.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you transform and enrich data with predictions in a batch ETL process, you aim for maximizing throughputs in order to reduce the overall amount of time needed for the whole data batch. On the other hand, when you process streaming data for online prediction, you aim for minimizing latency in order to receive each prediction in (near) real time. Thus, calling the model might become a bottleneck.&lt;/p&gt;
&lt;h3 id=&#34;core-components&#34;&gt;Core components&lt;/h3&gt;
&lt;p&gt;The batch and streaming experiments in this solution use three main technologies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://beam.apache.org/&#34;&gt;Apache Beam&lt;/a&gt; running on Dataflow to process the data.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt; to implement and train the ML model.&lt;/li&gt;
&lt;li&gt;For some experiments, &lt;a href=&#34;https://cloud.google.com/ml-engine&#34;&gt;AI Platform&lt;/a&gt; as a hosting platform for the trained ML models to perform batch and online predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We chose &lt;strong&gt;Apache Beam running on Dataflow&lt;/strong&gt; to run data pipelines in this solution because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Beam is an open-source unified programming model that runs both streaming and batch data processing jobs.&lt;/li&gt;
&lt;li&gt;Dataflow is a Google Cloud product that can run Apache Beam jobs without a server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt; is an open-source mathematical library by Google that is used as a machine learning framework. TensorFlow enables building, training, and serving models on a single machine or in distributed environments. Models are portable to various devices and can also leverage available &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit&#34;&gt;CPU&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Graphics_processing_unit&#34;&gt;GPU&lt;/a&gt;, or &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor_processing_unit&#34;&gt;TPU&lt;/a&gt; resources for training and serving.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI Platform&lt;/strong&gt; is a serverless platform that can train, tune (using the hyper-parameters tuning functionality), and serve TensorFlow models at scale with minimum management required by DevOps. AI Platform supports deploying trained models as REST APIs for online predictions, as well as submitting batch prediction jobs. AI Platform is one of several options that can serve your model as a microservice.&lt;/p&gt;
&lt;p&gt;The approaches detailed in this solution use Dataflow for the data processing pipeline and AI Platform to host the models as HTTP endpoints. However, these approaches could be replaced with other technologies. The performance comparisons between HTTP and a direct TensorFlow model would not drastically change.&lt;/p&gt;
&lt;h3 id=&#34;processing-batch-and-streaming-data&#34;&gt;Processing batch and streaming data&lt;/h3&gt;
&lt;p&gt;The experiments in this solution include both batch and streaming use cases. Each experiment leverages different Google Cloud products for input and output because unbounded and bounded sources have different operational requirements.&lt;/p&gt;
&lt;h4 id=&#34;batch-processing-a-bounded-dataset&#34;&gt;Batch-processing a bounded dataset&lt;/h4&gt;
&lt;p&gt;Figure 1 shows that in typical batch processing pipelines, raw input data is stored in object storage, such as &lt;a href=&#34;https://cloud.google.com/storage&#34;&gt;Cloud Storage&lt;/a&gt;. Structured data storage formats include comma-separated values (CSV), optimized row columnar (ORC), Parquet, or Avro. These formats are often used when data comes from databases or logs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-1-batch-processing.svg&#34; alt=&#34;Architecture of typical batch processing pipelines&#34;&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Batch-processing architecture&lt;/p&gt;
&lt;p&gt;Some analytical platforms such as &lt;a href=&#34;https://cloud.google.com/bigquery&#34;&gt;BigQuery&lt;/a&gt; provide storage in addition to query capabilities. BigQuery uses &lt;a href=&#34;https://cloud.google.com/blog/big-data/2016/04/inside-capacitor-bigquerys-next-generation-columnar-storage-format&#34;&gt;Capacitor&lt;/a&gt; for storage. Apache Beam on Dataflow can read from and write to both BigQuery and Cloud Storage, in addition to other storage options in batch processing pipelines.&lt;/p&gt;
&lt;h4 id=&#34;stream-processing-an-unbounded-datastream&#34;&gt;Stream-processing an unbounded datastream&lt;/h4&gt;
&lt;p&gt;For streaming, the inputs to a data processing pipeline are usually a messaging system, as shown in Figure 2. Technologies such as &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; or Kafka are typically used to ingest individual data points in JSON, CSV, or protobuf format.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-2-stream-processing.svg&#34; alt=&#34;Architecture of typical stream processing pipelines&#34;&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; Stream-processing architecture&lt;/p&gt;
&lt;p&gt;Data points can be processed individually, or as groups in micro-batches by using &lt;a href=&#34;https://beam.apache.org/documentation/programming-guide/#windowing&#34;&gt;windowing&lt;/a&gt; functions to perform temporal event processing. The processed data might go to several destinations, including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;BigQuery for ad hoc analytics, through the streaming APIs.&lt;/li&gt;
&lt;li&gt;Cloud Bigtable for serving real-time information.&lt;/li&gt;
&lt;li&gt;Pub/Sub topic for triggering subsequent processes/pipelines.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can find a complete list of source connectors (input) and sink connectors (output) for both bounded and unbounded data source sinks on the &lt;a href=&#34;https://beam.apache.org/documentation/io/built-in/&#34;&gt;Apache Beam I/O page&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;invoking-a-tensorflow-model&#34;&gt;Invoking a TensorFlow model&lt;/h3&gt;
&lt;p&gt;A TensorFlow-trained model can be invoked in three ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Through an HTTP endpoint for &lt;strong&gt;online&lt;/strong&gt; prediction.&lt;/li&gt;
&lt;li&gt;Directly, by using the saved model file for &lt;strong&gt;batch&lt;/strong&gt; and &lt;strong&gt;online&lt;/strong&gt; predictions.&lt;/li&gt;
&lt;li&gt;Through an AI Platform batch prediction job for &lt;strong&gt;batch&lt;/strong&gt; prediction.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;http-endpoints-for-online-prediction&#34;&gt;HTTP endpoints for online prediction&lt;/h4&gt;
&lt;p&gt;TensorFlow models are deployed as HTTP endpoints to be invoked and give predictions in real time, either through a stream data processing pipeline or through client apps.&lt;/p&gt;
&lt;p&gt;You can deploy a TensorFlow model as an HTTP endpoint for online predictions by using &lt;a href=&#34;https://github.com/tensorflow/serving&#34;&gt;TensorFlow Serving&lt;/a&gt; or any other hosting service, such as &lt;a href=&#34;https://www.seldon.io/&#34;&gt;Seldon&lt;/a&gt;. As shown in Figure 3, you can choose one of the following options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploy the model yourself on one or more Compute Engine instances.&lt;/li&gt;
&lt;li&gt;Use a &lt;a href=&#34;https://github.com/tensorflow/serving/tree/master/tensorflow_serving/tools/docker&#34;&gt;Docker image&lt;/a&gt; on &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/deploying-containers&#34;&gt;Compute Engine&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Leverage &lt;a href=&#34;https://github.com/kubeflow/kubeflow&#34;&gt;Kubeflow&lt;/a&gt; to facilitate deployment on &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; or Google Kubernetes Engine.&lt;/li&gt;
&lt;li&gt;Use App Engine with Endpoints to host the model in a web app.&lt;/li&gt;
&lt;li&gt;Use &lt;a href=&#34;https://cloud.google.com/ml-engine&#34;&gt;AI Platform&lt;/a&gt;, the fully managed ML training and serving service on Google Cloud.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-3-different-models-for-http-endpoints.svg&#34; alt=&#34;Options in Dataflow for serving a model as an HTTP endpoint&#34;&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Different options in Dataflow for serving a model as an HTTP endpoint&lt;/p&gt;
&lt;p&gt;AI Platform is a fully managed service, so it is easier to implement than the other options. Therefore, in our experiments we use it as the option for serving the model as an HTTP endpoint. We can then focus on the performance of a direct-model versus an HTTP endpoint in AI Platform, rather than comparing the different HTTP model-serving options.&lt;/p&gt;
&lt;h4 id=&#34;serving-online-predictions-with-ai-platform-prediction&#34;&gt;Serving online predictions with AI Platform Prediction&lt;/h4&gt;
&lt;p&gt;Two tasks are required in order to serve online predictions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploying a model.&lt;/li&gt;
&lt;li&gt;Interacting with the deployed model for inference (that is, making predictions).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models&#34;&gt;Deploying a model&lt;/a&gt; as an HTTP endpoint using AI Platform Prediction requires the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure that the trained model files are available on Cloud Storage.&lt;/li&gt;
&lt;li&gt;Create a model by using the &lt;code&gt;gcloud ml-engine models create&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Deploy a model version by using the &lt;code&gt;gcloud ml-engine versions create&lt;/code&gt; command, with the model files on Cloud Storage.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can deploy a model by using commands like the following:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/scripts/deploy-cmle.sh&#34;&gt;blogs/tf_dataflow_serving/scripts/deploy-cmle.sh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/scripts/deploy-cmle.sh&#34;&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;PROJECT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[PROJECT_ID]&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# change to your project name&lt;/span&gt;
REGION&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[REGION]&amp;#34;&lt;/span&gt;
BUCKET&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[BUCKET]&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# change to your bucket name&lt;/span&gt;
MODEL_NAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;babyweight_estimator&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# change to your estimator name&lt;/span&gt;
MODEL_VERSION&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# change to your model version&lt;/span&gt;
MODEL_BINARIES&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;gs://&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;BUCKET&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/models/&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MODEL_NAME&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# upload the local SavedModel to GCS&lt;/span&gt;
gsutil -m cp -r model/trained/v1/* gs://&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;BUCKET&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/models/&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MODEL_NAME&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# set the current project&lt;/span&gt;
gcloud config set project &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;PROJECT&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# list model files on GCS&lt;/span&gt;
gsutil ls &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MODEL_BINARIES&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# deploy model to GCP&lt;/span&gt;
gcloud ml-engine models create &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MODEL_NAME&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt; --regions&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;REGION&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# deploy model version&lt;/span&gt;
gcloud ml-engine versions create &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MODEL_VERSION&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt; --model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MODEL_NAME&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt; --origin&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MODEL_BINARIES&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt; --runtime-version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The code creates an AI Platform Prediction model called babyweight_estimator in the Google Cloud project, with model version v1.&lt;/p&gt;
&lt;p&gt;After the model is deployed, you can invoke it. The following Python code shows a way to invoke a model version in AI Platform Prediction as a REST API:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/model/inference.py&#34;&gt;blogs/tf_dataflow_serving/model/inference.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/model/inference.py&#34;&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;cmle_api &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;init_api&lt;/span&gt;():

    &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; cmle_api

    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; cmle_api &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None:
        cmle_api &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; discovery&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;build(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ml&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;v1&amp;#39;&lt;/span&gt;,
                              discoveryServiceUrl&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json&amp;#39;&lt;/span&gt;,
                              cache_discovery&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)


&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;estimate_cmle&lt;/span&gt;(instances):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Calls the babyweight estimator API on CMLE to get predictions
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Args:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;       instances: list of json objects
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Returns:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        int - estimated baby weight
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    init_api()

    request_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;instances&amp;#39;&lt;/span&gt;: instances}

    model_url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;projects/{}/models/{}/versions/{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(PROJECT, CMLE_MODEL_NAME, CMLE_MODEL_VERSION)
    response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cmle_api&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;projects()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(body&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;request_data, name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;model_url)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;execute()
    values &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;predictions&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; response[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;predictions&amp;#39;&lt;/span&gt;]]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; values
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you have a large dataset available in something like BigQuery or Cloud Storage and you want to maximize the throughput of the overall process, serving your ML model as an HTTP endpoint is not recommended for batch prediction. Doing this generates one HTTP request for each data point, which results in a huge volume of HTTP requests. The following section presents better options for batch prediction.&lt;/p&gt;
&lt;h4 id=&#34;direct-model-for-batch-and-online-predictions&#34;&gt;Direct-model for batch and online predictions&lt;/h4&gt;
&lt;p&gt;The direct-model prediction technique leverages a local TensorFlow &lt;code&gt;SavedModel&lt;/code&gt; on the Dataflow instances. The saved model is a copy of the output files created after you have finished building and training the TensorFlow model. The TensorFlow &lt;code&gt;SavedModel&lt;/code&gt; can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part of the pipeline source code that is submitted as a Dataflow job.&lt;/li&gt;
&lt;li&gt;Downloaded from Cloud Storage, as shown in Figure 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-4-direct-model-prediction.png&#34; alt=&#34;Direct-model prediction in Dataflow&#34;&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Direct-model prediction in Dataflow&lt;/p&gt;
&lt;p&gt;In this solution, we use a &lt;code&gt;SavedModel&lt;/code&gt; that is part of the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/tf_dataflow_serving/model&#34;&gt;source code&lt;/a&gt; on GitHub. To load a model on the instances, you do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;When you create the Dataflow job, specify the pipeline dependencies to be loaded, including the model file. The following Python code shows the &lt;code&gt;setup.py&lt;/code&gt; file that includes the model files to be submitted with the Dataflow job.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/setup.py&#34;&gt;blogs/tf_dataflow_serving/setup.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/setup.py&#34;&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; setuptools
   
requirements &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
   
setuptools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup(
    name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;TF-DATAFLOW-DEMO&amp;#39;&lt;/span&gt;,
    version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;v1&amp;#39;&lt;/span&gt;,
    install_requires&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;requirements,
    packages&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;setuptools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;find_packages(),
    package_data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;model&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;trained/*&amp;#39;&lt;/span&gt;,
                            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;trained/v1/*&amp;#39;&lt;/span&gt;,
                            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;trained/v1/variables/*&amp;#39;&lt;/span&gt;]
                  },
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Call the local model files during the pipeline. This produces the prediction for the given instances. The following Python code shows how to do this.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/model/inference.py&#34;&gt;blogs/tf_dataflow_serving/model/inference.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/model/inference.py&#34;&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;predictor_fn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
   
   
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;init_predictor&lt;/span&gt;():
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34; Loads the TensorFlow saved model to the predictor object
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;   
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Returns:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        predictor_fn
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
   
    &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; predictor_fn
   
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; predictor_fn &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None:
   
        logging&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Initialising predictor...&amp;#34;&lt;/span&gt;)
        dir_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dirname(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;realpath(__file__))
        export_dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(dir_path, SAVED_MODEL_DIR)
   
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists(export_dir):
            predictor_fn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;contrib&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predictor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_saved_model(
                export_dir&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;export_dir,
                signature_def_key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;predict&amp;#34;&lt;/span&gt;
            )
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            logging&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;error(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Model not found! - Invalid model path: {}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(export_dir))
   
   
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;estimate_local&lt;/span&gt;(instances):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Calls the local babyweight estimator to get predictions
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;   
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Args:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;       instances: list of json objects
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Returns:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        int - estimated baby weight
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
   
    init_predictor()
   
    inputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dict((k, [v]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; instances[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items())
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,len(instances)):
        instance &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; instances[i]
   
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; instance&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items():
            inputs[k] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; [v]
   
    values &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; predictor_fn(inputs)[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;predictions&amp;#39;&lt;/span&gt;]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [value&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item() &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; value &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; values&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See the &lt;a href=&#34;https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/#multiple-file-dependencies&#34;&gt;Apache Beam Multiple File Dependencies&lt;/a&gt; page for more details.&lt;/p&gt;
&lt;h4 id=&#34;ai-platform-batch-prediction-job&#34;&gt;AI Platform batch prediction job&lt;/h4&gt;
&lt;p&gt;Besides deploying the model as an HTTP endpoint, AI Platform lets you run a &lt;a href=&#34;https://cloud.google.com/ml-engine/docs/tensorflow/batch-predict&#34;&gt;batch prediction job&lt;/a&gt; by using a deployed model version or a TensorFlow &lt;code&gt;SavedModel&lt;/code&gt; in Cloud Storage.&lt;/p&gt;
&lt;p&gt;An AI Platform batch prediction job takes the Cloud Storage location of the input data files as a parameter. It uses the model to get predictions for that data, and then stores the prediction results in another Cloud Storage output location that is also given as a parameter. The following example shows &lt;code&gt;gcloud&lt;/code&gt; commands that submit an AI Platform batch prediction job.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/scripts/predict-batch-cmle.sh&#34;&gt;blogs/tf_dataflow_serving/scripts/predict-batch-cmle.sh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/scripts/predict-batch-cmle.sh&#34;&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;BUCKET&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;BUCKET&amp;gt;&amp;#39;&lt;/span&gt;
DATA_FORMAT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;TEXT&amp;#34;&lt;/span&gt;
INPUT_PATHS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;gs://&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;BUCKET&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/data/babyweight/experiments/outputs/data-prep-*
OUTPUT_PATH&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;gs://&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;BUCKET&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/data/babyweight/experiments/outputs/cmle-estimates
MODEL_NAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;babyweight_estimator&amp;#39;&lt;/span&gt;
VERSION_NAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;v1&amp;#39;&lt;/span&gt;
REGION&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;REGION&amp;gt;&amp;#39;&lt;/span&gt;
now&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;date +&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%Y%m%d_%H%M%S&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
JOB_NAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;batch_predict_&lt;/span&gt;$MODEL_NAME$now&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
MAX_WORKER_COUNT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;20&amp;#34;&lt;/span&gt;

gcloud ml-engine jobs submit prediction $JOB_NAME &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$MODEL_NAME &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --input-paths&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$INPUT_PATHS &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --output-path&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$OUTPUT_PATH &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --region&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$REGION &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --data-format&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$DATA_FORMAT &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --max-worker-count&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$MAX_WORKER_COUNT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;point-by-point-versus-micro-batching-for-online-prediction&#34;&gt;Point-by-point versus micro-batching for online prediction&lt;/h4&gt;
&lt;p&gt;In real-time prediction pipelines, whether you are serving the model as an HTTP endpoint or using the model directly from the workers, you have two options to get predictions for incoming data points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Individual point&lt;/strong&gt;. The obvious option is to send each data point to the model individually and get a prediction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Micro-batches&lt;/strong&gt;. A more optimized option is to use a windowing function to create micro-batches, grouping data points within a specific time period, such as every 5 seconds. The micro-batch is then sent to the model to get predictions for all the instances at at time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following Python code shows how to create time-based micro-batches using a windowing function in an Apache Beam pipeline.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/pipelines/stream_process.py&#34;&gt;blogs/tf_dataflow_serving/pipelines/stream_process.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/pipelines/stream_process.py&#34;&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;run_pipeline_with_micro_batches&lt;/span&gt;(inference_type, project,
                                    pubsub_topic, pubsub_subscription,
                                    bq_dataset, bq_table,
                                    window_size, runner, args&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):

    prepare_steaming_source(project, pubsub_topic, pubsub_subscription)
    prepare_steaming_sink(project, bq_dataset, bq_table)
    pubsub_subscription_url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;projects/{}/subscriptions/{}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(project, pubsub_subscription)
    options &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beam&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;PipelineOptions(flags&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[], &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;args)

    pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beam&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Pipeline(runner, options&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;options)
    (
            pipeline
            &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Read from PubSub&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; beam&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;io&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ReadStringsFromPubSub(subscription&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pubsub_subscription_url, id_label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source_id&amp;#34;&lt;/span&gt;)
            &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Micro-batch - Window Size: {} Seconds&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(window_size) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; beam&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;WindowInto(FixedWindows(size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;window_size))
            &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Estimate Targets - {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(inference_type) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; beam&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FlatMap(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; messages: estimate(messages, inference_type))
            &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Write to BigQuery&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; beam&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;io&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;WriteToBigQuery(project&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;project,
                                                             dataset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;bq_dataset,
                                                             table&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;bq_table
                                                             )
    )

    pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;run()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The micro-batching approach uses models deployed as HTTP endpoints, which dramatically reduces the number of HTTP requests and reduces latency. Even when the micro-batching technique used with the direct model, sending the model a tensor with N instances for prediction is more efficient than sending a tensor with a length of 1 because of the vectorized operations.&lt;/p&gt;
&lt;h2 id=&#34;batch-experiments&#34;&gt;Batch experiments&lt;/h2&gt;
&lt;p&gt;In the batch experiments, we want to estimate baby weights in the Natality dataset in BigQuery by using a TensorFlow regression model. We then want to save the prediction results in Cloud Storage as CSV files by using a Dataflow batch pipeline. The following section describes different experiments we tried to accomplish this task.&lt;/p&gt;
&lt;h3 id=&#34;approach-1-dataflow-with-direct-model-prediction&#34;&gt;Approach 1: Dataflow with direct-model prediction&lt;/h3&gt;
&lt;p&gt;In this approach, Dataflow workers host the TensorFlow &lt;code&gt;SavedModel&lt;/code&gt;, which is called directly for prediction during the batch processing pipeline for each record. Figure 5 shows the high-level architecture of this approach.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-5-batch-approach-1-dataflow-with-direct-model.png&#34; alt=&#34;Batch Approach 1: Dataflow with direct model prediction&#34;&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Batch Approach 1: Dataflow with direct model prediction&lt;/p&gt;
&lt;p&gt;The Dataflow pipeline performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read data from BigQuery.&lt;/li&gt;
&lt;li&gt;Prepare BigQuery record for prediction.&lt;/li&gt;
&lt;li&gt;Call the local TensorFlow &lt;code&gt;SavedModel&lt;/code&gt; to get a prediction for each record.&lt;/li&gt;
&lt;li&gt;Convert the result (input record and estimated baby weight) to a CSV file.&lt;/li&gt;
&lt;li&gt;Write the CSV file to Cloud Storage.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this approach, there are no calls to remote services, such as a deployed model on AI Platform as an HTTP endpoint. The prediction is done locally within each Dataflow worker by using the TensorFlow &lt;code&gt;SavedModel&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;approach-2-dataflow-with-ai-platform-batch-prediction&#34;&gt;Approach 2: Dataflow with AI Platform batch prediction&lt;/h3&gt;
&lt;p&gt;In this approach, the TensorFlow &lt;code&gt;SavedModel&lt;/code&gt; is stored in Cloud Storage and used by AI Platform for prediction. However, instead of making an API call to the deployed model for each record as with the previous approach, the data is prepared for prediction and submitted as a batch.&lt;/p&gt;
&lt;p&gt;This approach has two phases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Dataflow prepares the data from BigQuery for prediction, and then stores the data in Cloud Storage.&lt;/li&gt;
&lt;li&gt;The AI Platform batch prediction job is submitted with the prepared data, and the prediction results are stored in Cloud Storage.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Figure 6 shows the overall architecture of this two-phased approach.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-6-batch-approach-2-dataflow-with-cloud-ml.png&#34; alt=&#34;Batch Approach 2: Dataflow with AI Platform batch prediction&#34;&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; Batch Approach 2: Dataflow with AI Platform batch prediction&lt;/p&gt;
&lt;p&gt;The workflow steps, including the Dataflow pipeline, are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read data from BigQuery.&lt;/li&gt;
&lt;li&gt;Prepare BigQuery record for prediction.&lt;/li&gt;
&lt;li&gt;Write JSON data to Cloud Storage. The &lt;code&gt;serving_fn&lt;/code&gt; function in the model expects JSON instances as input.&lt;/li&gt;
&lt;li&gt;Submit an AI Platform batch prediction job with the prepared data in Cloud Storage. This job writes the prediction results to Cloud Storage as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Dataflow job prepares the data for prediction rather than submitting the AI Platform prediction job. In other words, the data preparation task and the batch prediction task are not tightly coupled. Cloud Functions, Airflow, or any scheduler can orchestrate the workflow by executing the Dataflow job and then submitting the AI Platform job for batch prediction.&lt;/p&gt;
&lt;p&gt;AI Platform batch prediction is recommended for both performance and ease of use if your data meets the following criteria:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your data is available in Cloud Storage, in the format expected for prediction, from a previous data ingestion process.&lt;/li&gt;
&lt;li&gt;You don&amp;rsquo;t have control of the first phase of the workflow, such as the Dataflow pipeline that prepares the data in Cloud Storage for prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experiment-configurations&#34;&gt;Experiment configurations&lt;/h3&gt;
&lt;p&gt;We used the following configurations in three experiments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data sizes: &lt;code&gt;10K&lt;/code&gt;, &lt;code&gt;100K&lt;/code&gt;, &lt;code&gt;1M&lt;/code&gt;, and &lt;code&gt;10M&lt;/code&gt; rows&lt;/li&gt;
&lt;li&gt;Cloud Storage class: &lt;code&gt;Regional Storage&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Cloud Storage location: &lt;code&gt;europe-west1-b&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dataflow region: &lt;code&gt;europe-west1-b&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dataflow worker machine type: &lt;code&gt;n1-standard-1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dataflow Autoscaling for batch data up to 1 million records&lt;/li&gt;
&lt;li&gt;Dataflow &lt;code&gt;num_worker&lt;/code&gt;: &lt;code&gt;20&lt;/code&gt; for batch data up to 10 million records&lt;/li&gt;
&lt;li&gt;AI Platform batch prediction &lt;code&gt;max-worker-count&lt;/code&gt; setting: &lt;code&gt;20&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Cloud Storage location and the Dataflow region should be the same. This solution uses the &lt;code&gt;europe-west1-b&lt;/code&gt; region as an arbitrary value.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The following table summarizes the results (timings) of performing the batch predictions and direct-model predictions with different sizes of datasets.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Batch data size&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Metric&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Dataflow then AI Platform batch prediction&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Dataflow with direct-model prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;10K rows&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Running time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;15 min 30 sec  (Dataflow: 7 min 47 sec + AI Platform: 7 min 43 sec)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8 min 24 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total vCPU time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.301 hr  (Dataflow: 0.151 hr + AI Platform: 0.15 hr)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.173 hr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;100K rows&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Running time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;16 min 37 sec  (Dataflow: 8 min 39 sec + AI Platform: 7 min 58 sec)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;10 min 31 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total vCPU time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.334 hr  (Dataflow: 0.184 hr + AI Platform: 0.15 hr)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.243 hr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;1M rows&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Running time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;21 min 11 sec (Dataflow: 11 min 07 sec + AI Platform: 10 min 04 sec)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;17 min 12 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total vCPU time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.446 hr  (Dataflow: 0.256 hr + AI Platform: 0.19 hr)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.115 hr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;10M rows&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Running time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;33 min 08 sec (Dataflow: 12 min 15 sec + AI Platform: 20 min 53 sec)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;25 min 02 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total vCPU time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5.251 hr  (Dataflow: 3.581 hr + AI Platform: 1.67 hr)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7.878 hr&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Figure 7 shows a graph of these results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-7-barchart-batch-results.svg&#34; alt=&#34;Graph showing timings for 3 approaches for 4 different dataset sizes&#34;&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Graph showing timings for 3 approaches for 4 different dataset sizes&lt;/p&gt;
&lt;p&gt;As the results show, the AI Platform batch prediction job on its own takes less time to produce predictions for the input data, given that the data is already in Cloud Storage in the format used for prediction. However, when the batch prediction job is combined with a preprocessing step (extracting and preparing the data from BigQuery to Cloud Storage for prediction) and with a post-processing step (storing the data back to BigQuery), the direct-model approach produces better end-to-end execution time. In addition, the performance of the direct-model prediction approach can be further optimized using micro-batching (which we discuss later for the streaming experiments).&lt;/p&gt;
&lt;h2 id=&#34;stream-experiments&#34;&gt;Stream experiments&lt;/h2&gt;
&lt;p&gt;In the streaming experiments, the Dataflow pipeline reads data points from a Pub/Sub topic and writes the data to BigQuery by using the streaming APIs. The Dataflow streaming pipeline processes the data and gets predictions using the TensorFlow baby-weight estimation model.&lt;/p&gt;
&lt;p&gt;The topic receives data from a stream simulator that generates data points, which are the instances to estimate the baby weight for, at a predefined rate of events per second. This simulates a real-world example of an unbounded data source. The following Python code simulates the data stream sent to a Pub/Sub topic.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/simulate_stream.py&#34;&gt;blogs/tf_dataflow_serving/simulate_stream.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/tf_dataflow_serving/simulate_stream.py&#34;&gt;View on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;client &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pubsub&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Client(project&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;PARAMS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;project_id)
topic &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; client&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;topic(PARAMS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pubsub_topic)
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; topic&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists():
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Topic does not exist. Please run a stream pipeline first to create the topic.&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Simulation aborted.&amp;#39;&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(PARAMS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stream_sample_size):

    message &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; send_message(topic, index)

    &lt;span style=&#34;color:#75715e&#34;&gt;# for debugging&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; PARAMS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_message:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Message {} was sent: {}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(index&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, message)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;

    time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sleep(sleep_time_per_msg)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;approach-1-dataflow-with-ai-platform-online-prediction&#34;&gt;Approach 1: Dataflow with AI Platform online prediction&lt;/h3&gt;
&lt;p&gt;In this approach, the TensorFlow model is deployed and hosted in AI Platform as a REST API. The Dataflow streaming pipeline calls the API for each message consumed from Pub/Sub get predictions. The high-level architecture of this approach is shown in Figure 8.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-8-stream-approach-1-dataflow-with-cloud-ml-online.png&#34; alt=&#34;Stream Approach 1: Dataflow with AI Platform online prediction&#34;&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Stream Approach 1: Dataflow with AI Platform online prediction. The HTTP request might include a single data point or a group of data points in a micro-batch.&lt;/p&gt;
&lt;p&gt;In this approach, the Dataflow pipeline performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read messages from a Pub/Sub topic.&lt;/li&gt;
&lt;li&gt;Send an HTTP request to the AI Platform model&amp;rsquo;s API to get predictions for each message.&lt;/li&gt;
&lt;li&gt;Write results to BigQuery by using streaming APIs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Micro-batching is a better approach. That is, instead of sending an HTTP request to the model&amp;rsquo;s REST API for each message that is read from Pub/Sub, Dataflow groups messages received during a 1-second window. It then sends this group of messages as a micro-batch in a single HTTP request to the model&amp;rsquo;s API. In this approach, the Dataflow pipeline performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read messages from Pub/Sub topic.&lt;/li&gt;
&lt;li&gt;Apply a 1-second windowing operation to create a micro-batch of messages.&lt;/li&gt;
&lt;li&gt;Send an HTTP request with the micro-batch to the AI Platform model&amp;rsquo;s API to get predictions for the messages.&lt;/li&gt;
&lt;li&gt;Write results to BigQuery by using streaming APIs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rationale behind this approach is that it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reduces the number of calls to the remote service, such as the AI Platform model.&lt;/li&gt;
&lt;li&gt;Reduces the average latency of serving each message.&lt;/li&gt;
&lt;li&gt;Reduces the overall processing time of the pipeline.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this experiment, the time window was set to 1 second. However, the micro-batch size, which is the number of messages sent as a batch to AI Platform mode, varies. The micro-batch size depends on the message generation frequency—the number of messages per second.&lt;/p&gt;
&lt;p&gt;The following section describes experiments with three different frequencies: 50, 100, and 500 messages per second. That is, the micro-batch size is 50, 100, and 500.&lt;/p&gt;
&lt;h3 id=&#34;approach-2-dataflow-with-direct-model-prediction&#34;&gt;Approach 2: Dataflow with direct-model prediction&lt;/h3&gt;
&lt;p&gt;This approach is similar to the approach that was used in the batch experiments. The TensorFlow &lt;code&gt;SavedModel&lt;/code&gt; is hosted on Dataflow workers and is called for prediction during the stream processing pipeline for each record. Figure 9 shows the high-level architecture of this approach.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-9-stream-approach-2-dataflow-with-direct-model.png&#34; alt=&#34;Stream approach 2: Dataflow with direct-model prediction&#34;&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; Stream approach 2: Dataflow with direct-model prediction&lt;/p&gt;
&lt;p&gt;In this approach, the Dataflow pipeline performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read messages from Pub/Sub topic.&lt;/li&gt;
&lt;li&gt;Call the local TensorFlow &lt;code&gt;SavedModel&lt;/code&gt; to get predictions for each record.&lt;/li&gt;
&lt;li&gt;Write results to BigQuery by using streaming APIs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The micro-batching technique can also be used in the stream pipeline with the direct-model prediction approach. Instead of sending a tensor of one data instance to the model, we can send a tensor of N data instances, where N is equal to the messages received within the Dataflow window to the model. This technique uses the vectorized operations of the TensorFlow model and gets several predictions in parallel.&lt;/p&gt;
&lt;h3 id=&#34;experiment-configurations-1&#34;&gt;Experiment configurations&lt;/h3&gt;
&lt;p&gt;We used the following configurations for these experiments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stream data size: &lt;code&gt;10K records (messages)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Simulated messages per second (MPS): &lt;code&gt;50&lt;/code&gt;, &lt;code&gt;100&lt;/code&gt;, and &lt;code&gt;500&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Window size (in micro-batch experiments): &lt;code&gt;1 second&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dataflow region: &lt;code&gt;europe-west1-b&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dataflow worker machine type: &lt;code&gt;n1-standard-1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dataflow &lt;code&gt;num_worker&lt;/code&gt;: &lt;code&gt;5&lt;/code&gt; (no auto-scaling)&lt;/li&gt;
&lt;li&gt;AI Platform model API nodes: &lt;code&gt;3 (manualScale)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results-1&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The following table summarizes the results of performing the streaming experiments with different volumes of data (messages per second). &lt;em&gt;Messages frequency&lt;/em&gt; refers to the number of messages sent per second, and &lt;em&gt;simulation time&lt;/em&gt; refers to the time to send all the messages.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Stream messages frequency&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Metric&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Dataflow with AI Platform online prediction&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Dataflow with direct-model prediction&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;Single message&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;Micro-batching&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;Single message&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;Micro-batching&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;50 msg per sec&lt;/strong&gt;  (Simulation time: 3 min 20 sec)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;9 min 34 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7 min 44 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3 min 43 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3 min 22 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;100 msg per sec&lt;/strong&gt;  (Simulation time**:** 1 min 40 sec)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6 min 03 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4 min 34 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1 min 51 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1 min 41 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;strong&gt;500 msg per sec&lt;/strong&gt;  (Simulation time**:** 20 sec)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cloud.google.com/ml-engine/docs/tensorflow/quotas&#34;&gt;NA - Default AI Platform Online Prediction Quota&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2 min 47 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1 min 23 sec&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;48 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Figure 10 shows a graph of these results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.google.com/solutions/images/comparing-ml-model-predictions-using-cloud-dataflow-fig-10-barchart-streaming-results.svg&#34; alt=&#34;Graph showing timings for different approaches and frequencies&#34;&gt;&lt;strong&gt;Figure 10.&lt;/strong&gt; Graph showing timings for different approaches and frequencies&lt;/p&gt;
&lt;p&gt;As shown in the results, the micro-batching technique improves the execution performance in both AI Platform online prediction and in direct-model prediction. In addition, using direct-model with streaming pipeline shows 2 times to 4 times the performance improvement compared to calling an external REST/HTTP API for online prediction.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;According to the described approaches and experiment results, we recommend the following approaches.&lt;/p&gt;
&lt;h3 id=&#34;batch-processing&#34;&gt;Batch processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you are building your batch data processing pipeline, and you want prediction as part of the pipeline, use the direct-model approach for the best performance.&lt;/li&gt;
&lt;li&gt;Improve the performance of the direct-model approach by creating micro-batches of the data points before calling the local model for prediction to make use of the parallelization of the vectorized operations.&lt;/li&gt;
&lt;li&gt;If your data is populated to Cloud Storage in the format expected for prediction, use AI Platform batch prediction for the best performance.&lt;/li&gt;
&lt;li&gt;Use AI Platform if you want to use the power of GPUs for batch prediction.&lt;/li&gt;
&lt;li&gt;Do not use AI Platform online prediction for batch prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stream-processing&#34;&gt;Stream processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use direct-model in the streaming pipeline for best performance and reduced average latency. Predictions are performed locally, with no HTTP calls to remote services.&lt;/li&gt;
&lt;li&gt;Decouple your model from your data processing pipelines for better maintainability of models used in online predictions. The best approach is to serve your model as an independent microservice by using AI Platform or any other web hosting service.&lt;/li&gt;
&lt;li&gt;Deploy your model as an independent web service to allow multiple data processing pipelines and online apps to consume the model service as an endpoint. In addition, changes to the model are transparent to the apps and pipelines that consume it.&lt;/li&gt;
&lt;li&gt;Deploy multiple instances of the service with load balancing to improve the scalability and the availability of the model web service. With AI Platform, you only need to specify the number of nodes (&lt;code&gt;manualScaling&lt;/code&gt;) or &lt;code&gt;minNodes&lt;/code&gt; (&lt;code&gt;autoScaling&lt;/code&gt;) in the yaml configuration file when you deploy a model version.&lt;/li&gt;
&lt;li&gt;If you deploy your model in a separate microservice, there are extra costs, depending on the underlying serving infrastructure. See the pricing &lt;a href=&#34;https://cloud.google.com/ml-engine/docs/tensorflow/pricing-faq&#34;&gt;FAQ&lt;/a&gt; for AI Platform online prediction.&lt;/li&gt;
&lt;li&gt;Use micro-batching in your streaming data processing pipeline for better performance with both the direct-model and HTTP-model service. Micro-batching reduces the number of HTTP requests to the model service, and uses the vectorized operations of the TensorFlow model to get predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learn how to build and train the baby-weight model in the &lt;a href=&#34;https://cloud.google.com/solutions/machine-learning/ml-on-structured-data-analysis-prep-1#explore_the_public_natality_dataset&#34;&gt;Machine Learning with Structured Data&lt;/a&gt; solution.&lt;/li&gt;
&lt;li&gt;Have a look at the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/tf_dataflow_serving&#34;&gt;companion repository&lt;/a&gt; on GitHub.&lt;/li&gt;
&lt;li&gt;Try out other Google Cloud features for yourself. Have a look at our &lt;a href=&#34;https://cloud.google.com/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/solutions/comparing-ml-model-predictions-using-cloud-dataflow-pipelines&#34;&gt;Comparing Machine Learning Models for Predictions in Cloud Dataflow Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>recommendations</title>
      <link>http://www.yezheng.pro/post/specialization/artificial-intelligence/recommendation-system/product-overview/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/artificial-intelligence/recommendation-system/product-overview/</guid>
      
        <description>&lt;h2 id=&#34;implementing-recommendations&#34;&gt;Implementing Recommendations&lt;/h2&gt;
&lt;h3 id=&#34;steps&#34;&gt;Steps&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set up a project&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Import your &lt;strong&gt;product catalog&lt;/strong&gt;
You can add items to your Recommendations AI product catalog individually by using the import Files or API.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Information of the products sold to customers. This includes the product title, description, in stock availability, pricing, and so on.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Record &lt;strong&gt;user events&lt;/strong&gt;
User events track user actions such as clicking on a product, adding an item to a shopping cart, or purchasing an item, and so on. Recommendations AI relies on user event data in order to generate personalized recommendations. User events need to be ingested in real time to accurately reflect the behavior of your users.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;End user behavior on your website. This includes users searching for, viewing, or purchasing a specific item, your website showing users a list of products, and so on.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;Determine your &lt;strong&gt;recommendation types&lt;/strong&gt; and &lt;strong&gt;placements&lt;/strong&gt;
Reviewing the available recommendation types, optimization objectives, and other model tuning options to determine the best options for your business objectives. The location of the recommendation panel and the objective for that panel impact model tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Import historical user events
Your models need sufficient training data before they can provide accurate predictions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create your model
After you have met the data requirements, create your model to initiate model training and tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create your placements and preview your recommendations
After your model has been activated, you can create your placements and &lt;strong&gt;preview&lt;/strong&gt; the recommendations from your model to &lt;strong&gt;ensure&lt;/strong&gt; your setup is functioning as expected.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up an A/B experiment (Optional)
You can compare the performance of your website with Recommendations AI recommendations to a &lt;strong&gt;baseline&lt;/strong&gt; version of your website without Recommendations AI recommendations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluate your model
You can associate recommendations and user events and Recommendations AI provides reporting of metrics to help you determine how incorporating the recommendations is affecting your business, then view recommendation metrics for your project in the Dashboard.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can upload and manage product catalog information and user event logs for your websites. Recommendations AI uses this information to train and update recommendation models.&lt;/p&gt;
&lt;h2 id=&#34;recommendation-model-types&#34;&gt;Recommendation model types&lt;/h2&gt;
&lt;p&gt;When you request recommendations from Recommendations AI, you provide the &lt;code&gt;placement&lt;/code&gt; value, which determines which model is used to return your recommendations.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Model type&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Optimization objective&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;placement&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;User event types&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Minimum data requirement&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Data collection window&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Recommended for you&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Click-through rate&lt;/td&gt;
&lt;td&gt;home_page&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;detail-page-view&lt;/code&gt; &lt;code&gt;add-to-cart&lt;/code&gt; &lt;code&gt;purchase-complete&lt;/code&gt; &lt;code&gt;home-page-view&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1 week, with an average of 10 &lt;code&gt;detail-page-view&lt;/code&gt; events per joined catalog item.OR60 days with at least one joined &lt;code&gt;detail-page-view&lt;/code&gt; event.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3 months&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Recommended for you&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Conversion rate&lt;/td&gt;
&lt;td&gt;home_page&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;detail-page-view&lt;/code&gt; &lt;code&gt;add-to-cart&lt;/code&gt; &lt;code&gt;purchase-complete&lt;/code&gt; &lt;code&gt;home-page-view&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1 week, with an average of 10 &lt;code&gt;add-to-cart&lt;/code&gt; events per joined catalog item.OR60 days with at least one joined &lt;code&gt;add-to-cart&lt;/code&gt; event.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3 months&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Others you may like&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Click-through rate&lt;/td&gt;
&lt;td&gt;product_detail&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;detail-page-view&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1 week, with an average of 10 &lt;code&gt;detail-page-view&lt;/code&gt; events per joined catalog item.OR60 days with at least one joined &lt;code&gt;detail-page-view&lt;/code&gt; event.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3 months&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Others you may like&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Conversion rate&lt;/td&gt;
&lt;td&gt;product_detail&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;add-to-cart&lt;/code&gt; &lt;code&gt;detail-page-view&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1 week, with an average of 10 &lt;code&gt;add-to-cart&lt;/code&gt; events per joined catalog item.OR60 days with at least one joined &lt;code&gt;add-to-cart&lt;/code&gt; event.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3 months&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Frequently bought together&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;revenue per order&lt;/td&gt;
&lt;td&gt;shopping_cart&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;purchase-complete&lt;/code&gt; &lt;code&gt;detail-page-view&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;An average of 10 &lt;code&gt;purchase-complete&lt;/code&gt; events per joined catalog item.OR90 days of &lt;code&gt;purchase-complete&lt;/code&gt; events.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;12 months&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;user-events&#34;&gt;User events&lt;/h2&gt;
&lt;h3 id=&#34;event-type-priority&#34;&gt;Event type priority&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Priority&lt;/th&gt;
&lt;th&gt;User Events&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Required for initial live experiment&lt;/td&gt;
&lt;td&gt;add-to-cart, detail-page-view, home-page-view, purchase-complete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Important for improving model quality over time&lt;/td&gt;
&lt;td&gt;checkout-start, category-page-view, remove-from-cart, search, shopping-cart-page-view&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nice to have&lt;/td&gt;
&lt;td&gt;add-to-list, page-visit, refund, remove-from-list&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;user-event-type-examples-and-schemas&#34;&gt;User event type examples and schemas&lt;/h3&gt;
&lt;p&gt;Hadoop/Bigquery/Snowflake/Redshift&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/recommendations-ai/docs/setting-up&#34;&gt;recommendations-ai&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Deploying machine learning models in production</title>
      <link>http://www.yezheng.pro/post/specialization/artificial-intelligence/deep-learning/deploy/</link>
      <pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/artificial-intelligence/deep-learning/deploy/</guid>
      
        <description>&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/329372124/answer/743251971&#34;&gt;训练好的深度学习模型是怎么部署的？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/151406969&#34;&gt;PyTorch模型的加速及部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ahkarami/Deep-Learning-in-Production&#34;&gt;ahkarami/Deep-Learning-in-Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html&#34;&gt;Accelerating Inference In TF-TRT User Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorrt&#34;&gt;tensorflow/tensorrt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/api/index.html&#34;&gt;Tensorrt API Reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow&#34;&gt;Containers: nvidia:tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/applications/machine-learning/model-inference/resnet-model-inference-tensorrt.html&#34;&gt;Model inference using TensorFlow and TensorRT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.tensorflow.org/2018/04/speed-up-tensorflow-inference-on-gpus-tensorRT.html&#34;&gt;Speed up TensorFlow Inference on GPUs with TensorRT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/vinhngx/tensorrt/blob/vinhn-tf20-notebook/tftrt/examples/image-classification/TFv2-TF-TRT-inference-from-Keras-saved-model.ipynb&#34;&gt;TF20-TF-TRT-inference-from-Keras-saved-model.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Developing Lightweight Microservices Using Kubernetes</title>
      <link>http://www.yezheng.pro/post/specialization/web-application/spring/microservice-with-kubernetes/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/web-application/spring/microservice-with-kubernetes/</guid>
      
        <description>&lt;h2 id=&#34;introduction-to-kubernetes&#34;&gt;Introduction to Kubernetes&lt;/h2&gt;
&lt;h2 id=&#34;deploying-our-microservices-in-kubernetes&#34;&gt;Deploying Our Microservices in Kubernetes&lt;/h2&gt;
&lt;h2 id=&#34;implementing-kubernetes-features-as-an-alternative&#34;&gt;Implementing Kubernetes Features as an Alternative&lt;/h2&gt;
&lt;h2 id=&#34;using-a-service-mesh-to-improve-observability-and-management&#34;&gt;Using a Service Mesh to Improve Observability and Management&lt;/h2&gt;
&lt;h2 id=&#34;centralized-logging-with-the-efk-stack&#34;&gt;Centralized Logging with the EFK Stack&lt;/h2&gt;
&lt;h2 id=&#34;monitoring-microservices&#34;&gt;Monitoring Microservices&lt;/h2&gt;
</description>
      
    </item>
    
    <item>
      <title>Getting Started with Microservice Development Using Spring Boot</title>
      <link>http://www.yezheng.pro/post/specialization/web-application/spring/microservice-with-spring-boot/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/web-application/spring/microservice-with-spring-boot/</guid>
      
        <description>&lt;h2 id=&#34;introduction-to-microservices&#34;&gt;Introduction to Microservices&lt;/h2&gt;
&lt;h3 id=&#34;benefits&#34;&gt;Benefits&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Each component in the platform can be &lt;strong&gt;delivered and upgraded separately&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Each component in the platform can also be &lt;strong&gt;scaled&lt;/strong&gt; out to multiple servers &lt;strong&gt;independently&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;challenges&#34;&gt;Challenges&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Many small components that use synchronous communication can cause &lt;em&gt;a&lt;/em&gt; &lt;em&gt;chain of failure&lt;/em&gt; problem, especially under high load.&lt;/li&gt;
&lt;li&gt;Keeping the configuration consistent and up to date in all the instances&lt;/li&gt;
&lt;li&gt;Monitoring the state of the platform in terms of latency issues and hardware usage was more complicated&lt;/li&gt;
&lt;li&gt;Collecting log files and correlating related log events from the components was also difficult.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;microservice-must-fulfill-certain-criteria&#34;&gt;microservice must fulfill certain criteria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It must conform to a shared-nothing architecture; that is, microservices &lt;strong&gt;don&amp;rsquo;t share data in databases&lt;/strong&gt; with each other!&lt;/li&gt;
&lt;li&gt;It must &lt;strong&gt;only communicate through well-defined interfaces&lt;/strong&gt;, for example, using synchronous services or preferably by sending messages to each other using APIs and message formats that are stable, well-documented, and evolve by following a defined versioning strategy.&lt;/li&gt;
&lt;li&gt;It must be deployed as separate runtime processes. Each instance of a microservice runs in a &lt;strong&gt;separate runtime process&lt;/strong&gt;, for example, a Docker container.&lt;/li&gt;
&lt;li&gt;Microservice instances are &lt;strong&gt;stateless&lt;/strong&gt; so that incoming requests to a microservice can be handled by any of its instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-big-should-a-microservice-be&#34;&gt;How big should a microservice be?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Small enough to fit in the head of a developer&lt;/li&gt;
&lt;li&gt;Big enough to not jeopardize performance (that is, latency) and/or data consistency (SQL foreign keys between data that&amp;rsquo;s stored in different microservices are no longer something you can take for granted)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;design-patterns-for-microservices&#34;&gt;Design patterns for microservices&lt;/h3&gt;
&lt;h4 id=&#34;service-discovery&#34;&gt;Service discovery&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;Microservices instances are typically assigned dynamically allocated IP addresses, how can clients find microservices and their instances?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;a &lt;strong&gt;service discovery&lt;/strong&gt; service keeps track of currently available microservices and the IP addresses of its instances.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;p&gt;Automatically register/unregister microservices&lt;/p&gt;
&lt;p&gt;The request will be routed to one of the microservices available instances.&lt;/p&gt;
&lt;p&gt;Requests to a microservice must be load-balanced over the available instances.&lt;/p&gt;
&lt;p&gt;We must be able to detect instances that are not currently healthy; that is, requests will not be routed to them.&lt;/p&gt;
&lt;p&gt;This design pattern can be implemented using two different strategies: &lt;strong&gt;Client-side routing&lt;/strong&gt; or &lt;strong&gt;Server-side routing&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;edge-server&#34;&gt;Edge server&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;It is in many cases desirable to expose some of the microservices to the outside and hide the remaining microservices from external access. The exposed microservices must be protected against requests from malicious clients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;all incoming requests will go through an &lt;strong&gt;Edge Server&lt;/strong&gt;, Implementation notes: An edge server typically behaves like a reverse proxy and can be integrated with a discovery service to provide dynamic load balancing capabilities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;p&gt;Hide internal services that should not be exposed outside their context; that is, only route requests to microservices that are configured to allow external requests.&lt;/p&gt;
&lt;p&gt;Expose external services and protect them from malicious requests; that is, use standard protocols and best practices such as OAuth, OIDC, JWT tokens, and API keys to ensure that the clients are trustworthy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reactive-microservices&#34;&gt;Reactive microservices&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;we are used to implementing  &lt;strong&gt;synchronous&lt;/strong&gt; communication using &lt;strong&gt;blocking I/O&lt;/strong&gt;, for example, a RESTful JSON API over HTTP. a server might run out of available threads in the operating system, causing problems ranging from longer response times to crashing servers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;overusing&lt;/strong&gt; blocking I/O can make a system of microservices prone to errors which is also known as a &lt;strong&gt;chain of failures&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;Use non-blocking I/O to ensure that no threads are allocated while waiting for processing to occur in another service, that is, a database or another microservice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;p&gt;Whenever feasible, use an &lt;strong&gt;asynchronous programming&lt;/strong&gt; model; that is, send messages without waiting for the receiver to process them.&lt;/p&gt;
&lt;p&gt;If a synchronous programming model is preferred, ensure that reactive frameworks are used that can execute &lt;strong&gt;synchronous requests using non-blocking I/O&lt;/strong&gt;, that is, without allocating a thread while waiting for a response. This will make the microservices easier to scale in order to handle an increased workload.&lt;/p&gt;
&lt;p&gt;Microservices must also be designed to be resilient, that is, &lt;strong&gt;capable of producing a response, even if a service that it depends on fails&lt;/strong&gt;. Once the failing service is operational again, its clients must be able to resume using it, which is known as self-healing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;central-configuration&#34;&gt;Central configuration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;How do I get a complete picture of the configuration that is in place for all the running microservice instances?&lt;/p&gt;
&lt;p&gt;How do I update the configuration and make sure that all the affected microservice instances are updated correctly?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;a configuration server to store the configuration of all the microservices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;p&gt;Make it possible to store configuration information for a group of microservices in one place, with different settings for different environments (for example, dev, test, qa, and prod).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;centralized-log-analysis&#34;&gt;Centralized log analysis&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;How do I find out if any of the microservice instances get into trouble and start writing error messages to their log files?&lt;/p&gt;
&lt;p&gt;If end users start to report problems, how can I find related log messages; that is, how can I identify which microservice instance is the root cause of the problem?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;Add a new component that can manage &lt;strong&gt;centralized logging&lt;/strong&gt; and is capable of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting new microservice instances and collecting log events from them&lt;/li&gt;
&lt;li&gt;Interpreting and storing log events in a structured and searchable way in a central database&lt;/li&gt;
&lt;li&gt;Providing APIs and graphical tools for querying and analyzing log events&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;distributed-tracing&#34;&gt;Distributed tracing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;If end users start to file support cases regarding a specific failure, how can we identify the microservice that caused the problem, that is, the root cause?&lt;/p&gt;
&lt;p&gt;If one support case mentions problems related to a specific entity, for example, a specific order number, how can we find log messages related to processing this specific order&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;To track the processing between cooperating microservices, we need to ensure that all related requests and messages are marked with a common correlation ID and that the correlation ID is part of all log events. Based on a correlation ID, we can use the centralized logging service to find all related log events.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;p&gt;Assign unique correlation IDs to all incoming or new requests and events in a well-known place, such as a header with a recognized name.&lt;/p&gt;
&lt;p&gt;When a microservice makes an outgoing request or sends a message, it must add the correlation ID to the request and message.&lt;/p&gt;
&lt;p&gt;All log events must include the correlation ID in a predefined format so that the centralized logging service can extract the correlation ID from the log event and make it searchable.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;circuit-breaker&#34;&gt;Circuit Breaker&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;A system landscape of microservices that uses &lt;strong&gt;synchronous&lt;/strong&gt; intercommunication can be exposed to a &lt;em&gt;&lt;strong&gt;chain of failure&lt;/strong&gt;&lt;/em&gt;. If one microservice stops responding, its clients might get into problems as well and stop responding to requests from their clients. The problem can propagate recursively throughout a system landscape.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;Add a Circuit Breaker that prevents new outgoing requests from a caller if it detects a problem with the service it calls.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open the circuit and fail fast (without waiting for a timeout) if problems with the service are detected.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probe&lt;/strong&gt; (探针) for failure correction (also known as a &lt;strong&gt;half-open circuit&lt;/strong&gt;); that is, allow a single request to go through on a regular basis to &lt;strong&gt;see if&lt;/strong&gt; the service operates normally again.&lt;/li&gt;
&lt;li&gt;Close the circuit if the probe detects that the service operates normally again. This capability is very important since it makes the system landscape resilient to these kinds of problems; that is, it &lt;strong&gt;self-heals&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;all &lt;strong&gt;synchronous&lt;/strong&gt; communication within the system landscape of microservices &lt;strong&gt;goes through Circuit Breakers&lt;/strong&gt;. All the Circuit Breakers are closed; that is, they allow traffic, except for one Circuit Breaker detected problems in the service the requests go to.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;control-loop&#34;&gt;Control loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;In a system landscape with a large number of microservice instances spread out over a number of servers, it is very difficult to manually detect and correct problems such as crashed or hung microservice instances.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;Add a new component, a &lt;strong&gt;control loop&lt;/strong&gt;, to the system landscape; this constantly observes the actual state of the system landscape; compares it with the desired state, as specified by the operators; and, if required, takes action.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://static.packt-cdn.com/products/9781789613476/graphics/4716aa50-5154-4e6a-b6d2-32ae7728d640.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;p&gt;Implementation notes: In the world of containers, a &lt;em&gt;container orchestrator&lt;/em&gt; such as &lt;strong&gt;Kubernetes&lt;/strong&gt; is typically used to implement this pattern.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;centralized-monitoring-and-alarms&#34;&gt;Centralized monitoring and alarms&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The problem&lt;/p&gt;
&lt;p&gt;If observed response times and/or the usage of hardware resources become unacceptably high, it can be very hard to discover the root cause of the problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A solution&lt;/p&gt;
&lt;p&gt;Add a new component, a &lt;strong&gt;monitor service&lt;/strong&gt;, which is capable of collecting metrics about hardware resource usage for each microservice instance level.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requirements for the solution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It must be able to collect metrics from all the servers that are used by the system landscape, which includes auto-scaling servers.&lt;/li&gt;
&lt;li&gt;It must be able to detect new microservice instances as they are launched on the available servers and start to collect metrics from them.&lt;/li&gt;
&lt;li&gt;It must be able to provide APIs and graphical tools for querying and analyzing the collected metrics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Use &lt;strong&gt;Grafana&lt;/strong&gt; visualizes metrics from &lt;strong&gt;Prometheus&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software-enablers&#34;&gt;Software enablers&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Design Pattern&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Spring Boot&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Spring Cloud&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Service discovery&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Netflix Eureka and Netflix Ribbon&lt;/td&gt;
&lt;td&gt;Kubernetes kube-proxy and service resources&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Edge server&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Spring Cloud and Spring Security OAuth&lt;/td&gt;
&lt;td&gt;Kubernetes Ingress controller&lt;/td&gt;
&lt;td&gt;Istio ingress gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Reactive microservices&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Spring Reactor and Spring WebFlux&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Central configuration&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Spring Config Server&lt;/td&gt;
&lt;td&gt;Kubernetes ConfigMaps and Secrets&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Centralized log analysis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Elasticsearch, Fluentd, and Kibana  &lt;strong&gt;Note&lt;/strong&gt;: Actually not part of Kubernetes  but can easily be deployed and configured together with Kubernetes&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Distributed tracing&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Spring Cloud Sleuth and Zipkin&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Jaeger&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Circuit Breaker&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Resilience4j&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Outlier detection&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Control loop&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes controller manager&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Centralized monitoring and alarms&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Grafana and Prometheus&lt;!-- raw HTML omitted --&gt; &lt;strong&gt;Note:&lt;/strong&gt; Actually not part of Kubernetes&lt;/td&gt;
&lt;td&gt;Kiali, Grafana, and Prometheus&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Please note that Spring Cloud, Kubernetes, and Istio can be used to implement some design patterns, such as service discovery, edge server, and central configuration. We will discuss the pros and cons of using these alternatives later in this book.&lt;/p&gt;
&lt;h3 id=&#34;other-important-considerations&#34;&gt;Other important considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Importance of Dev/Ops&lt;/strong&gt;: microservice architecture enables shorter delivery times and allows the &lt;em&gt;continuous delivery&lt;/em&gt; of new versions.  the teams also need to &lt;strong&gt;automate the delivery chain&lt;/strong&gt;, that is, the steps for building, testing, packaging, and deploying the microservices to the various deployment environments. This is known as setting up a &lt;em&gt;delivery pipeline&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Decomposing a monolithic application into microservices&lt;/strong&gt;: One of the most difficult and expensive decisions is how to decompose a monolithic application into a set of cooperating microservices. If this is done in the wrong way, you will end up with problems such as the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slow delivery&lt;/li&gt;
&lt;li&gt;Slow performance&lt;/li&gt;
&lt;li&gt;Inconsistent data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A good approach to finding proper boundaries for microservices is to apply &lt;strong&gt;Domain-Driven Design&lt;/strong&gt; and its &lt;strong&gt;Bounded Context&lt;/strong&gt; concept. According to Eric Evans, a &lt;em&gt;Bounded Context&lt;/em&gt; is &amp;ldquo;&lt;em&gt;A description of a boundary (typically a subsystem, or the work of a particular team) within which a particular model is defined and applicable.&amp;quot;&lt;/em&gt; This means that the microservice defined by a Bounded Context will have a well-defined model of its own data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Importance of API design&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the description in terms of the naming and data types used.&lt;/li&gt;
&lt;li&gt;It is of great importance that APIs are allowed to evolve in a controlled manner. This typically requires applying a proper versioning schema for the APIs, allowing clients of the API to migrate to new major versions at their own pace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Migration paths from on-premise to the cloud&lt;/strong&gt;: An appealing migration approach can be to &lt;strong&gt;first move&lt;/strong&gt; the workload into Kubernetes on-premise (as microservices or not) &lt;strong&gt;and then redeploy&lt;/strong&gt; it on a &lt;em&gt;Kubernetes as a Service&lt;/em&gt; offering provided by a preferred cloud provider.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Good design principles for microservices, the 12-factor app&lt;/strong&gt;: The 12-factor app (&lt;a href=&#34;https://12factor.net/&#34;&gt;https://12factor.net&lt;/a&gt;) is a set of design principles for building software that can be deployed in the cloud.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction-to-spring-boot&#34;&gt;Introduction to Spring Boot&lt;/h2&gt;
&lt;p&gt;We will develop microservices that contain business logic based on plain Spring Beans and REST APIs using Spring WebFlux, the Swagger/OpenAPI-based documentation of the REST APIs, and SpringFox and data persistence, while using Spring Data to store data in both SQL and NoSQL databases&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;plain Spring Beans, Spring WebFlux,  Swagger, SpringFox, Spring Data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;create reactive microservices in this chapter, including both non-blocking synchronous REST APIs and message-based asynchronous services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use &lt;strong&gt;Spring WebFlux&lt;/strong&gt; to develop non-blocking synchronous REST APIs and &lt;strong&gt;Spring Cloud Stream&lt;/strong&gt; to develop message-based asynchronous services.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-about-spring-boot&#34;&gt;Learning about Spring Boot&lt;/h3&gt;
&lt;p&gt;Spring Boot does that by applying a number of &lt;strong&gt;conventions&lt;/strong&gt; by default, minimizing the need for configuration. Whenever required, each convention can be overridden by writing some configuration, case by case.  Configuration, when required, is in my opinion written best using Java and &lt;strong&gt;annotations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A fat &lt;strong&gt;JAR&lt;/strong&gt; file contains not only the classes and resource files of the application itself, but also all the .jar files the application depends on. This means that the fat JAR file is the only JAR file required to run the application.&lt;/p&gt;
&lt;p&gt;Starting a fat JAR requires no separately installed Java EE web server, such as Apache Tomcat. Instead, it can be started with a simple command such as java -jar app.jar, making it a perfect choice for running in a Docker container!&lt;/p&gt;
&lt;h4 id=&#34;component-scanning&#34;&gt;Component scanning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s assume we have the following Spring &lt;strong&gt;component in the package of the application class&lt;/strong&gt; (or in one of its sub-packages):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@Component
public class MyComponentImpl implements MyComponent { ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Another component in the application can get the component automatically injected, also known as &lt;strong&gt;auto-wiring&lt;/strong&gt;, using the &lt;strong&gt;&lt;code&gt;@Autowired&lt;/code&gt;&lt;/strong&gt; annotation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public class AnotherComponent {
  
  private final MyComponent myComponent;
  
  @Autowired
  public AnotherComponent(MyComponent myComponent) {
    this.myComponent = myComponent;
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I prefer using constructor injection (over field and setter injection) to keep the state in my components immutable. The immutable state is important if you want to be able to run the component in a multithreaded runtime environment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we want to use components that are &lt;strong&gt;declared in a package outside the applications package&lt;/strong&gt;, for example, a utility component shared by multiple Spring Boot applications, we can complement the @SpringBootApplication annotation in the application class with a &lt;strong&gt;&lt;code&gt;@ComponentScan&lt;/code&gt;&lt;/strong&gt; annotation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package se.magnus.myapp;
  
@SpringBootApplication
@ComponentScan({&amp;quot;se.magnus.myapp&amp;quot;,&amp;quot;se.magnus.utils&amp;quot;})
public class MyApplication {
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can now auto-wire components from the se.magnus.util package in the application code, for example, a utility component, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package se.magnus.utils;
  
@Component
public class MyUtility { ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This utility component can be auto-wired in an application component like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package se.magnus.myapp.services;
  
public class AnotherComponent {
  
 private final MyUtility myUtility;
  
 @Autowired
 public AnotherComponent(MyUtility myUtility) {
   this.myUtility = myUtility;
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;java-based-configuration&#34;&gt;Java-based configuration&lt;/h4&gt;
&lt;p&gt;If we want to override Spring Boot&amp;rsquo;s default configuration or if we want to add our own configuration, we can simply annotate a class with &lt;strong&gt;&lt;code&gt;@Configuration&lt;/code&gt;&lt;/strong&gt;. for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@Configuration
public class SubscriberApplication {

  @Bean
  public Filter logFilter() {
    CommonsRequestLoggingFilter filter = new 
        CommonsRequestLoggingFilter();
    filter.setIncludeQueryString(true);
    filter.setIncludePayload(true);
    filter.setMaxPayloadLength(5120);
    return filter;
  }
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;reactive-non-blocking-synchronous-rest-services-with-spring-webflux&#34;&gt;Reactive non-blocking synchronous REST services with Spring WebFlux.&lt;/h3&gt;
&lt;p&gt;Spring Framework uses &lt;strong&gt;Project Reactor&lt;/strong&gt; as the base implementation of its reactive support, and also comes with a new web framework, Spring WebFlux, which supports the development of reactive, that is, non-blocking, HTTP clients and services.&lt;/p&gt;
&lt;p&gt;Spring WebFlux supports running on a servlet container, but also supports reactive non-servlet-based embedded web servers such as &lt;strong&gt;Netty&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Spring WebFlux starter dependency will be added to the build.gradle file. It looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;implementation(&#39;org.springframework.boot:spring-boot-starter-webflux&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When the microservice is started up, Spring Boot will detect Spring WebFlux on the classpath and configure it,  then start up an embedded web server, Netty is used by default,&lt;/p&gt;
&lt;p&gt;If we want to switch from Netty to Tomcat as our embedded web server, we can override the default configuration by excluding Netty from the starter dependency and add the starter dependency for Tomcat:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;implementation(&#39;org.springframework.boot:spring-boot-starter-webflux&#39;) 
{
 exclude group: &#39;org.springframework.boot&#39;, module: &#39;spring-boot-
 starter-reactor-netty&#39;
}
implementation(&#39;org.springframework.boot:spring-boot-starter-tomcat&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spring Boot application property files can either be a .properties file or a YAML file. By default, they are named application.properties and application.yml, respectively. Avoid port collisions with other microservices running on the same server, add the following line to the application.yml file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server.port: 7001
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Sample RestController&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@RestController
public class MyRestService {

  @GetMapping(value = &amp;quot;/my-resource&amp;quot;, produces = &amp;quot;application/json&amp;quot;)
  List&amp;lt;Resource&amp;gt; listResources() {
    ...
  }
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;swagger-and-openapi-based-documentation-with-springfox&#34;&gt;Swagger and OpenAPI-based documentation with SpringFox&lt;/h3&gt;
&lt;p&gt;SpringFox is an open-source project, separate from the Spring Framework, that can create Swagger-based API documentation at runtime. It does so by examining the application at startup, for example, inspecting WebFlux and Swagger-based annotations.&lt;/p&gt;
&lt;h3 id=&#34;persistent-data-with-spring-data&#34;&gt;Persistent data with Spring Data&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Spring Data&lt;/strong&gt; comes with a common programming model for persisting data in various types of database engine, ranging from traditional relational databases (SQL databases) to various types of NoSQL database engine, such as document databases (for example, MongoDB), key-value databases (for example, Redis), and graph databases (for example, Neo4J).&lt;/p&gt;
&lt;p&gt;The Spring Data project is divided into several subprojects and in this book we will use &lt;strong&gt;Spring Data&lt;/strong&gt; subprojects for MongoDB and &lt;strong&gt;JPA&lt;/strong&gt; that have been mapped to a MySQL database.&lt;/p&gt;
&lt;p&gt;The two core concepts of the programming model in Spring Data are &lt;strong&gt;entities&lt;/strong&gt; and &lt;strong&gt;repositories&lt;/strong&gt;. Entities and repositories generalize how data is stored and accessed from the various types of database.&lt;/p&gt;
&lt;h4 id=&#34;entity&#34;&gt;Entity&lt;/h4&gt;
&lt;p&gt;Entity classes are, in general, annotated with a mix of generic Spring Data annotations and annotations that are specific to each database technology.&lt;/p&gt;
&lt;p&gt;For example, an entity that will be stored in a relational database can be annotated with JPA annotations such as the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import javax.persistence.Entity;
import javax.persistence.Id;
import javax.persistence.IdClass;
import javax.persistence.Table;

@Entity
@IdClass(ReviewEntityPK.class)
@Table(name = &amp;quot;review&amp;quot;)
public class ReviewEntity {
 @Id private int productId;
 @Id private int reviewId;
 private String author;
 private String subject;
 private String content;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If an entity is to be stored in a MongoDB database, annotations from the Spring Data MongoDB subproject can be used together with generic Spring Data annotations. For example, consider the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.springframework.data.annotation.Id;
import org.springframework.data.annotation.Version;
import org.springframework.data.mongodb.core.mapping.Document;

@Document
public class RecommendationEntity {

    @Id
    private String id;

    @Version
    private int version;

    private int productId;
    private int recommendationId;
    private String author;
    private int rate;
    private String content;
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;repositories&#34;&gt;Repositories&lt;/h4&gt;
&lt;p&gt;Repositories are used to store and access data from different types of database. In its most basic form, a repository can be declared as a Java interface.&lt;/p&gt;
&lt;p&gt;Spring Data also comes with some base Java interfaces, for example, CrudRepository, to make the definition of a repository even simpler.&lt;/p&gt;
&lt;p&gt;To specify a repository for handling the JPA entity, ReviewEntity, we only need to declare the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.springframework.data.repository.CrudRepository;

public interface ReviewRepository extends CrudRepository&amp;lt;ReviewEntity, ReviewEntityPK&amp;gt; {
    Collection&amp;lt;ReviewEntity&amp;gt; findByProductId(int productId);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this example we use a class, ReviewEntityPK, to describe a composite primary key. It looks as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public class ReviewEntityPK implements Serializable {
    public int productId;
    public int reviewId;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We have also added an extra method, findByProductId, which allows us to look up Review entities based on productId&lt;/p&gt;
&lt;p&gt;If we want to use the repository, we can simply inject it and then start to use it, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;private final ReviewRepository repository;

@Autowired
public ReviewService(ReviewRepository repository) {
 this.repository = repository;
}

public void someMethod() {
  repository.save(entity);
  repository.delete(entity);
  repository.findByProductId(productId);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spring Data also provides a reactive base interface, ReactiveCrudRepository, which enables &lt;strong&gt;reactive repositories&lt;/strong&gt;. The methods in this interface do not return objects or collections of objects; instead, they return &lt;strong&gt;Mono&lt;/strong&gt; and &lt;strong&gt;Flux&lt;/strong&gt; objects.&lt;/p&gt;
&lt;p&gt;The reactive-based interface can only be used by Spring Data subprojects that support reactive database drivers; that is, they are based on non-blocking I/O. The Spring Data MongoDB subproject supports reactive repositories, while Spring Data JPA does not.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.springframework.data.repository.reactive.ReactiveCrudRepository;
import reactor.core.publisher.Flux;

public interface RecommendationRepository extends ReactiveCrudRepository&amp;lt;RecommendationEntity, String&amp;gt; {
    Flux&amp;lt;RecommendationEntity&amp;gt; findByProductId(int productId);
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;message-based-asynchronous-services-with-spring-cloud-stream&#34;&gt;Message-based asynchronous services with Spring Cloud Stream&lt;/h3&gt;
&lt;p&gt;Spring Cloud Stream provides a streaming abstraction over messaging, based on the publish-and-subscribe integration pattern. Spring Cloud Stream currently comes with support for Apache Kafka and RabbitMQ out of the box. A number of separate projects exist that provide integration with other popular messaging systems. See &lt;a href=&#34;https://github.com/spring-cloud?q=binder&#34;&gt;https://github.com/spring-cloud?q=binder&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;The core concepts in Spring Cloud Stream are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Message:&lt;/strong&gt; A data structure that&amp;rsquo;s used to describe data sent to and received from a messaging system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publisher:&lt;/strong&gt; Sends messages to the messaging system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subscriber&lt;/strong&gt;: Receives messages from the messaging system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Channel:&lt;/strong&gt; Used to communicate with the messaging system. Publishers use output channels and subscribers use input channels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binder:&lt;/strong&gt; A binder provides the actual integration with a specific messaging system, similar to what a JDBC driver does for a specific type of database.&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;examples&#34;&gt;examples&lt;/h6&gt;
&lt;p&gt;Let&amp;rsquo;s assume that we have a simple message class such as the following (constructors, getters, and setters have been left out for improved readability):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public class MyMessage {
  private String attribute1 = null;
  private String attribute2 = null;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spring Cloud Stream comes with default input and output &lt;strong&gt;channels&lt;/strong&gt;, &lt;strong&gt;Sink&lt;/strong&gt; and &lt;strong&gt;Source&lt;/strong&gt;, so we don&amp;rsquo;t need to create our own to get started. To publish a message, we can use the following source code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.springframework.cloud.stream.messaging.Source;

@EnableBinding(Source.class)
public class MyPublisher {

 @Autowired private Source mysource;

 public String processMessage(MyMessage message) {
   mysource.output().send(MessageBuilder.withPayload(message).build());
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To receive messages, we can use the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.springframework.cloud.stream.messaging.Sink;

@EnableBinding(Sink.class)
public class MySubscriber {

 @StreamListener(target = Sink.INPUT)
 public void receive(MyMessage message) {
 LOG.info(&amp;quot;Received: {}&amp;quot;,message);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To bind to RabbitMQ, we will use a dedicated starter dependency in the build file, build.gradle:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;implementation(&#39;org.springframework.cloud:spring-cloud-starter-stream-rabbit&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For the subscriber to receive messages from the publisher, we need to configure &lt;strong&gt;the input and output channel&lt;/strong&gt; to use the same destination. If we use YAML to describe our configuration, it might look like the following for the publisher:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spring.cloud.stream:
  default.contentType: application/json
  bindings.output.destination: mydestination
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The configuration for the subscriber is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spring.cloud.stream:
  default.contentType: application/json
  bindings.input.destination: mydestination
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;learning-about-docker&#34;&gt;Learning about Docker&lt;/h3&gt;
&lt;p&gt;For example, we can write scripts in order to automate end-to-end tests of our microservice landscape. A test script can start up the microservice landscape, run tests using the exposed services, and tear down the landscape. This type of automated test script is very useful.&lt;/p&gt;
&lt;p&gt;A build server can run these types of test in its continuous integration and deployment process whenever a developer pushes code to the source repository.&lt;/p&gt;
&lt;p&gt;The following Dockerfile is all that is required to run the microservice as a Docker container&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM openjdk:12.0.2

MAINTAINER Magnus Larsson &amp;lt;magnus.larsson.ml@gmail.com&amp;gt;

EXPOSE 8080
ADD ./build/libs/*.jar app.jar
ENTRYPOINT [&amp;quot;java&amp;quot;,&amp;quot;-jar&amp;quot;,&amp;quot;/app.jar&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we want to start and stop many containers with one command, Docker Compose is the perfect tool. Docker Compose uses a YAML file to describe the containers to be managed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;product&lt;/span&gt;:
 &lt;span style=&#34;color:#f92672&#34;&gt;build&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;microservices/product-service&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;recommendation&lt;/span&gt;:
 &lt;span style=&#34;color:#f92672&#34;&gt;build&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;microservices/recommendation-service&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;review&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;build&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;microservices/review-service&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;composite&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;build&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;microservices/product-composite-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;8080:8080&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;creating-a-set-of-cooperating-microservices&#34;&gt;Creating a Set of Cooperating Microservices&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter03&#34;&gt;https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter03&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;introducing-the-microservice-landscape&#34;&gt;Introducing the microservice landscape&lt;/h3&gt;
&lt;p&gt;the microservice-based system demo landscape&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://static.packt-cdn.com/products/9781789613476/graphics/cf74f5f6-c0f7-471c-8eae-a2566ecee996.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It consists of three core microservices, the &lt;strong&gt;Product&lt;/strong&gt;, &lt;strong&gt;Review&lt;/strong&gt;, and &lt;strong&gt;Recommendation&lt;/strong&gt; services, all of which deal with one type of resource, and a composite microservice called the &lt;strong&gt;Product Composite&lt;/strong&gt; service, which aggregates information from the three core services.&lt;/p&gt;
&lt;p&gt;At this stage, we don&amp;rsquo;t have any service discovery mechanism in place, we will use hardcoded port numbers for each microservice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product composite service: 7000&lt;/li&gt;
&lt;li&gt;Product service: 7001&lt;/li&gt;
&lt;li&gt;Review service: 7002&lt;/li&gt;
&lt;li&gt;Recommendation service: 7003&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Information handled by microservices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Product service&lt;/p&gt;
&lt;p&gt;The product service manages product information and describes each product with the following attributes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product ID&lt;/li&gt;
&lt;li&gt;Name&lt;/li&gt;
&lt;li&gt;Weight&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Review service&lt;/p&gt;
&lt;p&gt;The review service manages product reviews and stores the following information about each review:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product ID&lt;/li&gt;
&lt;li&gt;Review ID&lt;/li&gt;
&lt;li&gt;Author&lt;/li&gt;
&lt;li&gt;Subject&lt;/li&gt;
&lt;li&gt;Content&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recommendation service&lt;/p&gt;
&lt;p&gt;The recommendation service manages product recommendations and stores the following information about each recommendation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product ID&lt;/li&gt;
&lt;li&gt;Recommendation ID&lt;/li&gt;
&lt;li&gt;Author&lt;/li&gt;
&lt;li&gt;Rate&lt;/li&gt;
&lt;li&gt;Content&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Product composite service&lt;/p&gt;
&lt;p&gt;The product composite service aggregates information from the three core services and presents information about a product as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product information, as described in the product service&lt;/li&gt;
&lt;li&gt;A list of product reviews for the specified product, as described in the review service&lt;/li&gt;
&lt;li&gt;A list of product recommendations for the specified product, as described in the recommendation service&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Infrastructure-related information&lt;/p&gt;
&lt;p&gt;Once we start to run our microservices as containers that are managed by the infrastructure (first Docker and later on Kubernetes), it will be of interest to track which container actually responded to our requests. To simplify this tracking, we have also added a serviceAddress attribute to all our responses, formatted as hostname/ip-address:port.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;generating-skeleton-microservices&#34;&gt;Generating skeleton microservices&lt;/h3&gt;
&lt;h4 id=&#34;using-spring-initializr-to-generate-skeleton-code&#34;&gt;Using Spring Initializr to generate skeleton code&lt;/h4&gt;
&lt;p&gt;To create skeleton code for our microservices, we need to run the following command for product-service:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spring init \
--boot-version=2.1.0.RC1 \
--build=gradle \
--java-version=1.8 \
--packaging=jar \
--name=product-service \
--package-name=se.magnus.microservices.core.product \
--groupId=se.magnus.microservices.core.product \
--dependencies=actuator,webflux \
--version=1.0.0-SNAPSHOT \
product-service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Spring Boot Actuator enables a number of valuable endpoints for management and monitoring. We will see them in action later on. Spring WebFlux will be used here to create our RESTful APIs.&lt;/p&gt;
&lt;p&gt;We can build each microservice separately with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd microservices/product-composite-service; ./gradlew build; cd -; \
cd microservices/product-service;           ./gradlew build; cd -; \
cd microservices/recommendation-service;    ./gradlew build; cd -; \
cd microservices/review-service;            ./gradlew build; cd -; 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;setting-up-multi-project-builds-in-gradle&#34;&gt;Setting up multi-project builds in Gradle&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;create the settings.gradle file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; settings.gradle
include &#39;:microservices:product-service&#39;
include &#39;:microservices:review-service&#39;
include &#39;:microservices:recommendation-service&#39;
include &#39;:microservices:product-composite-service&#39;
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;copy the Gradle executable files that were generated from one of the projects so that we can reuse them for the multi-project builds:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cp -r microservices/product-service/gradle .
cp microservices/product-service/gradlew .
cp microservices/product-service/gradlew.bat .
cp microservices/product-service/.gitignore .
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;no longer need the generated Gradle executable files in each project&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;find microservices -depth -name &amp;quot;gradle&amp;quot; -exec rm -rfv &amp;quot;{}&amp;quot; \; 
find microservices -depth -name &amp;quot;gradlew*&amp;quot; -exec rm -fv &amp;quot;{}&amp;quot; \; 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;build all the microservices with one command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./gradlew build
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;adding-restful-apis&#34;&gt;Adding RESTful APIs&lt;/h3&gt;
&lt;h4 id=&#34;adding-an-api-and-a-util-project&#34;&gt;Adding an API and a util project&lt;/h4&gt;
&lt;p&gt;add two projects (api and util) that will contain code that is shared by the microservice projects&lt;/p&gt;
&lt;p&gt;It is debatable whether it is good practice to store API definitions for a group of microservices in a common API module. To me, it is a good choice for microservices that are part of the same delivery organization, that is, whose releases are governed by one and the same organization (compare to a &lt;em&gt;Bounded Context&lt;/em&gt; in &lt;em&gt;Domain-Driven Design&lt;/em&gt;, where our microservices are placed in one and the same bounded context).&lt;/p&gt;
&lt;h6 id=&#34;the-api-project&#34;&gt;The api project&lt;/h6&gt;
&lt;h6 id=&#34;the-util-project&#34;&gt;The util project&lt;/h6&gt;
&lt;h4 id=&#34;implementing-the-restful-apis&#34;&gt;Implementing the RESTful APIs&lt;/h4&gt;
&lt;h3 id=&#34;adding-a-composite-microservice&#34;&gt;Adding a composite microservice&lt;/h3&gt;
&lt;h3 id=&#34;adding-error-handling&#34;&gt;Adding error handling&lt;/h3&gt;
&lt;h3 id=&#34;testing-the-apis-manually&#34;&gt;Testing the APIs manually&lt;/h3&gt;
&lt;h3 id=&#34;adding-automated-tests-of-microservices-in-isolation&#34;&gt;Adding automated tests of microservices in isolation&lt;/h3&gt;
&lt;h3 id=&#34;adding-semi-automated-tests-to-a-microservice-landscape&#34;&gt;Adding semi-automated tests to a microservice landscape&lt;/h3&gt;
&lt;h2 id=&#34;deploying-our-microservices-using-docker&#34;&gt;Deploying Our Microservices Using Docker&lt;/h2&gt;
&lt;h2 id=&#34;adding-api-description-using-openapiswagger&#34;&gt;Adding API Description Using OpenAPI/Swagger&lt;/h2&gt;
&lt;h2 id=&#34;adding-persistence&#34;&gt;Adding Persistence&lt;/h2&gt;
&lt;h2 id=&#34;developing-reactive-microservices&#34;&gt;Developing Reactive Microservices&lt;/h2&gt;
</description>
      
    </item>
    
    <item>
      <title>Leveraging Spring Cloud to Manage Microservices</title>
      <link>http://www.yezheng.pro/post/specialization/web-application/spring/microservice-with-spring-cloud/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/web-application/spring/microservice-with-spring-cloud/</guid>
      
        <description>&lt;h2 id=&#34;introduction-to-spring-cloud&#34;&gt;Introduction to Spring Cloud&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Design pattern&lt;/th&gt;
&lt;th&gt;Current component&lt;/th&gt;
&lt;th&gt;Replaced by Software component&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Service discovery&lt;/td&gt;
&lt;td&gt;Netflix Eureka &amp;amp; Netflix Ribbon&lt;/td&gt;
&lt;td&gt;Netflix Eureka and Spring Cloud load balancer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Edge server&lt;/td&gt;
&lt;td&gt;Netflix Zuul&lt;/td&gt;
&lt;td&gt;Spring Cloud Gateway and Spring Security OAuth&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Centralized configuration&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Spring Cloud Configuration Server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Circuit breaker&lt;/td&gt;
&lt;td&gt;Netflix Hystrix&lt;/td&gt;
&lt;td&gt;Resilience4j&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Distributed tracing&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Spring Cloud Sleuth and Zipkin&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;adding-service-discovery-using-netflix-eureka-and-ribbon&#34;&gt;Adding Service Discovery Using Netflix Eureka and Ribbon&lt;/h2&gt;
&lt;h2 id=&#34;using-spring-cloud-gateway-to-hide-microservices-behind-an-edge-server&#34;&gt;Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server&lt;/h2&gt;
&lt;h2 id=&#34;securing-access-to-apis&#34;&gt;Securing Access to APIs&lt;/h2&gt;
&lt;h2 id=&#34;centralized-configuration&#34;&gt;Centralized Configuration&lt;/h2&gt;
&lt;h2 id=&#34;improving-resilience-using-resilience4j&#34;&gt;Improving Resilience Using Resilience4j&lt;/h2&gt;
&lt;h2 id=&#34;understanding-distributed-tracing&#34;&gt;Understanding Distributed Tracing&lt;/h2&gt;
</description>
      
    </item>
    
    <item>
      <title>Link custom domain to Github pages</title>
      <link>http://www.yezheng.pro/post/any-path/blog/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/any-path/blog/</guid>
      
        <description>&lt;h3 id=&#34;register-domain-name&#34;&gt;Register Domain Name&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.namesilo.com/&#34;&gt;Namesilo&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;use-a-custom-domain&#34;&gt;Use a Custom Domain&lt;/h3&gt;
&lt;p&gt;If you’d like to use a custom domain for your GitHub Pages site, create a file &lt;code&gt;static/CNAME&lt;/code&gt;. Your custom domain name should be the only contents inside &lt;code&gt;CNAME&lt;/code&gt;. Since it’s inside &lt;code&gt;static&lt;/code&gt;, the published site will contain the CNAME file at the root of the published site, which is a requirements of GitHub Pages.&lt;/p&gt;
&lt;h3 id=&#34;dns-service-provider&#34;&gt;DNS service provider&lt;/h3&gt;
&lt;h4 id=&#34;create-cloudflare-account&#34;&gt;Create Cloudflare account&lt;/h4&gt;
&lt;h4 id=&#34;enter-your-domain-name&#34;&gt;Enter your domain name&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;yezheng.pro
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;review-your-dns-records&#34;&gt;Review your DNS records&lt;/h4&gt;
&lt;p&gt;Add more DNS records for yezheng.pro&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Content&lt;/th&gt;
&lt;th&gt;TTL&lt;/th&gt;
&lt;th&gt;Proxy status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;yezheng.pro&lt;/td&gt;
&lt;td&gt;192.30.252.154&lt;/td&gt;
&lt;td&gt;Auto&lt;/td&gt;
&lt;td&gt;Proxied&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;yezheng.pro&lt;/td&gt;
&lt;td&gt;192.30.252.153&lt;/td&gt;
&lt;td&gt;Auto&lt;/td&gt;
&lt;td&gt;Proxied&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNAME&lt;/td&gt;
&lt;td&gt;www&lt;/td&gt;
&lt;td&gt;csyezheng.github.io&lt;/td&gt;
&lt;td&gt;Auto&lt;/td&gt;
&lt;td&gt;Proxied&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;change-your-nameservers&#34;&gt;Change your nameservers&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Log in to your registrar account&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Determine your registrar via &lt;a href=&#34;https://whois.icann.org/en/lookup?name=yezheng.pro&#34;&gt;WHOIS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Remove these nameservers:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ns3.dnsowl.comns1.dnsowl.comns2.dnsowl.com
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Replace with Cloudflare&amp;rsquo;s nameservers&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nameserver 1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;isabel.ns.cloudflare.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Nameserver 2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;phil.ns.cloudflare.com
&lt;/code&gt;&lt;/pre&gt;</description>
      
    </item>
    
    <item>
      <title>read open source</title>
      <link>http://www.yezheng.pro/post/master-path/read-open-source/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/master-path/read-open-source/</guid>
      
        <description>&lt;h5 id=&#34;awesome-project-lists-using-ginhttpsgithubcomgin-gonicgin-web-framework&#34;&gt;Awesome project lists using &lt;a href=&#34;https://github.com/gin-gonic/gin&#34;&gt;Gin&lt;/a&gt; web framework:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/appleboy/gorush&#34;&gt;gorush&lt;/a&gt;: A push notification server written in Go.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fnproject/fn&#34;&gt;fnproject&lt;/a&gt;: The container native, cloud agnostic serverless platform.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/photoprism/photoprism&#34;&gt;photoprism&lt;/a&gt;: Personal photo management powered by Go and Google TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/devopsfaith/krakend&#34;&gt;krakend&lt;/a&gt;: Ultra performant API Gateway with middlewares.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/thoas/picfit&#34;&gt;picfit&lt;/a&gt;: An image resizing server written in Go.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gotify/server&#34;&gt;gotify&lt;/a&gt;: A simple server for sending and receiving messages in real-time per web socket.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ovh/cds&#34;&gt;cds&lt;/a&gt;: Enterprise-Grade Continuous Delivery &amp;amp; DevOps Automation Open Source Platform.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;awesome-project-lists-using-spring-boot-web-framework&#34;&gt;Awesome project lists using spring boot web framework:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;ctripcorp / apollo Apollo（阿波罗）是携程框架部门研发的分布式配置中心&lt;/li&gt;
&lt;li&gt;sqshq / piggymetrics  a tutorial project demonstrates Microservice Architecture with Spring Boot, Spring Cloud and Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/edx/edx-platform/tree/master/common/djangoapps/track&#34;&gt;https://github.com/edx/edx-platform/tree/master/common/djangoapps/track&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/edx/event-tracking&#34;&gt;https://github.com/edx/event-tracking&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>flink</title>
      <link>http://www.yezheng.pro/post/specialization/big-data/flink/</link>
      <pubDate>Thu, 12 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>http://www.yezheng.pro/post/specialization/big-data/flink/</guid>
      
        <description>&lt;p&gt;&lt;a href=&#34;https://github.com/apache/flink/tree/master/flink-runtime-web&#34;&gt;Apache Flink Web Dashboard&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>
